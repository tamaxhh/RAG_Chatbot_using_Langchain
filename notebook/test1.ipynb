{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b75b506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii\n"
     ]
    }
   ],
   "source": [
    "print(\"hii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "\n",
    "def load_document(file_path: str) -> str:\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "    elif ext == \".txt\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_document(\"/workspaces/RAG_Chatbot_using_Langchain/Data /Data Science from Scratch.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> list[str]:\n",
    "    # Simple chunking by paragraphs; use NLTK/SpaCy for better.\n",
    "    chunks = text.split(\"\\n\\n\")\n",
    "    return [chunk.strip() for chunk in chunks if chunk.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATA/DATA SCIENCEData Science from Scratch\\nISBN: 978-1-491-90142-7US $39.99  CAN $45.99“\\tJoel\\ttakes\\t you\\ton\\ta\\t\\njourney \\tfrom\\tbeing \\t\\ndata-curious \\tto\\tgetting \\ta\\t\\nthorough \\tunderstanding \\t\\nof\\tthe\\tbread-and-butter \\t\\nalgorithms \\tthat\\tevery \\tdata\\t\\nscientist \\tshould \\tknow.”\\n—Rohit Sivaprasad\\nData Science, Soylent  \\ndatatau.com\\nTwitter: @oreillymedia\\nfacebook.com/oreillyData science libraries, frameworks, modules, and toolkits are great for \\ndoing data science, but they’re also a good way to dive into the discipline \\nwithout actually understanding data science. In this book, you’ll learn how \\nmany of the most fundamental data science tools and algorithms work by \\nimplementing them from scratch.\\nIf you have an aptitude for mathematics and some programming skills, \\nauthor Joel Grus will help you get comfortable with the math and statistics \\nat the core of data science, and with hacking skills you need to get started \\nas a data scientist. Today’s messy glut of data holds answers to questions \\nno one’s even thought to ask. This book provides you with the know-how \\nto dig those answers out.\\n ■Get a crash course in Python\\n ■Learn the basics of linear algebra, statistics, and probability—\\nand understand how and when they\\'re used in data science\\n ■Collect, explore, clean, munge, and manipulate data\\n ■Dive into the fundamentals of machine learning\\n ■Implement models such as k-nearest neighbors, Naive Bayes, \\nlinear and logistic regression, decision trees, neural networks, \\nand clustering\\n ■Explore recommender systems, natural language processing, \\nnetwork analysis, MapReduce, and databases\\nJoel Grus is a software engineer at Google. Before that, he worked as a data \\nscientist at multiple startups. He lives in Seattle, where he regularly attends data \\nscience happy hours. He blogs infrequently at joelgrus.com  and tweets all day \\nlong at @joelgrus.\\nJoel GrusData Science \\nfrom Scratch\\nFIRST PRINCIPLES WITH PYTHON\\nData Science from Scratch\\nGrus\\nwww.it-ebooks.infoDATA/DATA SCIENCEData Science from Scratch\\nISBN: 978-1-491-90142-7US $39.99  CAN $45.99“\\tJoel\\ttakes\\t you\\ton\\ta\\t\\njourney \\tfrom\\tbeing \\t\\ndata-curious \\tto\\tgetting \\ta\\t\\nthorough \\tunderstanding \\t\\nof\\tthe\\tbread-and-butter \\t\\nalgorithms \\tthat\\tevery \\tdata\\t\\nscientist \\tshould \\tknow.”\\n—Rohit Sivaprasad\\nData Science, Soylent  \\ndatatau.com\\nTwitter: @oreillymedia\\nfacebook.com/oreillyData science libraries, frameworks, modules, and toolkits are great for \\ndoing data science, but they’re also a good way to dive into the discipline \\nwithout actually understanding data science. In this book, you’ll learn how \\nmany of the most fundamental data science tools and algorithms work by \\nimplementing them from scratch.\\nIf you have an aptitude for mathematics and some programming skills, \\nauthor Joel Grus will help you get comfortable with the math and statistics \\nat the core of data science, and with hacking skills you need to get started \\nas a data scientist. Today’s messy glut of data holds answers to questions \\nno one’s even thought to ask. This book provides you with the know-how \\nto dig those answers out.\\n ■Get a crash course in Python\\n ■Learn the basics of linear algebra, statistics, and probability—\\nand understand how and when they\\'re used in data science\\n ■Collect, explore, clean, munge, and manipulate data\\n ■Dive into the fundamentals of machine learning\\n ■Implement models such as k-nearest neighbors, Naive Bayes, \\nlinear and logistic regression, decision trees, neural networks, \\nand clustering\\n ■Explore recommender systems, natural language processing, \\nnetwork analysis, MapReduce, and databases\\nJoel Grus is a software engineer at Google. Before that, he worked as a data \\nscientist at multiple startups. He lives in Seattle, where he regularly attends data \\nscience happy hours. He blogs infrequently at joelgrus.com  and tweets all day \\nlong at @joelgrus.\\nJoel GrusData Science \\nfrom Scratch\\nFIRST PRINCIPLES WITH PYTHON\\nData Science from Scratch\\nGrus\\nwww.it-ebooks.infoJoel GrusData Science from Scratch\\nwww.it-ebooks.info978-1-491-90142-7\\n[LSI]Data Science from Scratch\\nby Joel Grus\\nCopyright © 2015 O’Reilly Media. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://safaribooksonline.com ). For more information, contact our corporate/\\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com .\\nEditor:  Marie Beaugureau\\nProduction Editor:  Melanie Y arbrough\\nCopyeditor:  Nan Reinhardt\\nProofreader:  Eileen CohenIndexer:  Ellen Troutman-Zaig\\nInterior Designer:  David Futato\\nCover Designer:  Karen Montgomery\\nIllustrator:  Rebecca Demarest\\nApril 2015:  First Edition\\nRevision History for the First Edition\\n2015-04-10: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491901427  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Data Science from Scratch, the cover\\nimage of a Rock Ptarmigan, and related trade dress are trademarks of O’Reilly Media, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.\\nwww.it-ebooks.infoTable of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\n1.Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nThe Ascendance of Data                                                                                                   1\\nWhat Is Data Science?                                                                                                       1\\nMotivating Hypothetical: DataSciencester                                                                     2\\nFinding Key Connectors                                                                                                3\\nData Scientists Y ou May Know                                                                                     6\\nSalaries and Experience                                                                                                 8\\nPaid Accounts                                                                                                               11\\nTopics of Interest                                                                                                          11\\nOnward                                                                                                                          13\\n2.A Crash Course in Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  15\\nThe Basics                                                                                                                          15\\nGetting Python                                                                                                             15\\nThe Zen of Python                                                                                                       16\\nWhitespace Formatting                                                                                               16\\nModules                                                                                                                         17\\nArithmetic                                                                                                                     18\\nFunctions                                                                                                                       18\\nStrings                                                                                                                            19\\nExceptions                                                                                                                     19\\nLists                                                                                                                                 20\\nTuples                                                                                                                             21\\nDictionaries                                                                                                                   21\\nSets                                                                                                                                  24\\nControl Flow                                                                                                                 25\\niii\\nwww.it-ebooks.infoTruthiness                                                                                                                      25\\nThe Not-So-Basics                                                                                                           26\\nSorting                                                                                                                            27\\nList Comprehensions                                                                                                   27\\nGenerators and Iterators                                                                                             28\\nRandomness                                                                                                                  29\\nRegular Expressions                                                                                                     30\\nObject-Oriented Programming                                                                                  30\\nFunctional Tools                                                                                                           31\\nenumerate                                                                                                                      32\\nzip and Argument Unpacking                                                                                    33\\nargs and kwargs                                                                                                            34\\nWelcome to DataSciencester!                                                                                     35\\nFor Further Exploration                                                                                                  35\\n3.Visualizing Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\\nmatplotlib                                                                                                                          37\\nBar Charts                                                                                                                         39\\nLine Charts                                                                                                                        43\\nScatterplots                                                                                                                        44\\nFor Further Exploration                                                                                                  47\\n4.Linear Algebra. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  49\\nVectors                                                                                                                               49\\nMatrices                                                                                                                             53\\nFor Further Exploration                                                                                                  55\\n5.Statistics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  57\\nDescribing a Single Set of Data                                                                                      57\\nCentral Tendencies                                                                                                       59\\nDispersion                                                                                                                     61\\nCorrelation                                                                                                                        62\\nSimpson’s Paradox                                                                                                            65\\nSome Other Correlational Caveats                                                                                66\\nCorrelation and Causation                                                                                             67\\nFor Further Exploration                                                                                                  68\\n6.Probability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  69\\nDependence and Independence                                                                                     69\\nConditional Probability                                                                                                  70\\nBayes’s Theorem                                                                                                               72\\nRandom Variables                                                                                                            73\\niv | Table of Contents\\nwww.it-ebooks.infoContinuous Distributions                                                                                               74\\nThe Normal Distribution                                                                                                75\\nThe Central Limit Theorem                                                                                           78\\nFor Further Exploration                                                                                                  80\\n7.Hypothesis and Inference. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81\\nStatistical Hypothesis Testing                                                                                         81\\nExample: Flipping a Coin                                                                                               81\\nConfidence Intervals                                                                                                       85\\nP-hacking                                                                                                                          86\\nExample: Running an A/B Test                                                                                      87\\nBayesian Inference                                                                                                           88\\nFor Further Exploration                                                                                                  92\\n8.Gradient Descent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  93\\nThe Idea Behind Gradient Descent                                                                               93\\nEstimating the Gradient                                                                                                  94\\nUsing the Gradient                                                                                                           97\\nChoosing the Right Step Size                                                                                         97\\nPutting It All Together                                                                                                    98\\nStochastic Gradient Descent                                                                                           99\\nFor Further Exploration                                                                                                100\\n9.Getting Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  103\\nstdin and stdout                                                                                                             103\\nReading Files                                                                                                                   105\\nThe Basics of Text Files                                                                                              105\\nDelimited Files                                                                                                            106\\nScraping the Web                                                                                                           108\\nHTML and the Parsing Thereof                                                                               108\\nExample: O’Reilly Books About Data                                                                      110\\nUsing APIs                                                                                                                      114\\nJSON (and XML)                                                                                                        114\\nUsing an Unauthenticated API                                                                                 115\\nFinding APIs                                                                                                               116\\nExample: Using the Twitter APIs                                                                                 117\\nGetting Credentials                                                                                                    117\\nFor Further Exploration                                                                                                120\\n10. Working with Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  121\\nExploring Y our Data                                                                                                      121\\nExploring One-Dimensional Data                                                                           121\\nTable of Contents | v\\nwww.it-ebooks.infoTwo Dimensions                                                                                                        123\\nMany Dimensions                                                                                                      125\\nCleaning and Munging                                                                                                 127\\nManipulating Data                                                                                                         129\\nRescaling                                                                                                                         132\\nDimensionality Reduction                                                                                           134\\nFor Further Exploration                                                                                                139\\n11. Machine Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  141\\nModeling                                                                                                                         141\\nWhat Is Machine Learning?                                                                                         142\\nOverfitting and Underfitting                                                                                        142\\nCorrectness                                                                                                                     145\\nThe Bias-Variance Trade-off                                                                                        147\\nFeature Extraction and Selection                                                                                 148\\nFor Further Exploration                                                                                                150\\n12. k-Nearest Neighbors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  151\\nThe Model                                                                                                                       151\\nExample: Favorite Languages                                                                                       153\\nThe Curse of Dimensionality                                                                                       156\\nFor Further Exploration                                                                                                163\\n13. Naive Bayes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  165\\nA Really Dumb Spam Filter                                                                                         165\\nA More Sophisticated Spam Filter                                                                               166\\nImplementation                                                                                                              168\\nTesting Our Model                                                                                                         169\\nFor Further Exploration                                                                                                172\\n14. Simple Linear Regression. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  173\\nThe Model                                                                                                                       173\\nUsing Gradient Descent                                                                                                176\\nMaximum Likelihood Estimation                                                                               177\\nFor Further Exploration                                                                                                177\\n15. Multiple Regression. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  179\\nThe Model                                                                                                                       179\\nFurther Assumptions of the Least Squares Model                                                    180\\nFitting the Model                                                                                                           181\\nInterpreting the Model                                                                                                  182\\nGoodness of Fit                                                                                                              183\\nvi | Table of Contents\\nwww.it-ebooks.infoDigression: The Bootstrap                                                                                            183\\nStandard Errors of Regression Coefficients                                                               184\\nRegularization                                                                                                                186\\nFor Further Exploration                                                                                                188\\n16. Logistic Regression. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  189\\nThe Problem                                                                                                                   189\\nThe Logistic Function                                                                                                   192\\nApplying the Model                                                                                                       194\\nGoodness of Fit                                                                                                              195\\nSupport Vector Machines                                                                                             196\\nFor Further Investigation                                                                                              200\\n17. Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  201\\nWhat Is a Decision Tree?                                                                                              201\\nEntropy                                                                                                                            203\\nThe Entropy of a Partition                                                                                            205\\nCreating a Decision Tree                                                                                               206\\nPutting It All Together                                                                                                  208\\nRandom Forests                                                                                                             211\\nFor Further Exploration                                                                                                212\\n18. Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  213\\nPerceptrons                                                                                                                     213\\nFeed-Forward Neural Networks                                                                                  215\\nBackpropagation                                                                                                            218\\nExample: Defeating a CAPTCHA                                                                               219\\nFor Further Exploration                                                                                                224\\n19. Clustering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  225\\nThe Idea                                                                                                                           225\\nThe Model                                                                                                                       226\\nExample: Meetups                                                                                                          227\\nChoosing k                                                                                                                      230\\nExample: Clustering Colors                                                                                          231\\nBottom-up Hierarchical Clustering                                                                            233\\nFor Further Exploration                                                                                                238\\n20. Natural Language Processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  239\\nWord Clouds                                                                                                                   239\\nn-gram Models                                                                                                               241\\nGrammars                                                                                                                       244\\nTable of Contents | vii\\nwww.it-ebooks.infoAn Aside: Gibbs Sampling                                                                                            246\\nTopic Modeling                                                                                                              247\\nFor Further Exploration                                                                                                253\\n21. Network Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  255\\nBetweenness Centrality                                                                                                 255\\nEigenvector Centrality                                                                                                   260\\nMatrix Multiplication                                                                                                260\\nCentrality                                                                                                                     262\\nDirected Graphs and PageRank                                                                                   264\\nFor Further Exploration                                                                                                266\\n22. Recommender Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  267\\nManual Curation                                                                                                            268\\nRecommending What’s Popular                                                                                  268\\nUser-Based Collaborative Filtering                                                                             269\\nItem-Based Collaborative Filtering                                                                             272\\nFor Further Exploration                                                                                                274\\n23. Databases and SQL. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  275\\nCREATE TABLE and INSERT                                                                                     275\\nUPDATE                                                                                                                         277\\nDELETE                                                                                                                          278\\nSELECT                                                                                                                           278\\nGROUP BY                                                                                                                     280\\nORDER BY                                                                                                                     282\\nJOIN                                                                                                                                 283\\nSubqueries                                                                                                                       285\\nIndexes                                                                                                                            285\\nQuery Optimization                                                                                                      286\\nNoSQL                                                                                                                             287\\nFor Further Exploration                                                                                                287\\n24. MapReduce. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  289\\nExample: Word Count                                                                                                   289\\nWhy MapReduce?                                                                                                          291\\nMapReduce More Generally                                                                                        292\\nExample: Analyzing Status Updates                                                                            293\\nExample: Matrix Multiplication                                                                                   294\\nAn Aside: Combiners                                                                                                    296\\nFor Further Exploration                                                                                                296\\nviii | Table of Contents\\nwww.it-ebooks.info25. Go Forth and Do Data Science. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  299\\nIPython                                                                                                                            299\\nMathematics                                                                                                                   300\\nNot from Scratch                                                                                                            300\\nNumPy                                                                                                                         301\\npandas                                                                                                                          301\\nscikit-learn                                                                                                                   301\\nVisualization                                                                                                               301\\nR                                                                                                                                    302\\nFind Data                                                                                                                         302\\nDo Data Science                                                                                                             303\\nHacker News                                                                                                               303\\nFire Trucks                                                                                                                   303\\nT-shirts                                                                                                                         303\\nAnd Y ou?                                                                                                                     304\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\\nTable of Contents | ix\\nwww.it-ebooks.infowww.it-ebooks.infoPreface\\nData Science\\nData scientist has been called “the sexiest job of the 21st century, ”  presumably by\\nsomeone who has never visited a fire station. Nonetheless, data science is a hot and\\ngrowing field, and it doesn’t take a great deal of sleuthing to find analysts breathlessly\\nprognosticating that over the next 10 years, we’ll need billions and billions more data\\nscientists than we currently have.\\nBut what is data science? After all, we can’t produce data scientists if we don’t know\\nwhat data science is. According to a Venn diagram  that is somewhat famous in the\\nindustry, data science lies at the intersection of:\\n•Hacking skills\\n•Math and statistics knowledge\\n•Substantive expertise\\nAlthough I originally intended to write a book covering all three, I quickly realized\\nthat a thorough treatment of “substantive expertise” would require tens of thousands\\nof pages. At that point, I decided to focus on the first two. My goal is to help you\\ndevelop the hacking skills that you’ll need to get started doing data science. And my\\ngoal is to help you get comfortable with the mathematics and statistics that are at the\\ncore of data science.\\nThis is a somewhat heavy aspiration for a book. The best way to learn hacking skills is\\nby hacking on things. By reading this book, you will get a good understanding of the\\nway I hack on things, which may not necessarily be the best way for you to hack on\\nthings. Y ou will get a good understanding of some of the tools I use, which will not\\nnecessarily be the best tools for you to use. Y ou will get a good understanding of the\\nway I approach data problems, which may not necessarily be the best way for you to\\napproach data problems. The intent (and the hope) is that my examples will inspire\\nxi\\nwww.it-ebooks.infoyou try things your own way. All the code and data from the book is available on\\nGitHub  to get you started.\\nSimilarly, the best way to learn mathematics is by doing mathematics. This is emphat‐\\nically not a math book, and for the most part, we won’t be “doing mathematics. ” How‐\\never, you can’t really do data science without some  understanding of probability and\\nstatistics and linear algebra. This means that, where appropriate, we will dive into\\nmathematical equations, mathematical intuition, mathematical axioms, and cartoon\\nversions of big mathematical ideas. I hope that you won’t be afraid to dive in with me.\\nThroughout it all, I also hope to give you a sense that playing with data is fun,\\nbecause, well, playing with data is fun! (Especially compared to some of the alterna‐\\ntives, like tax preparation or coal mining.)\\nFrom Scratch\\nThere are lots and lots of data science libraries, frameworks, modules, and toolkits\\nthat efficiently implement the most common (as well as the least common) data sci‐\\nence algorithms and techniques. If you become a data scientist, you will become inti‐\\nmately familiar with NumPy, with scikit-learn, with pandas, and with a panoply of\\nother libraries. They are great for doing data science. But they are also a good way to\\nstart doing data science without actually understanding data science.\\nIn this book, we will be approaching data science from scratch. That means we’ll be\\nbuilding tools and implementing algorithms by hand in order to better understand\\nthem. I put a lot of thought into creating implementations and examples that are\\nclear, well-commented, and readable. In most cases, the tools we build will be illumi‐\\nnating but impractical. They will work well on small toy data sets but fall over on\\n“web scale” ones.\\nThroughout the book, I will point you to libraries you might use to apply these tech‐\\nniques to larger data sets. But we won’t be using them here.\\nThere is a healthy debate raging over the best language for learning data science.\\nMany people believe it’s the statistical programming language R. (We call those peo‐\\nple wrong .) A few people suggest Java or Scala.  However, in my opinion, Python is the\\nobvious choice.\\nPython has several features that make it well suited for learning (and doing) data sci‐\\nence:\\n•It’s free.\\n•It’s relatively simple to code in (and, in particular, to understand).\\n•It has lots of useful data science–related libraries.\\nxii | Preface\\nwww.it-ebooks.infoI am hesitant to call Python my favorite programming language. There are other lan‐\\nguages I find more pleasant, better-designed, or just more fun to code in. And yet\\npretty much every time I start a new data science project, I end up using Python.\\nEvery time I need to quickly prototype something that just works, I end up using\\nPython. And every time I want to demonstrate data science concepts in a clear, easy-\\nto-understand way, I end up using Python. Accordingly, this book uses Python.\\nThe goal of this book is not to teach you Python. (Although it is nearly certain that by\\nreading this book you will learn some Python.) I’ll take you through a chapter-long\\ncrash course that highlights the features that are most important for our purposes,\\nbut if you know nothing about programming in Python (or about programming at\\nall) then you might want to supplement this book with some sort of “Python for\\nBeginners” tutorial.\\nThe remainder of our introduction to data science will take this same approach  —\\ngoing into detail where going into detail seems crucial or illuminating, at other times\\nleaving details for you to figure out yourself (or look up on Wikipedia).\\nOver the years, I’ve trained a number of data scientists. While not all of them have\\ngone on to become world-changing data ninja rockstars, I’ve left them all better data\\nscientists than I found them. And I’ve grown to believe that anyone who has some\\namount of mathematical aptitude and some amount of programming skill has the\\nnecessary raw materials to do data science. All she needs is an inquisitive mind, a\\nwillingness to work hard, and this book. Hence this book.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nPreface | xiii\\nwww.it-ebooks.infoThis element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nUsing Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/joelgrus/data-science-from-scratch .\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Data Science from Scratch  by Joel\\nGrus (O’Reilly). Copyright 2015 Joel Grus, 978-1-4919-0142-7. ”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com .\\nSafari® Books Online\\nSafari Books Online  is an on-demand digital library that deliv‐\\ners expert content  in both book and video form from the\\nworld’s leading authors in technology and business.\\nxiv | Preface\\nwww.it-ebooks.infoTechnology professionals, software developers, web designers, and business and crea‐\\ntive professionals use Safari Books Online as their primary resource for research,\\nproblem solving, learning, and certification training.\\nSafari Books Online offers a range of plans and pricing  for enterprise , government ,\\neducation , and individuals.\\nMembers have access to thousands of books, training videos, and prepublication\\nmanuscripts in one fully searchable database from publishers like O’Reilly Media,\\nPrentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que,\\nPeachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kauf‐\\nmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,\\nMcGraw-Hill, Jones & Bartlett, Course Technology, and hundreds more . For more\\ninformation about Safari Books Online, please visit us online .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/data-science-from-scratch .\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com .\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com .\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nAcknowledgments\\nFirst, I would like to thank Mike Loukides for accepting my proposal for this book\\n(and for insisting that I pare it down to a reasonable size). It would have been very\\neasy for him to say, “Who’s this person who keeps emailing me sample chapters, and\\nPreface | xv\\nwww.it-ebooks.infohow do I get him to go away?” I’m grateful he didn’t. I’ d also like to thank my editor,\\nMarie Beaugureau, for guiding me through the publishing process and getting the\\nbook in a much better state than I ever would have gotten it on my own.\\nI couldn’t have written this book if I’ d never learned data science, and I probably\\nwouldn’t have learned data science if not for the influence of Dave Hsu, Igor Tatari‐\\nnov, John Rauser, and the rest of the Farecast gang. (So long ago that it wasn’t even\\ncalled data science at the time!) The good folks at Coursera deserve a lot of credit,\\ntoo.\\nI am also grateful to my beta readers and reviewers. Jay Fundling found a ton of mis‐\\ntakes and pointed out many unclear explanations, and the book is much better (and\\nmuch more correct) thanks to him. Debashis Ghosh is a hero for sanity-checking all\\nof my statistics. Andrew Musselman suggested toning down the “people who prefer R\\nto Python are moral reprobates” aspect of the book, which I think ended up being\\npretty good advice. Trey Causey, Ryan Matthew Balfanz, Loris Mularoni, Núria Pujol,\\nRob Jefferson, Mary Pat Campbell, Zach Geary, and Wendy Grus also provided\\ninvaluable feedback. Any errors remaining are of course my responsibility.\\nI owe a lot to the Twitter #datascience commmunity, for exposing me to a ton of new\\nconcepts, introducing me to a lot of great people, and making me feel like enough of\\nan underachiever that I went out and wrote a book to compensate. Special thanks to\\nTrey Causey (again), for (inadvertently) reminding me to include a chapter on linear\\nalgebra, and to Sean J. Taylor, for (inadvertently) pointing out a couple of huge gaps\\nin the “Working with Data” chapter.\\nAbove all, I owe immense thanks to Ganga and Madeline. The only thing harder than\\nwriting a book is living with someone who’s writing a book, and I couldn’t have pulled\\nit off without their support.\\nxvi | Preface\\nwww.it-ebooks.infoCHAPTER 1\\nIntroduction\\n“Data! Data! Data!” he cried impatiently. “I can’t make bricks without clay. ”\\n—Arthur Conan Doyle\\nThe Ascendance of Data\\nWe live in a world that’s drowning in data. Websites track every user’s every click.\\nY our smartphone is building up a record of your location and speed every second of\\nevery day. “Quantified selfers” wear pedometers-on-steroids that are ever recording\\ntheir heart rates, movement habits, diet, and sleep patterns. Smart cars collect driving\\nhabits, smart homes collect living habits, and smart marketers collect purchasing\\nhabits. The Internet itself represents a huge graph of knowledge that contains (among\\nother things) an enormous cross-referenced encyclopedia; domain-specific databases\\nabout movies, music, sports results, pinball machines, memes, and cocktails; and too\\nmany government statistics (some of them nearly true!) from too many governments\\nto wrap your head around.\\nBuried in these data are answers to countless questions that no one’s ever thought to\\nask. In this book, we’ll learn how to find them.\\nWhat Is Data Science?\\nThere’s a joke that says a data scientist is someone who knows more statistics than a\\ncomputer scientist and more computer science than a statistician. (I didn’t say it was a\\ngood joke.) In fact, some data scientists are—for all practical purposes—statisticians,\\nwhile others are pretty much indistinguishable from software engineers. Some are\\nmachine-learning experts, while others couldn’t machine-learn their way out of kin‐\\ndergarten. Some are PhDs with impressive publication records, while others have\\nnever read an academic paper (shame on them, though). In short, pretty much no\\n1\\nwww.it-ebooks.infomatter how you define data science, you’ll find practitioners for whom the definition\\nis totally, absolutely wrong.\\nNonetheless, we won’t let that stop us from trying. We’ll say that a data scientist is\\nsomeone who extracts insights from messy data. Today’s world is full of people trying\\nto turn data into insight.\\nFor instance, the dating site OkCupid asks its members to answer thousands of ques‐\\ntions in order to find the most appropriate matches for them. But it also analyzes\\nthese results to figure out innocuous-sounding questions you can ask someone to\\nfind out how likely someone is to sleep with you on the first date .\\nFacebook asks you to list your hometown and your current location, ostensibly to\\nmake it easier for your friends to find and connect with you. But it also analyzes these\\nlocations to identify global migration patterns  and where the fanbases of different\\nfootball teams live .\\nAs a large retailer, Target tracks your purchases and interactions, both online and in-\\nstore. And it uses the data to predictively model  which of its customers are pregnant,\\nto better market baby-related purchases to them.\\nIn 2012, the Obama campaign employed dozens of data scientists who data-mined\\nand experimented their way to identifying voters who needed extra attention, choos‐\\ning optimal donor-specific fundraising appeals and programs, and focusing get-out-\\nthe-vote efforts where they were most likely to be useful. It is generally agreed that\\nthese efforts played an important role in the president’s re-election, which means it is\\na safe bet that political campaigns of the future will become more and more data-\\ndriven, resulting in a never-ending arms race of data science and data collection.\\nNow, before you start feeling too jaded: some data scientists also occasionally use\\ntheir skills for good— using data to make government more effective , to help the\\nhomeless , and to improve public health . But it certainly won’t hurt your career if you\\nlike figuring out the best way to get people to click on advertisements.\\nMotivating Hypothetical: DataSciencester\\nCongratulations! Y ou’ve just been hired to lead the data science efforts at DataScien‐\\ncester, the social network for data scientists.\\nDespite being for data scientists, DataSciencester has never actually invested in build‐\\ning its own data science practice. (In fairness, DataSciencester has never really inves‐\\nted in building its product either.) That will be your job! Throughout the book, we’ll\\nbe learning about data science concepts by solving problems that you encounter at\\nwork. Sometimes we’ll look at data explicitly supplied by users, sometimes we’ll look\\nat data generated through their interactions with the site, and sometimes we’ll even\\nlook at data from experiments that we’ll design.\\n2 | Chapter 1: Introduction\\nwww.it-ebooks.infoAnd because DataSciencester has a strong “not-invented-here” mentality, we’ll be\\nbuilding our own tools from scratch. At the end, you’ll have a pretty solid under‐\\nstanding of the fundamentals of data science. And you’ll be ready to apply your skills\\nat a company with a less shaky premise, or to any other problems that happen to\\ninterest you.\\nWelcome aboard, and good luck! (Y ou’re allowed to wear jeans on Fridays, and the\\nbathroom is down the hall on the right.)\\nFinding Key Connectors\\nIt’s your first day on the job at DataSciencester, and the VP of Networking is full of\\nquestions about your users. Until now he’s had no one to ask, so he’s very excited to\\nhave you aboard.\\nIn particular, he wants you to identify who the “key connectors” are among data sci‐\\nentists. To this end, he gives you a dump of the entire DataSciencester network. (In\\nreal life, people don’t typically hand you the data you need. Chapter 9  is devoted to\\ngetting data.)\\nWhat does this data dump look like? It consists of a list of users, each represented by a\\ndict  that contains for each user his or her id (which is a number) and name  (which,\\nin one of the great cosmic coincidences, rhymes with the user’s id):\\nusers = [\\n    { \"id\": 0, \"name\": \"Hero\" },\\n    { \"id\": 1, \"name\": \"Dunn\" },\\n    { \"id\": 2, \"name\": \"Sue\" },\\n    { \"id\": 3, \"name\": \"Chi\" },\\n    { \"id\": 4, \"name\": \"Thor\" },\\n    { \"id\": 5, \"name\": \"Clive\" },\\n    { \"id\": 6, \"name\": \"Hicks\" },\\n    { \"id\": 7, \"name\": \"Devin\" },\\n    { \"id\": 8, \"name\": \"Kate\" },\\n    { \"id\": 9, \"name\": \"Klein\" }\\n]\\nHe also gives you the “friendship” data, represented as a list of pairs of IDs:\\nfriendships  = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\\n               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\\nFor example, the tuple (0, 1)  indicates that the data scientist with id 0 (Hero) and\\nthe data scientist with id 1 (Dunn) are friends. The network is illustrated in\\nFigure 1-1 .\\nMotivating Hypothetical: DataSciencester | 3\\nwww.it-ebooks.infoFigure 1-1. The DataSciencester network\\nSince we represented our users as dict s, it’s easy to augment them with extra data.\\nDon’t get too hung up on the details of the code right now. In\\nChapter 2 , we’ll take you through a crash course in Python. For\\nnow just try to get the general flavor of what we’re doing.\\nFor example, we might want to add a list of friends to each user. First we set each\\nuser’s friends  property to an empty list:\\nfor user in users:\\n    user[\"friends\" ] = []\\nAnd then we populate the lists using the friendships  data:\\nfor i, j in friendships :\\n    # this works because users[i] is the user whose id is i\\n    users[i][\"friends\" ].append(users[j]) # add i as a friend of j\\n    users[j][\"friends\" ].append(users[i]) # add j as a friend of i\\nOnce each user  dict  contains a list of friends, we can easily ask questions of our\\ngraph, like “what’s the average number of connections?”\\nFirst we find the total  number of connections, by summing up the lengths of all the\\nfriends  lists:\\ndef number_of_friends (user):\\n    \"\"\"how many friends does _user_ have?\"\"\"\\n    return len(user[\"friends\" ])                   # length of friend_ids list\\ntotal_connections  = sum(number_of_friends (user)\\n                        for user in users)        # 24\\nAnd then we just divide by the number of users:\\n4 | Chapter 1: Introduction\\nwww.it-ebooks.infofrom __future__  import division                    # integer division is lame\\nnum_users  = len(users)                            # length of the users list\\navg_connections  = total_connections  / num_users    # 2.4\\nIt’s also easy to find the most connected people—they’re the people who have the larg‐\\nest number of friends.\\nSince there aren’t very many users, we can sort them from “most friends” to “least\\nfriends”:\\n# create a list (user_id, number_of_friends)\\nnum_friends_by_id  = [(user[\"id\"], number_of_friends (user))\\n                     for user in users]\\nsorted(num_friends_by_id ,                                # get it sorted\\n       key=lambda (user_id, num_friends ): num_friends ,   # by num_friends\\n       reverse=True)                                     # largest to smallest\\n# each pair is (user_id, num_friends)\\n# [(1, 3), (2, 3), (3, 3), (5, 3), (8, 3),\\n#  (0, 2), (4, 2), (6, 2), (7, 2), (9, 1)]\\nOne way to think of what we’ve done is as a way of identifying people who are some‐\\nhow central to the network. In fact, what we’ve just computed is the network  metric\\ndegree centrality  (Figure 1-2 ).\\nFigure 1-2. The DataSciencester network sized by degree\\nThis has the virtue of being pretty easy to calculate, but it doesn’t always give the\\nresults you’ d want or expect. For example, in the DataSciencester network Thor ( id 4)\\nonly has two connections while Dunn ( id 1) has three. Y et looking at the network it\\nintuitively seems like Thor should be more central. In Chapter 21 , we’ll investigate\\nnetworks in more detail, and we’ll look at more complex notions of centrality that\\nmay or may not accord better with our intuition.\\nMotivating Hypothetical: DataSciencester | 5\\nwww.it-ebooks.infoData Scientists You May Know\\nWhile you’re still filling out new-hire paperwork, the VP of Fraternization comes by\\nyour desk. She wants to encourage more connections among your members, and she\\nasks you to design a “Data Scientists Y ou May Know” suggester.\\nY our first instinct is to suggest that a user might know the friends of friends. These\\nare easy to compute: for each of a user’s friends, iterate over that person’s friends, and\\ncollect all the results:\\ndef friends_of_friend_ids_bad (user):\\n    # \"foaf\" is short for \"friend of a friend\"\\n    return [foaf[\"id\"]\\n            for friend in user[\"friends\" ]     # for each of user\\'s friends\\n            for foaf in friend[\"friends\" ]]    # get each of _their_ friends\\nWhen we call this on users[0]  (Hero), it produces:\\n[0, 2, 3, 0, 1, 3]\\nIt includes user 0 (twice), since Hero is indeed friends with both of his friends. It\\nincludes users 1 and 2, although they are both friends with Hero already. And it\\nincludes user 3 twice, as Chi is reachable through two different friends:\\nprint [friend[\"id\"] for friend in users[0][\"friends\" ]]  # [1, 2]\\nprint [friend[\"id\"] for friend in users[1][\"friends\" ]]  # [0, 2, 3]\\nprint [friend[\"id\"] for friend in users[2][\"friends\" ]]  # [0, 1, 3]\\nKnowing that people are friends-of-friends in multiple ways seems like interesting\\ninformation, so maybe instead we should produce a count  of mutual friends. And we\\ndefinitely should use a helper function to exclude people already known to the user:\\nfrom collections  import Counter                       # not loaded by default\\ndef not_the_same (user, other_user ):\\n    \"\"\"two users are not the same if they have different ids\"\"\"\\n    return user[\"id\"] != other_user [\"id\"]\\ndef not_friends (user, other_user ):\\n    \"\"\"other_user is not a friend if he\\'s not in user[\"friends\"];\\n    that is, if he\\'s not_the_same as all the people in user[\"friends\"]\"\"\"\\n    return all(not_the_same (friend, other_user )\\n               for friend in user[\"friends\" ])\\ndef friends_of_friend_ids (user):\\n    return Counter(foaf[\"id\"]\\n                   for friend in user[\"friends\" ]    # for each of my friends\\n                   for foaf in friend[\"friends\" ]    # count *their* friends\\n                   if not_the_same (user, foaf)      # who aren\\'t me\\n                   and not_friends (user, foaf))     # and aren\\'t my friends\\nprint friends_of_friend_ids (users[3])               # Counter({0: 2, 5: 1})\\n6 | Chapter 1: Introduction\\nwww.it-ebooks.infoThis correctly tells Chi ( id 3) that she has two mutual friends with Hero ( id 0) but\\nonly one mutual friend with Clive ( id 5).\\nAs a data scientist, you know that you also might enjoy meeting users with similar\\ninterests. (This is a good example of the “substantive expertise” aspect of data sci‐\\nence.) After asking around, you manage to get your hands on this data, as a list of\\npairs (user_id, interest) :\\ninterests  = [\\n    (0, \"Hadoop\" ), (0, \"Big Data\" ), (0, \"HBase\"), (0, \"Java\"),\\n    (0, \"Spark\"), (0, \"Storm\"), (0, \"Cassandra\" ),\\n    (1, \"NoSQL\"), (1, \"MongoDB\" ), (1, \"Cassandra\" ), (1, \"HBase\"),\\n    (1, \"Postgres\" ), (2, \"Python\" ), (2, \"scikit-learn\" ), (2, \"scipy\"),\\n    (2, \"numpy\"), (2, \"statsmodels\" ), (2, \"pandas\" ), (3, \"R\"), (3, \"Python\" ),\\n    (3, \"statistics\" ), (3, \"regression\" ), (3, \"probability\" ),\\n    (4, \"machine learning\" ), (4, \"regression\" ), (4, \"decision trees\" ),\\n    (4, \"libsvm\" ), (5, \"Python\" ), (5, \"R\"), (5, \"Java\"), (5, \"C++\"),\\n    (5, \"Haskell\" ), (5, \"programming languages\" ), (6, \"statistics\" ),\\n    (6, \"probability\" ), (6, \"mathematics\" ), (6, \"theory\" ),\\n    (7, \"machine learning\" ), (7, \"scikit-learn\" ), (7, \"Mahout\" ),\\n    (7, \"neural networks\" ), (8, \"neural networks\" ), (8, \"deep learning\" ),\\n    (8, \"Big Data\" ), (8, \"artificial intelligence\" ), (9, \"Hadoop\" ),\\n    (9, \"Java\"), (9, \"MapReduce\" ), (9, \"Big Data\" )\\n]\\nFor example, Thor ( id 4) has no friends in common with Devin ( id 7), but they share\\nan interest in machine learning.\\nIt’s easy to build a function that finds users with a certain interest:\\ndef data_scientists_who_like (target_interest ):\\n    return [user_id\\n            for user_id, user_interest  in interests\\n            if user_interest  == target_interest ]\\nThis works, but it has to examine the whole list of interests for every search. If we\\nhave a lot of users and interests (or if we just want to do a lot of searches), we’re prob‐\\nably better off building an index from interests to users:\\nfrom collections  import defaultdict\\n# keys are interests, values are lists of user_ids with that interest\\nuser_ids_by_interest  = defaultdict (list)\\nfor user_id, interest  in interests :\\n    user_ids_by_interest [interest ].append(user_id)\\nAnd another from users to interests:\\n# keys are user_ids, values are lists of interests for that user_id\\ninterests_by_user_id  = defaultdict (list)\\nMotivating Hypothetical: DataSciencester | 7\\nwww.it-ebooks.infofor user_id, interest  in interests :\\n    interests_by_user_id [user_id].append(interest )\\nNow it’s easy to find who has the most interests in common with a given user:\\n•Iterate over the user’s interests.\\n•For each interest, iterate over the other users with that interest.\\n•Keep count of how many times we see each other user.\\ndef most_common_interests_with (user):\\n    return Counter(interested_user_id\\n        for interest  in interests_by_user_id [user[\"id\"]]\\n        for interested_user_id  in user_ids_by_interest [interest ]\\n        if interested_user_id  != user[\"id\"])\\nWe could then use this to build a richer “Data Scientists Y ou Should Know” feature\\nbased on a combination of mutual friends and mutual interests. We’ll explore these\\nkinds of applications in Chapter 22 .\\nSalaries and Experience\\nRight as you’re about to head to lunch, the VP of Public Relations asks if you can pro‐\\nvide some fun facts about how much data scientists earn. Salary data is of course sen‐\\nsitive, but he manages to provide you an anonymous data set containing each user’s\\nsalary  (in dollars) and tenure  as a data scientist (in years):\\nsalaries_and_tenures  = [(83000, 8.7), (88000, 8.1),\\n                        (48000, 0.7), (76000, 6),\\n                        (69000, 6.5), (76000, 7.5),\\n                        (60000, 2.5), (83000, 10),\\n                        (48000, 1.9), (63000, 4.2)]\\nThe natural first step is to plot the data (which we’ll see how to do in Chapter 3 ). Y ou\\ncan see the results in Figure 1-3 .\\n8 | Chapter 1: Introduction\\nwww.it-ebooks.infoFigure 1-3. Salary by years of experience\\nIt seems pretty clear that people with more experience tend to earn more. How can\\nyou turn this into a fun fact? Y our first idea is to look at the average salary for each\\ntenure:\\n# keys are years, values are lists of the salaries for each tenure\\nsalary_by_tenure  = defaultdict (list)\\nfor salary, tenure in salaries_and_tenures :\\n    salary_by_tenure [tenure].append(salary)\\n# keys are years, each value is average salary for that tenure\\naverage_salary_by_tenure  = {\\n    tenure : sum(salaries ) / len(salaries )\\n    for tenure, salaries  in salary_by_tenure .items()\\n}\\nThis turns out to be not particularly useful, as none of the users have the same tenure,\\nwhich means we’re just reporting the individual users’ salaries:\\n{0.7: 48000.0,\\n 1.9: 48000.0,\\n 2.5: 60000.0,\\n 4.2: 63000.0,\\nMotivating Hypothetical: DataSciencester | 9\\nwww.it-ebooks.info 6: 76000.0,\\n 6.5: 69000.0,\\n 7.5: 76000.0,\\n 8.1: 88000.0,\\n 8.7: 83000.0,\\n 10: 83000.0}\\nIt might be more helpful to bucket the tenures:\\ndef tenure_bucket (tenure):\\n    if tenure < 2:\\n        return \"less than two\"\\n    elif tenure < 5:\\n        return \"between two and five\"\\n    else:\\n        return \"more than five\"\\nThen group together the salaries corresponding to each bucket:\\n# keys are tenure buckets, values are lists of salaries for that bucket\\nsalary_by_tenure_bucket  = defaultdict (list)\\nfor salary, tenure in salaries_and_tenures :\\n    bucket = tenure_bucket (tenure)\\n    salary_by_tenure_bucket [bucket].append(salary)\\nAnd finally compute the average salary for each group:\\n# keys are tenure buckets, values are average salary for that bucket\\naverage_salary_by_bucket  = {\\n  tenure_bucket  : sum(salaries ) / len(salaries )\\n  for tenure_bucket , salaries  in salary_by_tenure_bucket .iteritems ()\\n}\\nwhich is more interesting:\\n{\\'between two and five\\' : 61500.0,\\n \\'less than two\\' : 48000.0,\\n \\'more than five\\' : 79166.66666666667 }\\nAnd you have your soundbite: “Data scientists with more than five years experience\\nearn 65% more than data scientists with little or no experience!”\\nBut we chose the buckets in a pretty arbitrary way. What we’ d really like is to make\\nsome sort of statement about the salary effect—on average—of having an additional\\nyear of experience. In addition to making for a snappier fun fact, this allows us to\\nmake predictions  about salaries that we don’t know. We’ll explore this idea in Chap‐\\nter 14 .\\n10 | Chapter 1: Introduction\\nwww.it-ebooks.infoPaid Accounts\\nWhen you get back to your desk, the VP of Revenue is waiting for you. She wants to\\nbetter understand which users pay for accounts and which don’t. (She knows their\\nnames, but that’s not particularly actionable information.)\\nY ou notice that there seems to be a correspondence between years of experience and\\npaid accounts:\\n0.7 paid\\n1.9 unpaid\\n2.5 paid\\n4.2 unpaid\\n6   unpaid\\n6.5 unpaid\\n7.5 unpaid\\n8.1 unpaid\\n8.7 paid\\n10  paid\\nUsers with very few and very many years of experience tend to pay; users with aver‐\\nage amounts of experience don’t.\\nAccordingly, if you wanted to create a model—though this is definitely not enough\\ndata to base a model on—you might try to predict “paid” for users with very few and\\nvery many years of experience, and “unpaid” for users with middling amounts of\\nexperience:\\ndef predict_paid_or_unpaid (years_experience ):\\n  if years_experience  < 3.0:\\n    return \"paid\"\\n  elif years_experience  < 8.5:\\n    return \"unpaid\"\\n  else:\\n    return \"paid\"\\nOf course, we totally eyeballed the cutoffs.\\nWith more data (and more mathematics), we could build a model predicting the like‐\\nlihood that a user would pay, based on his years of experience. We’ll investigate this\\nsort of problem in Chapter 16 .\\nTopics of Interest\\nAs you’re wrapping up your first day, the VP of Content Strategy asks you for data\\nabout what topics users are most interested in, so that she can plan out her blog cal‐\\nendar accordingly. Y ou already have the raw data from the friend-suggester project:\\ninterests  = [\\n    (0, \"Hadoop\" ), (0, \"Big Data\" ), (0, \"HBase\"), (0, \"Java\"),\\n    (0, \"Spark\"), (0, \"Storm\"), (0, \"Cassandra\" ),\\nMotivating Hypothetical: DataSciencester | 11\\nwww.it-ebooks.info    (1, \"NoSQL\"), (1, \"MongoDB\" ), (1, \"Cassandra\" ), (1, \"HBase\"),\\n    (1, \"Postgres\" ), (2, \"Python\" ), (2, \"scikit-learn\" ), (2, \"scipy\"),\\n    (2, \"numpy\"), (2, \"statsmodels\" ), (2, \"pandas\" ), (3, \"R\"), (3, \"Python\" ),\\n    (3, \"statistics\" ), (3, \"regression\" ), (3, \"probability\" ),\\n    (4, \"machine learning\" ), (4, \"regression\" ), (4, \"decision trees\" ),\\n    (4, \"libsvm\" ), (5, \"Python\" ), (5, \"R\"), (5, \"Java\"), (5, \"C++\"),\\n    (5, \"Haskell\" ), (5, \"programming languages\" ), (6, \"statistics\" ),\\n    (6, \"probability\" ), (6, \"mathematics\" ), (6, \"theory\" ),\\n    (7, \"machine learning\" ), (7, \"scikit-learn\" ), (7, \"Mahout\" ),\\n    (7, \"neural networks\" ), (8, \"neural networks\" ), (8, \"deep learning\" ),\\n    (8, \"Big Data\" ), (8, \"artificial intelligence\" ), (9, \"Hadoop\" ),\\n    (9, \"Java\"), (9, \"MapReduce\" ), (9, \"Big Data\" )\\n]\\nOne simple (if not particularly exciting) way to find the most popular interests is sim‐\\nply to count the words:\\n1.Lowercase each interest (since different users may or may not capitalize their\\ninterests).\\n2.Split it into words.\\n3.Count the results.\\nIn code:\\nwords_and_counts  = Counter(word\\n                           for user, interest  in interests\\n                           for word in interest .lower().split())\\nThis makes it easy to list out the words that occur more than once:\\nfor word, count in words_and_counts .most_common ():\\n    if count > 1:\\n        print word, count\\nwhich gives the results you’ d expect (unless you expect “scikit-learn” to get split into\\ntwo words, in which case it doesn’t give the results you expect):\\nlearning 3\\njava 3\\npython 3\\nbig 3\\ndata 3\\nhbase 2\\nregression 2\\ncassandra 2\\nstatistics 2\\nprobability 2\\nhadoop 2\\nnetworks 2\\nmachine 2\\nneural 2\\n12 | Chapter 1: Introduction\\nwww.it-ebooks.infoscikit-learn 2\\nr 2\\nWe’ll look at more sophisticated ways to extract topics from data in Chapter 20 .\\nOnward\\nIt’s been a successful first day! Exhausted, you slip out of the building before anyone\\nelse can ask you for anything else. Get a good night’s rest, because tomorrow is new\\nemployee orientation. (Y es, you went through a full day of work before  new employee\\norientation. Take it up with HR.)\\nMotivating Hypothetical: DataSciencester | 13\\nwww.it-ebooks.infowww.it-ebooks.infoCHAPTER 2\\nA Crash Course in Python\\nPeople are still crazy about Python after  twenty-five  years, which I find hard to believe.\\n—Michael Palin\\nAll new employees at DataSciencester are required to go through new employee ori‐\\nentation, the most interesting part of which is a crash course in Python.\\nThis is not a comprehensive Python tutorial but instead is intended to highlight the\\nparts of the language that will be most important to us (some of which are often not\\nthe focus of Python tutorials).\\nThe Basics\\nGetting Python\\nY ou can download Python from python.org . But if you don’t already have Python, I\\nrecommend instead installing the Anaconda  distribution, which already includes\\nmost of the libraries that you need to do data science.\\nAs I write this, the latest version of Python is 3.4. At DataSciencester, however, we use\\nold, reliable Python 2.7. Python 3 is not backward-compatible with Python 2, and\\nmany important libraries only work well with 2.7. The data science community is still\\nfirmly stuck on 2.7, which means we will be, too. Make sure to get that version.\\nIf you don’t get Anaconda, make sure to install pip, which is a Python package man‐\\nager that allows you to easily install third-party packages (some of which we’ll need). \\nIt’s also worth getting IPython , which is a much nicer Python shell to work with.\\n(If you installed Anaconda then it should have come with pip and IPython.)\\nJust run:\\n15\\nwww.it-ebooks.infopip install ipython\\nand then search the Internet for solutions to whatever cryptic error messages that\\ncauses.\\nThe Zen of Python\\nPython has a somewhat Zen description of its design principles , which you can also\\nfind inside the Python interpreter itself by typing import this .\\nOne of the most discussed of these is:\\nThere should be one—and preferably only one—obvious way to do it.\\nCode written in accordance with this “obvious” way (which may not be obvious at all\\nto a newcomer) is often described as “Pythonic. ” Although this is not a book about\\nPython, we will occasionally contrast Pythonic and non-Pythonic ways of accom‐\\nplishing the same things, and we will generally favor Pythonic solutions to our prob‐\\nlems.\\nWhitespace Formatting\\nMany languages use curly braces to delimit blocks of code. Python uses indentation:\\nfor i in [1, 2, 3, 4, 5]:\\n    print i                     # first line in \"for i\" block\\n    for j in [1, 2, 3, 4, 5]:\\n        print j                 # first line in \"for j\" block\\n        print i + j             # last line in \"for j\" block\\n    print i                     # last line in \"for i\" block\\nprint \"done looping\"\\nThis makes Python code very readable, but it also means that you have to be very\\ncareful with your formatting. Whitespace is ignored inside parentheses and brackets,\\nwhich can be helpful for long-winded computations:\\nlong_winded_computation  = (1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 + 12 +\\n                           13 + 14 + 15 + 16 + 17 + 18 + 19 + 20)\\nand for making code easier to read:\\nlist_of_lists  = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\\neasier_to_read_list_of_lists  = [ [1, 2, 3],\\n                                 [4, 5, 6],\\n                                 [7, 8, 9] ]\\nY ou can also use a backslash to indicate that a statement continues onto the next line,\\nalthough we’ll rarely do this:\\ntwo_plus_three  = 2 + \\\\\\n                 3\\n16 | Chapter 2: A Crash Course in Python\\nwww.it-ebooks.infoOne consequence of whitespace formatting is that it can be hard to copy and paste\\ncode into the Python shell. For example, if you tried to paste the code:\\nfor i in [1, 2, 3, 4, 5]:\\n    # notice the blank line\\n    print i\\ninto the ordinary Python shell, you would get a:\\nIndentationError : expected  an indented  block\\nbecause the interpreter thinks the blank line signals the end of the for loop’s block.\\nIPython has a magic function %paste , which correctly pastes whatever is on your\\nclipboard, whitespace and all. This alone is a good reason to use IPython.\\nModules\\nCertain features of Python are not loaded by default. These include both features\\nincluded as part of the language as well as third-party features that you download\\nyourself. In order to use these features, you’ll need to import  the modules that con‐\\ntain them.\\nOne approach is to simply import the module itself:\\nimport re\\nmy_regex  = re.compile(\"[0-9]+\" , re.I)\\nHere re is the module containing functions and constants for working with regular\\nexpressions. After this type of import  you can only access those functions by prefix‐\\ning them with re..\\nIf you already had a different re in your code you could use an alias:\\nimport re as regex\\nmy_regex  = regex.compile(\"[0-9]+\" , regex.I)\\nY ou might also do this if your module has an unwieldy name or if you’re going to be\\ntyping it a lot. For example, when visualizing data with matplotlib , a standard con‐\\nvention is:\\nimport matplotlib.pyplot  as plt\\nIf you need a few specific values from a module, you can import them explicitly and\\nuse them without qualification:\\nfrom collections  import defaultdict , Counter\\nlookup = defaultdict (int)\\nmy_counter  = Counter()\\nIf you were a bad person, you could import the entire contents of a module into your\\nnamespace, which might inadvertently overwrite variables you’ve already defined:\\nThe Basics | 17\\nwww.it-ebooks.infomatch = 10\\nfrom re import *    # uh oh, re has a match function\\nprint match         # \"<function re.match>\"\\nHowever, since you are not a bad person, you won’t ever do this.\\nArithmetic\\nPython 2.7 uses integer division by default,  so that 5 / 2  equals 2. Almost always this\\nis not what we want, so we will always start our files with:\\nfrom __future__  import division\\nafter which 5 / 2  equals 2.5. Every code example in this book uses this new-style\\ndivision. In the handful of cases where we need integer division, we can get it with a\\ndouble slash: 5 // 2 .\\nFunctions\\nA function is a rule for taking zero or more inputs and returning a corresponding\\noutput. In Python, we typically define functions using def:\\ndef double(x):\\n    \"\"\"this is where you put an optional docstring\\n    that explains what the function does.\\n    for example, this function multiplies its input by 2\"\"\"\\n    return x * 2\\nPython functions are first-class , which means that we can assign them to variables\\nand pass them into functions just like any other arguments:\\ndef apply_to_one (f):\\n    \"\"\"calls the function f with 1 as its argument\"\"\"\\n    return f(1)\\nmy_double  = double             # refers to the previously defined function\\nx = apply_to_one (my_double )    # equals 2\\nIt is also easy to create short anonymous functions, or lambdas:\\ny = apply_to_one (lambda x: x + 4)      # equals 5\\nY ou can assign lambdas to variables, although most people will tell you that you\\nshould just use def instead:\\nanother_double  = lambda x: 2 * x       # don\\'t do this\\ndef another_double (x): return 2 * x    # do this instead\\nFunction parameters can also be given default arguments, which only need to be\\nspecified when you want a value other than the default:\\ndef my_print (message=\"my default message\" ):\\n    print message\\n18 | Chapter 2: A Crash Course in Python\\nwww.it-ebooks.infomy_print (\"hello\")   # prints \\'hello\\'\\nmy_print ()          # prints \\'my default message\\'\\nIt is sometimes useful to specify arguments by name:\\ndef subtract (a=0, b=0):\\n    return a - b\\nsubtract (10, 5) # returns 5\\nsubtract (0, 5)  # returns -5\\nsubtract (b=5)   # same as previous\\nWe will be creating many, many functions.\\nStrings\\nStrings can be delimited by single or double quotation marks (but the quotes have to\\nmatch):\\nsingle_quoted_string  = \\'data science\\'\\ndouble_quoted_string  = \"data science\"\\nPython uses backslashes to encode special characters. For example:\\ntab_string  = \"\\\\t\"       # represents the tab character\\nlen(tab_string )         # is 1\\nIf you want backslashes as backslashes (which you might in Windows directory\\nnames or in regular expressions), you can create raw strings using r\"\":\\nnot_tab_string  = r\"\\\\t\"  # represents the characters \\'\\\\\\' and \\'t\\'\\nlen(not_tab_string )     # is 2\\nY ou can create multiline strings using triple-[double-]-quotes:\\nmulti_line_string  = \"\"\"This is the first line.\\nand this is the second line\\nand this is the third line\"\"\"\\nExceptions\\nWhen something goes wrong, Python raises an exception . Unhandled, these will cause\\nyour program to crash. Y ou can handle them using try and except :\\ntry:\\n    print 0 / 0\\nexcept ZeroDivisionError :\\n    print \"cannot divide by zero\"\\nAlthough in many languages exceptions are considered bad, in Python there is no\\nshame in using them to make your code cleaner, and we will occasionally do so.\\nThe Basics | 19\\nwww.it-ebooks.infoLists\\nProbably the most fundamental data structure in Python is the list . A list is simply\\nan ordered collection. (It is similar to what in other languages might be called an\\narray, but with some added functionality.)\\ninteger_list  = [1, 2, 3]\\nheterogeneous_list  = [\"string\" , 0.1, True]\\nlist_of_lists  = [ integer_list , heterogeneous_list , [] ]\\nlist_length  = len(integer_list )     # equals 3\\nlist_sum     = sum(integer_list )     # equals 6\\nY ou can get or set the nth element of a list with square brackets:\\nx = range(10)   # is the list [0, 1, ..., 9]\\nzero = x[0]     # equals 0, lists are 0-indexed\\none = x[1]      # equals 1\\nnine = x[-1]    # equals 9, \\'Pythonic\\' for last element\\neight = x[-2]   # equals 8, \\'Pythonic\\' for next-to-last element\\nx[0] = -1       # now x is [-1, 1, 2, 3, ..., 9]\\nY ou can also use square brackets to “slice” lists:\\nfirst_three    = x[:3]               # [-1, 1, 2]\\nthree_to_end  = x[3:]                # [3, 4, ..., 9]\\none_to_four  = x[1:5]                # [1, 2, 3, 4]\\nlast_three  = x[-3:]                 # [7, 8, 9]\\nwithout_first_and_last  = x[1:-1]    # [1, 2, ..., 8]\\ncopy_of_x  = x[:]                    # [-1, 1, 2, ..., 9]\\nPython has an in operator to check for list membership:\\n1 in [1, 2, 3]    # True\\n0 in [1, 2, 3]    # False\\nThis check involves examining the elements of the list one at a time, which means\\nthat you probably shouldn’t use it unless you know your list is pretty small (or unless\\nyou don’t care how long the check takes).\\nIt is easy to concatenate lists together:\\nx = [1, 2, 3]\\nx.extend([4, 5, 6])     # x is now [1,2,3,4,5,6]\\nIf you don’t want to modify x you can use list addition:\\nx = [1, 2, 3]\\ny = x + [4, 5, 6]       # y is [1, 2, 3, 4, 5, 6]; x is unchanged\\nMore frequently we will append to lists one item at a time:\\nx = [1, 2, 3]\\nx.append(0)      # x is now [1, 2, 3, 0]\\n20 | Chapter 2: A Crash Course in Python\\nwww.it-ebooks.infoy = x[-1]        # equals 0\\nz = len(x)       # equals 4\\nIt is often convenient to unpack  lists if you know how many elements they contain:\\nx, y = [1, 2]    # now x is 1, y is 2\\nalthough you will get a ValueError  if you don’t have the same numbers of elements\\non both sides.\\nIt’s common to use an underscore for a value you’re going to throw away:\\n_, y = [1, 2]    # now y == 2, didn\\'t care about the first element\\nTuples\\nTuples are lists’ immutable cousins. Pretty much anything you can do to a list that\\ndoesn’t involve modifying it, you can do to a tuple. Y ou specify a tuple by using\\nparentheses (or nothing) instead of square brackets:\\nmy_list = [1, 2]\\nmy_tuple  = (1, 2)\\nother_tuple  = 3, 4\\nmy_list[1] = 3      # my_list is now [1, 3]\\ntry:\\n    my_tuple [1] = 3\\nexcept TypeError :\\n    print \"cannot modify a tuple\"\\nTuples are a convenient way to return multiple values from functions:\\ndef sum_and_product (x, y):\\n    return (x + y),(x * y)\\nsp = sum_and_product (2, 3)    # equals (5, 6)\\ns, p = sum_and_product (5, 10) # s is 15, p is 50\\nTuples (and lists) can also be used for multiple assignment :\\nx, y = 1, 2     # now x is 1, y is 2\\nx, y = y, x     # Pythonic way to swap variables; now x is 2, y is 1\\nDictionaries\\nAnother fundamental data structure is a dictionary, which associates values  with keys\\nand allows you to quickly retrieve the value corresponding to a given key:\\nempty_dict  = {}                         # Pythonic\\nempty_dict2  = dict()                    # less Pythonic\\ngrades = { \"Joel\" : 80, \"Tim\" : 95 }    # dictionary literal\\nY ou can look up the value for a key using square brackets:\\nThe Basics | 21\\nwww.it-ebooks.infojoels_grade  = grades[\"Joel\"]            # equals 80\\nBut you’ll get a KeyError  if you ask for a key that’s not in the dictionary:\\ntry:\\n    kates_grade  = grades[\"Kate\"]\\nexcept KeyError :\\n    print \"no grade for Kate!\"\\nY ou can check for the existence of a key using in:\\njoel_has_grade  = \"Joel\" in grades     # True\\nkate_has_grade  = \"Kate\" in grades     # False\\nDictionaries have a get method that returns a default value (instead of raising an\\nexception) when you look up a key that’s not in the dictionary:\\njoels_grade  = grades.get(\"Joel\", 0)   # equals 80\\nkates_grade  = grades.get(\"Kate\", 0)   # equals 0\\nno_ones_grade  = grades.get(\"No One\" )  # default default is None\\nY ou assign key-value pairs using the same square brackets:\\ngrades[\"Tim\"] = 99                    # replaces the old value\\ngrades[\"Kate\"] = 100                  # adds a third entry\\nnum_students  = len(grades)            # equals 3\\nWe will frequently use dictionaries as a simple way to represent structured data:\\ntweet = {\\n    \"user\" : \"joelgrus\" ,\\n    \"text\" : \"Data Science is Awesome\" ,\\n    \"retweet_count\"  : 100,\\n    \"hashtags\"  : [\"#data\", \"#science\" , \"#datascience\" , \"#awesome\" , \"#yolo\"]\\n}\\nBesides looking for specific keys we can look at all of them:\\ntweet_keys    = tweet.keys()     # list of keys\\ntweet_values  = tweet.values()   # list of values\\ntweet_items   = tweet.items()    # list of (key, value) tuples\\n\"user\" in tweet_keys             # True, but uses a slow list in\\n\"user\" in tweet                 # more Pythonic, uses faster dict in\\n\"joelgrus\"  in tweet_values       # True\\nDictionary keys must be immutable; in particular, you cannot use list s as keys. If\\nyou need a multipart key, you should use a tuple  or figure out a way to turn the key\\ninto a string.\\ndefaultdict\\nImagine that you’re trying to count the words in a document.  An obvious approach is\\nto create a dictionary in which the keys are words and the values are counts. As you\\n22 | Chapter 2: A Crash Course in Python\\nwww.it-ebooks.infocheck each word, you can increment its count if it’s already in the dictionary and add\\nit to the dictionary if it’s not:\\nword_counts  = {}\\nfor word in document :\\n    if word in word_counts :\\n        word_counts [word] += 1\\n    else:\\n        word_counts [word] = 1\\nY ou could also use the “forgiveness is better than permission” approach and just han‐\\ndle the exception from trying to look up a missing key:\\nword_counts  = {}\\nfor word in document :\\n    try:\\n        word_counts [word] += 1\\n    except KeyError :\\n        word_counts [word] = 1\\nA third approach is to use get, which behaves gracefully for missing keys:\\nword_counts  = {}\\nfor word in document :\\n    previous_count  = word_counts .get(word, 0)\\n    word_counts [word] = previous_count  + 1\\nEvery one of these is slightly unwieldy, which is why defaultdict  is useful. A\\ndefaultdict  is like a regular dictionary, except that when you try to look up a key it\\ndoesn’t contain, it first adds a value for it using a zero-argument function you pro‐\\nvided when you created it. In order to use defaultdict s, you have to import them\\nfrom collections :\\nfrom collections  import defaultdict\\nword_counts  = defaultdict (int)          # int() produces 0\\nfor word in document :\\n    word_counts [word] += 1\\nThey can also be useful with list  or dict  or even your own functions:\\ndd_list = defaultdict (list)             # list() produces an empty list\\ndd_list[2].append(1)                    # now dd_list contains {2: [1]}\\ndd_dict = defaultdict (dict)             # dict() produces an empty dict\\ndd_dict[\"Joel\"][\"City\"] = \"Seattle\"      # { \"Joel\" : { \"City\" : Seattle\"}}\\ndd_pair = defaultdict (lambda: [0, 0])\\ndd_pair[2][1] = 1                       # now dd_pair contains {2: [0,1]}\\nThese will be useful when we’re using dictionaries to “collect” results by some key and\\ndon’t want to have to check every time to see if the key exists yet.\\nThe Basics | 23\\nwww.it-ebooks.infoCounter\\nA Counter  turns a sequence of values into a defaultdict(int) -like object mapping\\nkeys to counts. We will primarily use it to create histograms:\\nfrom collections  import Counter\\nc = Counter([0, 1, 2, 0])          # c is (basically) { 0 : 2, 1 : 1, 2 : 1 }\\nThis gives us a very simple way to solve our word_counts  problem:\\nword_counts  = Counter(document )\\nA Counter  instance has a most_common  method that is frequently useful:\\n# print the 10 most common words and their counts\\nfor word, count in word_counts .most_common (10):\\n    print word, count\\nSets\\nAnother data structure is set, which represents a collection of distinct  elements:\\ns = set()\\ns.add(1)       # s is now { 1 }\\ns.add(2)       # s is now { 1, 2 }\\ns.add(2)       # s is still { 1, 2 }\\nx = len(s)     # equals 2\\ny = 2 in s     # equals True\\nz = 3 in s     # equals False\\nWe’ll use sets for two main reasons.  The first is that in is a very fast operation on sets.\\nIf we have a large collection of items that we want to use for a membership test, a set\\nis more appropriate than a list:\\nstopwords_list  = [\"a\",\"an\",\"at\"] + hundreds_of_other_words  + [\"yet\", \"you\"]\\n\"zip\" in stopwords_list      # False, but have to check every element\\nstopwords_set  = set(stopwords_list )\\n\"zip\" in stopwords_set       # very fast to check\\nThe second reason is to find the distinct  items in a collection:\\nitem_list  = [1, 2, 3, 1, 2, 3]\\nnum_items  = len(item_list )                # 6\\nitem_set  = set(item_list )                 # {1, 2, 3}\\nnum_distinct_items  = len(item_set )        # 3\\ndistinct_item_list  = list(item_set )       # [1, 2, 3]\\nWe’ll use sets much less frequently than dict s and list s.\\n24 | Chapter 2: A Crash Course in Python\\nwww.it-ebooks.infoControl Flow\\nAs in most programming languages, you can perform an action conditionally using\\nif:\\nif 1 > 2:\\n    message = \"if only 1 were greater than two...\"\\nelif 1 > 3:\\n    message = \"elif stands for \\'else if\\'\"\\nelse:\\n    message = \"when all else fails use else (if you want to)\"\\nY ou can also write a ternary  if-then-else on one line, which we will do occasionally:\\nparity = \"even\" if x % 2 == 0 else \"odd\"\\nPython has a while  loop:\\nx = 0\\nwhile x < 10:\\n    print x, \"is less than 10\"\\n    x += 1\\nalthough more often we’ll use for and in:\\nfor x in range(10):\\n    print x, \"is less than 10\"\\nIf you need more-complex logic, you can use continue  and break :\\nfor x in range(10):\\n    if x == 3:\\n        continue   # go immediately to the next iteration\\n    if x == 5:\\n        break     # quit the loop entirely\\n    print x\\nThis will print 0, 1, 2, and 4.\\nTruthiness\\nBooleans in Python work as in most other languages, except that they’re capitalized:\\none_is_less_than_two  = 1 < 2          # equals True\\ntrue_equals_false  = True == False     # equals False\\nPython uses the value None  to indicate a nonexistent value. It is similar to other lan‐\\nguages’ null :\\nx = None\\nprint x == None    # prints True, but is not Pythonic\\nprint x is None    # prints True, and is Pythonic\\nPython lets you use any value where it expects a Boolean. The following are all\\n“Falsy”:\\nThe Basics | 25\\nwww.it-ebooks.info•False\\n•None\\n•[] (an empty list )\\n•{} (an empty dict )\\n•\"\"\\n•set()\\n•0\\n•0.0\\nPretty much anything else gets treated as True . This allows you to easily use if state‐\\nments to test for empty lists or empty strings or empty dictionaries or so on. It also\\nsometimes causes tricky bugs if you’re not expecting this behavior:\\ns = some_function_that_returns_a_string ()\\nif s:\\n    first_char  = s[0]\\nelse:\\n    first_char  = \"\"\\nA simpler way of doing the same is:\\nfirst_char  = s and s[0]\\nsince and returns its second value when the first is “truthy, ” the first value when it’s\\nnot. Similarly, if x is either a number or possibly None :\\nsafe_x = x or 0\\nis definitely a number.\\nPython has an all function, which takes a list and returns True  precisely when every\\nelement is truthy, and an any function, which returns True  when at least one element\\nis truthy:\\nall([True, 1, { 3 }])   # True\\nall([True, 1, {}])      # False, {} is falsy\\nany([True, 1, {}])      # True, True is truthy\\nall([])                 # True, no falsy elements in the list\\nany([])                 # False, no truthy elements in the list\\nThe Not-So-Basics\\nHere we’ll look at some more-advanced Python features that we’ll find useful for\\nworking with data.\\n26 | Chapter 2: A Crash Course in Python\\nwww.it-ebooks.infoSorting\\nEvery Python list has a sort  method that sorts it in place. If you don’t want to mess\\nup your list, you can use the sorted  function, which returns a new list:\\nx = [4,1,2,3]\\ny = sorted(x)     # is [1,2,3,4], x is unchanged\\nx.sort()          # now x is [1,2,3,4]\\nBy default, sort  (and sorted ) sort a list from smallest to largest based on naively\\ncomparing the elements to one another.\\nIf you want elements sorted from largest to smallest, you can specify a reverse=True\\nparameter. And instead of comparing the elements themselves, you can compare the\\nresults of a function that you specify with key:\\n# sort the list by absolute value from largest to smallest\\nx = sorted([-4,1,-2,3], key=abs, reverse=True)  # is [-4,3,-2,1]\\n# sort the words and counts from highest count to lowest\\nwc = sorted(word_counts .items(),\\n            key=lambda (word, count): count,\\n            reverse=True)\\nList Comprehensions\\nFrequently, you’ll want to transform a list into another list, by choosing only certain\\nelements, or by transforming elements, or both.  The Pythonic way of doing this is list\\ncomprehensions :\\neven_numbers  = [x for x in range(5) if x % 2 == 0]  # [0, 2, 4]\\nsquares      = [x * x for x in range(5)]            # [0, 1, 4, 9, 16]\\neven_squares  = [x * x for x in even_numbers ]        # [0, 4, 16]\\nY ou can similarly turn lists into dictionaries or sets:\\nsquare_dict  = { x : x * x for x in range(5) }  # { 0:0, 1:1, 2:4, 3:9, 4:16 }\\nsquare_set   = { x * x for x in [1, -1] }       # { 1 }\\nIf you don’t need the value from the list, it’s conventional to use an underscore as the\\nvariable:\\nzeroes = [0 for _ in even_numbers ]      # has the same length as even_numbers\\nA list comprehension can include multiple fors:\\npairs = [(x, y)\\n         for x in range(10)\\n         for y in range(10)]   # 100 pairs (0,0) (0,1) ... (9,8), (9,9)\\nand later fors can use the results of earlier ones:\\nThe Not-So-Basics | 27\\nwww.it-ebooks.infoincreasing_pairs  = [(x, y)                       # only pairs with x < y,\\n                    for x in range(10)           # range(lo, hi) equals\\n                    for y in range(x + 1, 10)]   # [lo, lo + 1, ..., hi - 1]\\nWe will use list comprehensions a lot.\\nGenerators and Iterators\\nA problem with lists is that they can easily grow very big. range(1000000)  creates an\\nactual list of 1 million elements.  If you only need to deal with them one at a time, this\\ncan be a huge source of inefficiency (or of running out of memory). If you potentially\\nonly need the first few values, then calculating them all is a waste.\\nA generator  is something that you can iterate over (for us, usually using for) but\\nwhose values are produced only as needed ( lazily ).\\nOne way to create generators is with functions and the yield  operator:\\ndef lazy_range (n):\\n    \"\"\"a lazy version of range\"\"\"\\n    i = 0\\n    while i < n:\\n        yield i\\n        i += 1\\nThe following loop will consume the yield ed values one at a time until none are left:\\nfor i in lazy_range (10):\\n    do_something_with (i)\\n(Python actually comes with a lazy_range  function called xrange , and in Python 3,\\nrange  itself is lazy.) This means you could even create an infinite sequence:\\ndef natural_numbers ():\\n    \"\"\"returns 1, 2, 3, ...\"\"\"\\n    n = 1\\n    while True:\\n        yield n\\n        n += 1\\nalthough you probably shouldn’t iterate over it without using some kind of break\\nlogic.\\nThe flip side of laziness is that you can only iterate through a gen‐\\nerator once. If you need to iterate through something multiple\\ntimes, you’ll need to either recreate the generator each time or use a\\nlist.\\nA second way to create generators is by using for comprehensions wrapped in paren‐\\ntheses:\\n28 | Chapter 2: A Crash Course in Python\\nwww.it-ebooks.infolazy_evens_below_20  = (i for i in lazy_range (20) if i % 2 == 0)\\nRecall also that every dict  has an items()  method that returns a list of its key-value\\npairs.  More frequently we’ll use the iteritems()  method, which lazily yield s the\\nkey-value pairs one at a time as we iterate over it.\\nRandomness\\nAs we learn data science, we will frequently need to generate random numbers, which\\nwe can do with the random  module:\\nimport random\\nfour_uniform_randoms  = [random.random() for _ in range(4)]\\n#  [0.8444218515250481,      # random.random() produces numbers\\n#   0.7579544029403025,      # uniformly between 0 and 1\\n#   0.420571580830845,       # it\\'s the random function we\\'ll use\\n#   0.25891675029296335]     # most often\\nThe random  module actually produces pseudorandom (that is, deterministic) num‐\\nbers based on an internal state that you can set with random.seed  if you want to get\\nreproducible results:\\nrandom.seed(10)         # set the seed to 10\\nprint random.random()   # 0.57140259469\\nrandom.seed(10)         # reset the seed to 10\\nprint random.random()   # 0.57140259469 again\\nWe’ll sometimes use random.randrange , which takes either 1 or 2 arguments and\\nreturns an element chosen randomly from the corresponding range() :\\nrandom.randrange (10)    # choose randomly from range(10) = [0, 1, ..., 9]\\nrandom.randrange (3, 6)  # choose randomly from range(3, 6) = [3, 4, 5]\\nThere are a few more methods that we’ll sometimes find convenient. random.shuffle\\nrandomly reorders the elements of a list:\\nup_to_ten  = range(10)\\nrandom.shuffle(up_to_ten )\\nprint up_to_ten\\n# [2, 5, 1, 9, 7, 3, 8, 6, 4, 0]   (your results will probably be different)\\nIf you need to randomly pick one element from a list you can use random.choice :\\nmy_best_friend  = random.choice([\"Alice\", \"Bob\", \"Charlie\" ])     # \"Bob\" for me\\nAnd if you need to randomly choose a sample of elements without replacement (i.e.,\\nwith no duplicates), you can use random.sample :\\nlottery_numbers  = range(60)\\nwinning_numbers  = random.sample(lottery_numbers , 6)  # [16, 36, 10, 6, 25, 9]\\nThe Not-So-Basics | 29\\nwww.it-ebooks.infoTo choose a sample of elements with  replacement (i.e., allowing duplicates), you can\\njust make multiple calls to random.choice :\\nfour_with_replacement  = [random.choice(range(10))\\n                         for _ in range(4)]\\n# [9, 4, 4, 2]\\nRegular Expressions\\nRegular expressions provide a way of searching text. They are incredibly useful but\\nalso fairly complicated, so much so that there are entire books written about them.\\nWe will explain their details the few times we encounter them; here are a few exam‐\\nples of how to use them in Python:\\nimport re\\nprint all([                                # all of these are true, because\\n    not re.match(\"a\", \"cat\"),              # * \\'cat\\' doesn\\'t start with \\'a\\'\\n    re.search(\"a\", \"cat\"),                 # * \\'cat\\' has an \\'a\\' in it\\n    not re.search(\"c\", \"dog\"),             # * \\'dog\\' doesn\\'t have a \\'c\\' in it\\n    3 == len(re.split(\"[ab]\", \"carbs\")),   # * split on a or b to [\\'c\\',\\'r\\',\\'s\\']\\n    \"R-D-\" == re.sub(\"[0-9]\", \"-\", \"R2D2\") # * replace digits with dashes\\n    ])  # prints True\\nObject-Oriented Programming\\nLike many languages, Python allows you to define classes  that encapsulate data and\\nthe functions that operate on them. We’ll use them sometimes to make our code\\ncleaner and simpler. It’s probably simplest to explain them by constructing a heavily\\nannotated example.\\nImagine we didn’t have the built-in Python set. Then we might want to create our\\nown Set class.\\nWhat behavior should our class have? Given an instance of Set, we’ll need to be able\\nto add items to it, remove  items from it, and check whether it contains  a certain\\nvalue. We’ll create all of these as member  functions, which means we’ll access them\\nwith a dot after a Set object:\\n# by convention, we give classes PascalCase names\\nclass Set:\\n    # these are the member functions\\n    # every one takes a first parameter \"self\" (another convention)\\n    # that refers to the particular Set object being used\\n    def __init__ (self, values=None):\\n        \"\"\"This is the constructor.\\n        It gets called when you create a new Set.\\n        You would use it like\\n30 | Chapter 2: A Crash Course in Python\\nwww.it-ebooks.info        s1 = Set()          # empty set\\n        s2 = Set([1,2,2,3]) # initialize with values\"\"\"\\n        self.dict = {} # each instance of Set has its own dict property\\n                       # which is what we\\'ll use to track memberships\\n        if values is not None:\\n            for value in values:\\n                self.add(value)\\n    def __repr__ (self):\\n        \"\"\"this is the string representation of a Set object\\n        if you type it at the Python prompt or pass it to str()\"\"\"\\n        return \"Set: \" + str(self.dict.keys())\\n    # we\\'ll represent membership by being a key in self.dict with value True\\n    def add(self, value):\\n        self.dict[value] = True\\n    # value is in the Set if it\\'s a key in the dictionary\\n    def contains (self, value):\\n        return value in self.dict\\n    def remove(self, value):\\n        del self.dict[value]\\nWhich we could then use like:\\ns = Set([1,2,3])\\ns.add(4)\\nprint s.contains (4)     # True\\ns.remove(3)\\nprint s.contains (3)     # False\\nFunctional Tools\\nWhen passing functions around, sometimes we’ll want to partially apply (or curry )\\nfunctions to create new functions.  As a simple example, imagine that we have a func‐\\ntion of two variables:\\ndef exp(base, power):\\n    return base ** power\\nand we want to use it to create a function of one variable two_to_the  whose input is a\\npower  and whose output is the result of exp(2, power) .\\nWe can, of course, do this with def, but this can sometimes get unwieldy:\\ndef two_to_the (power):\\n    return exp(2, power)\\nA different approach is to use functools.partial :\\nThe Not-So-Basics | 31\\nwww.it-ebooks.infofrom functools  import partial\\ntwo_to_the  = partial(exp, 2)     # is now a function of one variable\\nprint two_to_the (3)              # 8\\nY ou can also use partial  to fill in later arguments if you specify their names:\\nsquare_of  = partial(exp, power=2)\\nprint square_of (3)                  # 9\\nIt starts to get messy if you curry arguments in the middle of the function, so we’ll try\\nto avoid doing that.\\nWe will also occasionally use map, reduce , and filter , which provide functional \\nalternatives to list comprehensions:\\ndef double(x):\\n    return 2 * x\\nxs = [1, 2, 3, 4]\\ntwice_xs  = [double(x) for x in xs]        # [2, 4, 6, 8]\\ntwice_xs  = map(double, xs)                # same as above\\nlist_doubler  = partial(map, double)       # *function* that doubles a list\\ntwice_xs  = list_doubler (xs)               # again [2, 4, 6, 8]\\nY ou can use map with multiple-argument functions if you provide multiple lists:\\ndef multiply (x, y): return x * y\\nproducts  = map(multiply , [1, 2], [4, 5]) # [1 * 4, 2 * 5] = [4, 10]\\nSimilarly, filter  does the work of a list-comprehension if:\\ndef is_even(x):\\n    \"\"\"True if x is even, False if x is odd\"\"\"\\n    return x % 2 == 0\\nx_evens = [x for x in xs if is_even(x)]    # [2, 4]\\nx_evens = filter(is_even, xs)              # same as above\\nlist_evener  = partial(filter, is_even)     # *function* that filters a list\\nx_evens = list_evener (xs)                  # again [2, 4]\\nAnd reduce  combines the first two elements of a list, then that result with the third,\\nthat result with the fourth, and so on, producing a single result:\\nx_product  = reduce(multiply , xs)           # = 1 * 2 * 3 * 4 = 24\\nlist_product  = partial(reduce, multiply )   # *function* that reduces a list\\nx_product  = list_product (xs)               # again = 24\\nenumerate\\nNot infrequently, you’ll want to iterate over a list and use both its elements and their\\nindexes:\\n32 | Chapter 2: A Crash Course in Python\\nwww.it-ebooks.info# not Pythonic\\nfor i in range(len(documents )):\\n    document  = documents [i]\\n    do_something (i, document )\\n# also not Pythonic\\ni = 0\\nfor document  in documents :\\n    do_something (i, document )\\n    i += 1\\nThe Pythonic solution is enumerate , which produces tuples (index, element) :\\nfor i, document  in enumerate (documents ):\\n    do_something (i, document )\\nSimilarly, if we just want the indexes:\\nfor i in range(len(documents )): do_something (i)     # not Pythonic\\nfor i, _ in enumerate (documents ): do_something (i)   # Pythonic\\nWe’ll use this a lot.\\nzip and Argument Unpacking\\nOften we will need to zip two or more lists together. zip transforms multiple lists\\ninto a single list of tuples of corresponding elements:\\nlist1 = [\\'a\\', \\'b\\', \\'c\\']\\nlist2 = [1, 2, 3]\\nzip(list1, list2)        # is [(\\'a\\', 1), (\\'b\\', 2), (\\'c\\', 3)]\\nIf the lists are different lengths, zip stops as soon as the first list ends.\\nY ou can also “unzip” a list using a strange trick:\\npairs = [(\\'a\\', 1), (\\'b\\', 2), (\\'c\\', 3)]\\nletters, numbers = zip(*pairs)\\nThe asterisk performs argument unpacking , which uses the elements of pairs  as indi‐\\nvidual arguments to zip. It ends up the same as if you’ d called:\\nzip((\\'a\\', 1), (\\'b\\', 2), (\\'c\\', 3))\\nwhich returns [(\\'a\\',\\'b\\',\\'c\\'), (\\'1\\',\\'2\\',\\'3\\')] .\\nY ou can use argument unpacking with any function:\\ndef add(a, b): return a + b\\nadd(1, 2)      # returns 3\\nadd([1, 2])    # TypeError!\\nadd(*[1, 2])   # returns 3\\nIt is rare that we’ll find this useful, but when we do it’s a neat trick.\\nThe Not-So-Basics | 33\\nwww.it-ebooks.infoargs and kwargs\\nLet’s say we want to create a higher-order function that takes as input some function f\\nand returns a new function that for any input returns twice the value of f:\\ndef doubler(f):\\n    def g(x):\\n        return 2 * f(x)\\n    return g\\nThis works in some cases:\\ndef f1(x):\\n    return x + 1\\ng = doubler(f1)\\nprint g(3)          # 8 (== ( 3 + 1) * 2)\\nprint g(-1)         # 0 (== (-1 + 1) * 2)\\nHowever, it breaks down with functions that take more than a single argument:\\ndef f2(x, y):\\n    return x + y\\ng = doubler(f2)\\nprint g(1, 2)    # TypeError: g() takes exactly 1 argument (2 given)\\nWhat we need is a way to specify a function that takes arbitrary arguments. We can\\ndo this with argument unpacking and a little bit of magic:\\ndef magic(*args, **kwargs):\\n    print \"unnamed args:\" , args\\n    print \"keyword args:\" , kwargs\\nmagic(1, 2, key=\"word\", key2=\"word2\")\\n# prints\\n#  unnamed args: (1, 2)\\n#  keyword args: {\\'key2\\': \\'word2\\', \\'key\\': \\'word\\'}\\nThat is, when we define a function like this, args  is a tuple of its unnamed arguments\\nand kwargs  is a dict  of its named arguments.  It works the other way too, if you want\\nto use a list  (or tuple ) and dict  to supply  arguments to a function:\\ndef other_way_magic (x, y, z):\\n    return x + y + z\\nx_y_list  = [1, 2]\\nz_dict = { \"z\" : 3 }\\nprint other_way_magic (*x_y_list , **z_dict)   # 6\\nY ou could do all sorts of strange tricks with this; we will only use it to produce\\nhigher-order functions whose inputs can accept arbitrary arguments:\\n34 | Chapter 2: A Crash Course in Python\\nwww.it-ebooks.infodef doubler_correct (f):\\n    \"\"\"works no matter what kind of inputs f expects\"\"\"\\n    def g(*args, **kwargs):\\n        \"\"\"whatever arguments g is supplied, pass them through to f\"\"\"\\n        return 2 * f(*args, **kwargs)\\n    return g\\ng = doubler_correct (f2)\\nprint g(1, 2) # 6\\nWelcome to DataSciencester!\\nThis concludes new-employee orientation. Oh, and also, try not to embezzle any‐\\nthing.\\nFor Further Exploration\\n•There is no shortage of Python tutorials in the world. The official one  is not a bad\\nplace to start.\\n•The official IPython tutorial  is not quite as good. Y ou might be better off with\\ntheir videos and presentations . Alternatively, Wes McKinney’s Python for Data\\nAnalysis  (O’Reilly) has a really good IPython chapter.\\nFor Further Exploration | 35\\nwww.it-ebooks.infowww.it-ebooks.infoCHAPTER 3\\nVisualizing Data\\nI believe that visualization is one of the most powerful means of achieving personal goals.\\n—Harvey Mackay\\nA fundamental part of the data scientist’s toolkit is data visualization. Although it is\\nvery easy to create visualizations, it’s much harder to produce good  ones.\\nThere are two primary uses for data visualization:\\n•To explore  data\\n•To communicate  data\\nIn this chapter, we will concentrate on building the skills that you’ll need to start\\nexploring your own data and to produce the visualizations we’ll be using throughout\\nthe rest of the book. Like most of our chapter topics, data visualization is a rich field\\nof study that deserves its own book. Nonetheless, we’ll try to give you a sense of what\\nmakes for a good visualization and what doesn’t.\\nmatplotlib\\nA wide variety of tools exists for visualizing data. We will be using the matplotlib\\nlibrary , which is widely used (although sort of showing its age). If you are interested\\nin producing elaborate interactive visualizations for the Web, it is likely not the right\\nchoice, but for simple bar charts, line charts, and scatterplots, it works pretty well.\\nIn particular, we will be using the matplotlib.pyplot  module. In its simplest use,\\npyplot  maintains an internal state in which you build up a visualization step by step.\\nOnce you’re done, you can save it (with savefig() ) or display it (with show() ).\\nFor example, making simple plots (like Figure 3-1 ) is pretty simple:\\n37\\nwww.it-ebooks.infofrom matplotlib  import pyplot as plt\\nyears = [1950, 1960, 1970, 1980, 1990, 2000, 2010]\\ngdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3]\\n# create a line chart, years on x-axis, gdp on y-axis\\nplt.plot(years, gdp, color=\\'green\\', marker=\\'o\\', linestyle =\\'solid\\')\\n# add a title\\nplt.title(\"Nominal GDP\" )\\n# add a label to the y-axis\\nplt.ylabel(\"Billions of $\" )\\nplt.show()\\nFigure 3-1. A simple line chart\\nMaking plots that look publication-quality good is more complicated and beyond the\\nscope of this chapter. There are many ways you can customize your charts with (for\\nexample) axis labels, line styles, and point markers. Rather than attempt a compre‐\\nhensive treatment of these options, we’ll just use (and call attention to) some of them\\nin our examples.\\n38 | Chapter 3: Visualizing Data\\nwww.it-ebooks.infoAlthough we won’t be using much of this functionality, matplotlib\\nis capable of producing complicated plots within plots, sophistica‐\\nted formatting, and interactive visualizations. Check out its docu‐\\nmentation if you want to go deeper than we do in this book.\\nBar Charts\\nA bar chart is a good choice when you want to show how some quantity varies among\\nsome discrete  set of items. For instance, Figure 3-2  shows how many Academy\\nAwards were won by each of a variety of movies:\\nmovies = [\"Annie Hall\" , \"Ben-Hur\" , \"Casablanca\" , \"Gandhi\" , \"West Side Story\" ]\\nnum_oscars  = [5, 11, 3, 8, 10]\\n# bars are by default width 0.8, so we\\'ll add 0.1 to the left coordinates\\n# so that each bar is centered\\nxs = [i + 0.1 for i, _ in enumerate (movies)]\\n# plot bars with left x-coordinates [xs], heights [num_oscars]\\nplt.bar(xs, num_oscars )\\nplt.ylabel(\"# of Academy Awards\" )\\nplt.title(\"My Favorite Movies\" )\\n# label x-axis with movie names at bar centers\\nplt.xticks([i + 0.5 for i, _ in enumerate (movies)], movies)\\nplt.show()\\nBar Charts | 39\\nwww.it-ebooks.infoFigure 3-2. A simple bar chart\\nA bar chart can also be a good choice for plotting histograms of bucketed numeric\\nvalues, in order to visually explore how the values are distributed , as in Figure 3-3 :\\ngrades = [83,95,91,87,70,0,85,82,100,67,73,77,0]\\ndecile = lambda grade: grade // 10 * 10\\nhistogram  = Counter(decile(grade) for grade in grades)\\nplt.bar([x - 4 for x in histogram .keys()], # shift each bar to the left by 4\\n        histogram .values(),                # give each bar its correct height\\n        8)                                 # give each bar a width of 8\\nplt.axis([-5, 105, 0, 5])                  # x-axis from -5 to 105,\\n                                           # y-axis from 0 to 5\\nplt.xticks([10 * i for i in range(11)])    # x-axis labels at 0, 10, ..., 100\\nplt.xlabel(\"Decile\" )\\nplt.ylabel(\"# of Students\" )\\nplt.title(\"Distribution of Exam 1 Grades\" )\\nplt.show()\\n40 | Chapter 3: Visualizing Data\\nwww.it-ebooks.infoFigure 3-3. Using a bar chart for a histogram\\nThe third argument to plt.bar  specifies the bar width. Here we chose a width of 8\\n(which leaves a small gap between bars, since our buckets have width 10). And we\\nshifted the bar left by 4, so that (for example) the “80” bar has its left and right sides at\\n76 and 84, and (hence) its center at 80.\\nThe call to plt.axis  indicates that we want the x-axis to range from -5 to 105 (so that\\nthe “0” and “100” bars are fully shown), and that the y-axis should range from 0 to 5.\\nAnd the call to plt.xticks  puts x-axis labels at 0, 10, 20, …, 100.\\nBe judicious when using plt.axis() . When creating bar charts it is considered espe‐\\ncially bad form for your y-axis not to start at 0, since this is an easy way to mislead\\npeople ( Figure 3-4 ):\\nmentions  = [500, 505]\\nyears = [2013, 2014]\\nplt.bar([2012.6, 2013.6], mentions , 0.8)\\nplt.xticks(years)\\nplt.ylabel(\"# of times I heard someone say \\'data science\\'\" )\\n# if you don\\'t do this, matplotlib will label the x-axis 0, 1\\nBar Charts | 41\\nwww.it-ebooks.info# and then add a +2.013e3 off in the corner (bad matplotlib!)\\nplt.ticklabel_format (useOffset =False)\\n# misleading y-axis only shows the part above 500\\nplt.axis([2012.5,2014.5,499,506])\\nplt.title(\"Look at the \\'Huge\\' Increase!\" )\\nplt.show()\\nFigure 3-4. A chart with a misleading y-axis\\nIn Figure 3-5 , we use more-sensible axes, and it looks far less impressive:\\nplt.axis([2012.5,2014.5,0,550])\\nplt.title(\"Not So Huge Anymore\" )\\nplt.show()\\n42 | Chapter 3: Visualizing Data\\nwww.it-ebooks.infoFigure 3-5. The same chart with a nonmisleading y-axis\\nLine Charts\\nAs we saw already, we can make line charts using plt.plot() . These are a good\\nchoice for showing trends , as illustrated in Figure 3-6 :\\nvariance      = [1, 2, 4, 8, 16, 32, 64, 128, 256]\\nbias_squared  = [256, 128, 64, 32, 16, 8, 4, 2, 1]\\ntotal_error   = [x + y for x, y in zip(variance , bias_squared )]\\nxs = [i for i, _ in enumerate (variance )]\\n# we can make multiple calls to plt.plot\\n# to show multiple series on the same chart\\nplt.plot(xs, variance ,     \\'g-\\',  label=\\'variance\\' )    # green solid line\\nplt.plot(xs, bias_squared , \\'r-.\\', label=\\'bias^2\\' )      # red dot-dashed line\\nplt.plot(xs, total_error ,  \\'b:\\',  label=\\'total error\\' ) # blue dotted line\\n# because we\\'ve assigned labels to each series\\n# we can get a legend for free\\n# loc=9 means \"top center\"\\nplt.legend(loc=9)\\nplt.xlabel(\"model complexity\" )\\nplt.title(\"The Bias-Variance Tradeoff\" )\\nplt.show()\\nLine Charts | 43\\nwww.it-ebooks.infoFigure 3-6. Several line charts with a legend\\nScatterplots\\nA scatterplot is the right choice for visualizing the relationship between two paired\\nsets of data. For example, Figure 3-7  illustrates the relationship between the number\\nof friends your users have and the number of minutes they spend on the site every\\nday:\\nfriends = [ 70,  65,  72,  63,  71,  64,  60,  64,  67]\\nminutes = [175, 170, 205, 120, 220, 130, 105, 145, 190]\\nlabels =  [\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\', \\'f\\', \\'g\\', \\'h\\', \\'i\\']\\nplt.scatter(friends, minutes)\\n# label each point\\nfor label, friend_count , minute_count  in zip(labels, friends, minutes):\\n    plt.annotate (label,\\n        xy=(friend_count , minute_count ), # put the label with its point\\n        xytext=(5, -5),                  # but slightly offset\\n        textcoords =\\'offset points\\' )\\nplt.title(\"Daily Minutes vs. Number of Friends\" )\\n44 | Chapter 3: Visualizing Data\\nwww.it-ebooks.infoplt.xlabel(\"# of friends\" )\\nplt.ylabel(\"daily minutes spent on the site\" )\\nplt.show()\\nFigure 3-7. A scatterplot of friends and time on the site\\nIf you’re scattering comparable variables, you might get a misleading picture if you let\\nmatplotlib  choose the scale, as in Figure 3-8 :\\ntest_1_grades  = [ 99, 90, 85, 97, 80]\\ntest_2_grades  = [100, 85, 60, 90, 70]\\nplt.scatter(test_1_grades , test_2_grades )\\nplt.title(\"Axes Aren\\'t Comparable\" )\\nplt.xlabel(\"test 1 grade\" )\\nplt.ylabel(\"test 2 grade\" )\\nplt.show()\\nScatterplots | 45\\nwww.it-ebooks.infoFigure 3-8. A scatterplot with uncomparable axes\\nIf we include a call to plt.axis(\"equal\") , the plot ( Figure 3-9 ) more accurately\\nshows that most of the variation occurs on test 2.\\nThat’s enough to get you started doing visualization. We’ll learn much more about\\nvisualization throughout the book.\\n46 | Chapter 3: Visualizing Data\\nwww.it-ebooks.infoFigure 3-9. The same scatterplot with equal axes\\nFor Further Exploration\\n•seaborn  is built on top of matplotlib  and allows you to easily produce prettier\\n(and more complex) visualizations.\\n•D3.js  is a JavaScript library for producing sophisticated interactive visualizations\\nfor the web. Although it is not in Python, it is both trendy and widely used, and it\\nis well worth your while to be familiar with it.\\n•Bokeh  is a newer library that brings D3-style visualizations into Python.\\n•ggplot  is a Python port of the popular R library ggplot2 , which is widely used for\\ncreating “publication quality” charts and graphics. It’s probably most interesting\\nif you’re already an avid ggplot2  user, and possibly a little opaque if you’re not.\\nFor Further Exploration | 47\\nwww.it-ebooks.infowww.it-ebooks.infoCHAPTER 4\\nLinear Algebra\\nIs there anything more useless or less useful than Algebra?\\n—Billy Connolly\\nLinear algebra is the branch of mathematics that deals with vector spaces . Although I\\ncan’t hope to teach you linear algebra in a brief chapter, it underpins a large number\\nof data science concepts and techniques, which means I owe it to you to at least try.\\nWhat we learn in this chapter we’ll use heavily throughout the rest of the book.\\nVectors\\nAbstractly, vectors  are objects that can be added together (to form new vectors) and\\nthat can be multiplied by scalars  (i.e., numbers), also to form new vectors.\\nConcretely (for us), vectors are points in some finite-dimensional space. Although\\nyou might not think of your data as vectors, they are a good way to represent numeric\\ndata.\\nFor example, if you have the heights, weights, and ages of a large number of people,\\nyou can treat your data as three-dimensional vectors (height, weight, age) . If\\nyou’re teaching a class with four exams, you can treat student grades as four-\\ndimensional vectors (exam1, exam2, exam3, exam4) .\\nThe simplest from-scratch approach is to represent vectors as lists of numbers. A list\\nof three numbers corresponds to a vector in three-dimensional space, and vice versa:\\nheight_weight_age  = [70,  # inches,\\n                     170, # pounds,\\n                     40 ] # years\\ngrades = [95,   # exam1\\n          80,   # exam2\\n49\\nwww.it-ebooks.info          75,   # exam3\\n          62 ]  # exam4\\nOne problem with this approach is that we will want to perform arithmetic  on vec‐\\ntors.  Because Python lists aren’t vectors (and hence provide no facilities for vector\\narithmetic), we’ll need to build these arithmetic tools ourselves. So let’s start with that.\\nTo begin with, we’ll frequently need to add two vectors. Vectors add componentwise .\\nThis means that if two vectors v and w are the same length, their sum is just the vector\\nwhose first element is v[0] + w[0] , whose second element is v[1] + w[1] , and so\\non. (If they’re not the same length, then we’re not allowed to add them.)\\nFor example, adding the vectors [1, 2]  and [2, 1]  results in [1 + 2, 2 + 1]  or [3,\\n3], as shown in Figure 4-1 .\\nFigure 4-1. Adding two vectors\\nWe can easily implement this by zip-ing the vectors together and using a list compre‐\\nhension to add the corresponding elements:\\ndef vector_add (v, w):\\n    \"\"\"adds corresponding elements\"\"\"\\n    return [v_i + w_i\\n            for v_i, w_i in zip(v, w)]\\n50 | Chapter 4: Linear Algebra\\nwww.it-ebooks.infoSimilarly, to subtract two vectors we just subtract corresponding elements:\\ndef vector_subtract (v, w):\\n    \"\"\"subtracts corresponding elements\"\"\"\\n    return [v_i - w_i\\n            for v_i, w_i in zip(v, w)]\\nWe’ll also sometimes want to componentwise sum a list of vectors. That is, create a\\nnew vector whose first element is the sum of all the first elements, whose second ele‐\\nment is the sum of all the second elements, and so on. The easiest way to do this is by\\nadding one vector at a time:\\ndef vector_sum (vectors):\\n    \"\"\"sums all corresponding elements\"\"\"\\n    result = vectors[0]                         # start with the first vector\\n    for vector in vectors[1:]:                  # then loop over the others\\n        result = vector_add (result, vector)     # and add them to the result\\n    return result\\nIf you think about it, we are just reduce -ing the list of vectors using vector_add ,\\nwhich means we can rewrite this more briefly using higher-order functions:\\ndef vector_sum (vectors):\\n    return reduce(vector_add , vectors)\\nor even:\\nvector_sum  = partial(reduce, vector_add )\\nalthough this last one is probably more clever than helpful.\\nWe’ll also need to be able to multiply a vector by a scalar, which we do simply by mul‐\\ntiplying each element of the vector by that number:\\ndef scalar_multiply (c, v):\\n    \"\"\"c is a number, v is a vector\"\"\"\\n    return [c * v_i for v_i in v]\\nThis allows us to compute the componentwise means of a list of (same-sized) vectors:\\ndef vector_mean (vectors):\\n    \"\"\"compute the vector whose ith element is the mean of the\\n    ith elements of the input vectors\"\"\"\\n    n = len(vectors)\\n    return scalar_multiply (1/n, vector_sum (vectors))\\nA less obvious tool is the dot product . The dot product of two vectors is the sum of\\ntheir componentwise products:\\ndef dot(v, w):\\n    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\\n    return sum(v_i * w_i\\n               for v_i, w_i in zip(v, w))\\nVectors | 51\\nwww.it-ebooks.infoThe dot product measures how far the vector v extends in the w direction. For exam‐\\nple, if w = [1, 0]  then dot(v, w)  is just the first component of v. Another way of\\nsaying this is that it’s the length of the vector you’ d get if you projected  v onto w\\n(Figure 4-2 ).\\nFigure 4-2. The dot product as vector projection\\nUsing this, it’s easy to compute a vector’s sum of squares :\\ndef sum_of_squares (v):\\n    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\\n    return dot(v, v)\\nWhich we can use to compute its magnitude  (or length):\\nimport math\\ndef magnitude (v):\\n    return math.sqrt(sum_of_squares (v))   # math.sqrt is square root function\\nWe now have all the pieces we need to compute the distance between two vectors,\\ndefined as:\\nv1−w12+ ... + vn−wn2\\n52 | Chapter 4: Linear Algebra\\nwww.it-ebooks.infodef squared_distance (v, w):\\n    \"\"\"(v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\\n    return sum_of_squares (vector_subtract (v, w))\\ndef distance (v, w):\\n   return math.sqrt(squared_distance (v, w))\\nWhich is possibly clearer if we write it as (the equivalent):\\ndef distance (v, w):\\n    return magnitude (vector_subtract (v, w))\\nThat should be plenty to get us started. We’ll be using these functions heavily\\nthroughout the book.\\nUsing lists as vectors is great for exposition but terrible for perfor‐\\nmance.\\nIn production code, you would want to use the NumPy library,\\nwhich includes a high-performance array class with all sorts of\\narithmetic operations included.\\nMatrices\\nA matrix  is a two-dimensional collection of numbers. We will represent matrices as\\nlist s of list s, with each inner list having the same size and representing a row of the\\nmatrix. If A is a matrix, then A[i][j]  is the element in the ith row and the jth column.\\nPer mathematical convention, we will typically use capital letters to represent matri‐\\nces. For example:\\nA = [[1, 2, 3],  # A has 2 rows and 3 columns\\n     [4, 5, 6]]\\nB = [[1, 2],     # B has 3 rows and 2 columns\\n     [3, 4],\\n     [5, 6]]\\nIn mathematics, you would usually name the first row of the matrix\\n“row 1” and the first column “column 1. ” Because we’re represent‐\\ning matrices with Python list s, which are zero-indexed, we’ll call\\nthe first row of a matrix “row 0” and the first column “column 0. ”\\nGiven this list-of-lists representation, the matrix A has len(A)  rows and len(A[0])\\ncolumns, which we consider its shape :\\ndef shape(A):\\n    num_rows  = len(A)\\n    num_cols  = len(A[0]) if A else 0   # number of elements in first row\\n    return num_rows , num_cols\\nMatrices | 53\\nwww.it-ebooks.infoIf a matrix has n rows and k columns, we will refer to it as a n×k matrix. We can\\n(and sometimes will) think of each row of a n×k matrix as a vector of length k, and\\neach column as a vector of length n:\\ndef get_row(A, i):\\n    return A[i]             # A[i] is already the ith row\\ndef get_column (A, j):\\n    return [A_i[j]          # jth element of row A_i\\n            for A_i in A]   # for each row A_i\\nWe’ll also want to be able to create a matrix given its shape and a function for generat‐\\ning its elements. We can do this using a nested list comprehension:\\ndef make_matrix (num_rows , num_cols , entry_fn ):\\n    \"\"\"returns a num_rows x num_cols matrix\\n    whose (i,j)th entry is entry_fn(i, j)\"\"\"\\n    return [[entry_fn (i, j)             # given i, create a list\\n             for j in range(num_cols )]  #   [entry_fn(i, 0), ... ]\\n            for i in range(num_rows )]   # create one list for each i\\nGiven this function, you could make a 5 × 5 identity matrix  (with 1s on the diagonal\\nand 0s elsewhere) with:\\ndef is_diagonal (i, j):\\n    \"\"\"1\\'s on the \\'diagonal\\', 0\\'s everywhere else\"\"\"\\n    return 1 if i == j else 0\\nidentity_matrix  = make_matrix (5, 5, is_diagonal )\\n# [[1, 0, 0, 0, 0],\\n#  [0, 1, 0, 0, 0],\\n#  [0, 0, 1, 0, 0],\\n#  [0, 0, 0, 1, 0],\\n#  [0, 0, 0, 0, 1]]\\nMatrices will be important to us for several reasons.\\nFirst, we can use a matrix to represent a  data set consisting of multiple vectors, simply\\nby considering each vector as a row of the matrix. For example, if you had the\\nheights, weights, and ages of 1,000 people you could put them in a 1, 000 × 3  matrix:\\ndata = [[70, 170, 40],\\n        [65, 120, 26],\\n        [77, 250, 19],\\n        # ....\\n       ]\\nSecond, as we’ll see later, we can use an n×k matrix to represent a linear function\\nthat maps k-dimensional vectors to n-dimensional vectors. Several of our techniques\\nand concepts will involve such functions.\\n54 | Chapter 4: Linear Algebra\\nwww.it-ebooks.infoThird, matrices can be used to represent binary relationships.  In Chapter 1 , we repre‐\\nsented the edges of a network as a collection of pairs (i, j) . An alternative represen‐\\ntation would be to create a matrix A such that A[i][j]  is 1 if nodes i and j are\\nconnected and 0 otherwise.\\nRecall that before we had:\\nfriendships  = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\\n               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\\nWe could also represent this as:\\n     #     user 0  1  2  3  4  5  6  7  8  9\\n     #\\nfriendships  = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0], # user 0\\n               [1, 0, 1, 1, 0, 0, 0, 0, 0, 0], # user 1\\n               [1, 1, 0, 1, 0, 0, 0, 0, 0, 0], # user 2\\n               [0, 1, 1, 0, 1, 0, 0, 0, 0, 0], # user 3\\n               [0, 0, 0, 1, 0, 1, 0, 0, 0, 0], # user 4\\n               [0, 0, 0, 0, 1, 0, 1, 1, 0, 0], # user 5\\n               [0, 0, 0, 0, 0, 1, 0, 0, 1, 0], # user 6\\n               [0, 0, 0, 0, 0, 1, 0, 0, 1, 0], # user 7\\n               [0, 0, 0, 0, 0, 0, 1, 1, 0, 1], # user 8\\n               [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]] # user 9\\nIf there are very few connections, this is a much more inefficient representation, since\\nyou end up having to store a lot of zeroes. However, with the matrix representation it\\nis much quicker to check whether two nodes are connected—you just have to do a\\nmatrix lookup instead of (potentially) inspecting every edge:\\nfriendships [0][2] == 1   # True, 0 and 2 are friends\\nfriendships [0][8] == 1   # False, 0 and 8 are not friends\\nSimilarly, to find the connections a node has, you only need to inspect the column (or\\nthe row) corresponding to that node:\\nfriends_of_five  = [i                                              # only need\\n                   for i, is_friend  in enumerate (friendships [5])  # to look at\\n                   if is_friend ]                                  # one row\\nPreviously we added a list of connections to each node object to speed up this pro‐\\ncess, but for a large, evolving graph that would probably be too expensive and diffi‐\\ncult to maintain.\\nWe’ll revisit matrices throughout the book.\\nFor Further Exploration\\n•Linear algebra is widely used by data scientists (frequently implicitly, and not\\ninfrequently by people who don’t understand it). It wouldn’t be a bad idea to read\\na textbook. Y ou can find several freely available online:\\nFor Further Exploration | 55\\nwww.it-ebooks.info—Linear Algebra, from UC Davis\\n—Linear Algebra, from Saint Michael’s College\\n—If you are feeling adventurous, Linear Algebra Done Wrong  is a more\\nadvanced introduction\\n•All of the machinery we built here you get for free if you use NumPy . (Y ou get a\\nlot more too.)\\n56 | Chapter 4: Linear Algebra\\nwww.it-ebooks.infoCHAPTER 5\\nStatistics\\nFacts are stubborn, but statistics are more pliable.\\n—Mark Twain\\nStatistics  refers to the mathematics and techniques with which we understand data. It\\nis a rich, enormous field, more suited to a shelf (or room) in a library rather than a\\nchapter in a book, and so our discussion will necessarily not be a deep one. Instead,\\nI’ll try to teach you just enough to be dangerous, and pique your interest just enough\\nthat you’ll go off and learn more.\\nDescribing a Single Set of Data\\nThrough a combination of word-of-mouth and luck, DataSciencester has grown to\\ndozens of members, and the VP of Fundraising asks you for some sort of description\\nof how many friends your members have that he can include in his elevator pitches.\\nUsing techniques from Chapter 1 , you are easily able to produce this data. But now\\nyou are faced with the problem of how to describe  it.\\nOne obvious description of any data set is simply the data itself:\\nnum_friends  = [100, 49, 41, 40, 25,\\n               # ... and lots more\\n              ]\\nFor a small enough data set this might even be the best description. But for a larger\\ndata set, this is unwieldy and probably opaque. (Imagine staring at a list of 1 million\\nnumbers.) For that reason we use statistics to distill and communicate relevant fea‐\\ntures of our data.\\nAs a first approach you put the friend counts into a histogram using Counter  and\\nplt.bar()  (Figure 5-1 ):\\n57\\nwww.it-ebooks.infofriend_counts  = Counter(num_friends )\\nxs = range(101)                         # largest value is 100\\nys = [friend_counts [x] for x in xs]     # height is just # of friends\\nplt.bar(xs, ys)\\nplt.axis([0, 101, 0, 25])\\nplt.title(\"Histogram of Friend Counts\" )\\nplt.xlabel(\"# of friends\" )\\nplt.ylabel(\"# of people\" )\\nplt.show()\\nFigure 5-1. A histogram of friend counts\\nUnfortunately, this chart is still too difficult to slip into conversations. So you start\\ngenerating some statistics. Probably the simplest statistic is simply the number of data\\npoints:\\nnum_points  = len(num_friends )               # 204\\nY ou’re probably also interested in the largest and smallest values:\\nlargest_value  = max(num_friends )            # 100\\nsmallest_value  = min(num_friends )           # 1\\nwhich are just special cases of wanting to know the values in specific positions:\\n58 | Chapter 5: Statistics\\nwww.it-ebooks.infosorted_values  = sorted(num_friends )\\nsmallest_value  = sorted_values [0]           # 1\\nsecond_smallest_value  = sorted_values [1]    # 1\\nsecond_largest_value  = sorted_values [-2]    # 49\\nBut we’re only getting started.\\nCentral Tendencies\\nUsually, we’ll want some notion of where our data is centered. Most commonly we’ll\\nuse the mean  (or average), which is just the sum of the data divided by its count:\\n# this isn\\'t right if you don\\'t from __future__ import division\\ndef mean(x):\\n    return sum(x) / len(x)\\nmean(num_friends )   # 7.333333\\nIf you have two data points, the mean is simply the point halfway between them. As\\nyou add more points, the mean shifts around, but it always depends on the value of\\nevery point.\\nWe’ll also sometimes be interested in the median , which is the middle-most value (if\\nthe number of data points is odd) or the average of the two middle-most values (if the\\nnumber of data points is even).\\nFor instance, if we have five data points in a sorted vector x, the median is x[5 // 2]\\nor x[2] . If we have six data points, we want the average of x[2]  (the third point) and\\nx[3]  (the fourth point).\\nNotice that—unlike the mean—the median doesn’t depend on every value in your\\ndata. For example, if you make the largest point larger (or the smallest point smaller),\\nthe middle points remain unchanged, which means so does the median.\\nThe median  function is slightly more complicated than you might expect, mostly\\nbecause of the “even” case:\\ndef median(v):\\n    \"\"\"finds the \\'middle-most\\' value of v\"\"\"\\n    n = len(v)\\n    sorted_v  = sorted(v)\\n    midpoint  = n // 2\\n    if n % 2 == 1:\\n        # if odd, return the middle value\\n        return sorted_v [midpoint ]\\n    else:\\n        # if even, return the average of the middle values\\n        lo = midpoint  - 1\\n        hi = midpoint\\n        return (sorted_v [lo] + sorted_v [hi]) / 2\\nDescribing a Single Set of Data | 59\\nwww.it-ebooks.infomedian(num_friends ) # 6.0\\nClearly, the mean is simpler to compute, and it varies smoothly as our data changes. If\\nwe have n data points and one of them increases by some small amount e, then neces‐\\nsarily the mean will increase by e / n. (This makes the mean amenable to all sorts of\\ncalculus tricks.) Whereas in order to find the median, we have to sort our data. And\\nchanging one of our data points by a small amount e might increase the median by e,\\nby some number less than e, or not at all (depending on the rest of the data).\\nThere are, in fact, nonobvious tricks to efficiently compute\\nmedians  without sorting the data. However, they are beyond the\\nscope of this book, so we have to sort the data.\\nAt the same time, the mean is very sensitive to outliers in our data. If our friendliest\\nuser had 200 friends (instead of 100), then the mean would rise to 7.82, while the\\nmedian would stay the same. If outliers are likely to be bad data (or otherwise unrep‐\\nresentative of whatever phenomenon we’re trying to understand), then the mean can\\nsometimes give us a misleading picture. For example, the story is often told that in\\nthe mid-1980s, the major at the University of North Carolina with the highest average\\nstarting salary was geography, mostly on account of NBA star (and outlier) Michael\\nJordan.\\nA generalization of the median is the quantile , which represents the value less than\\nwhich a certain percentile of the data lies. (The median represents the value less than\\nwhich 50% of the data lies.)\\ndef quantile (x, p):\\n    \"\"\"returns the pth-percentile value in x\"\"\"\\n    p_index = int(p * len(x))\\n    return sorted(x)[p_index]\\nquantile (num_friends , 0.10) # 1\\nquantile (num_friends , 0.25) # 3\\nquantile (num_friends , 0.75) # 9\\nquantile (num_friends , 0.90) # 13\\nLess commonly you might want to look at the mode , or most-common value[s]:\\ndef mode(x):\\n    \"\"\"returns a list, might be more than one mode\"\"\"\\n    counts = Counter(x)\\n    max_count  = max(counts.values())\\n    return [x_i for x_i, count in counts.iteritems ()\\n            if count == max_count ]\\nmode(num_friends )       # 1 and 6\\n60 | Chapter 5: Statistics\\nwww.it-ebooks.infoBut most frequently we’ll just use the mean.\\nDispersion\\nDispersion  refers to measures of how spread out our data is.  Typically they’re statistics\\nfor which values near zero signify not spread out at all  and for which large values\\n(whatever that means) signify very spread out . For instance, a very simple measure  is\\nthe range , which is just the difference between the largest and smallest elements:\\n# \"range\" already means something in Python, so we\\'ll use a different name\\ndef data_range (x):\\n    return max(x) - min(x)\\ndata_range (num_friends ) # 99\\nThe range is zero precisely when the max and min are equal, which can only happen if\\nthe elements of x are all the same, which means the data is as undispersed as possible.\\nConversely, if the range is large, then the max is much larger than the min and the data\\nis more spread out.\\nLike the median, the range doesn’t really depend on the whole data set. A data set\\nwhose points are all either 0 or 100 has the same range as a data set whose values are\\n0, 100, and lots of 50s. But it seems like the first data set “should” be more spread out.\\nA more complex measure of dispersion is the variance , which is computed as:\\ndef de_mean(x):\\n    \"\"\"translate x by subtracting its mean (so the result has mean 0)\"\"\"\\n    x_bar = mean(x)\\n    return [x_i - x_bar for x_i in x]\\ndef variance (x):\\n    \"\"\"assumes x has at least two elements\"\"\"\\n    n = len(x)\\n    deviations  = de_mean(x)\\n    return sum_of_squares (deviations ) / (n - 1)\\nvariance (num_friends ) # 81.54\\nThis looks like it is almost the average squared deviation from the\\nmean, except that we’re dividing by n-1 instead of n. In fact, when\\nwe’re dealing with a sample from a larger population, x_bar  is only\\nan estimate  of the actual mean, which means that on average (x_i\\n- x_bar) ** 2  is an underestimate of x_i’s squared deviation from\\nthe mean, which is why we divide by n-1 instead of n. See Wikipe‐\\ndia.\\nDescribing a Single Set of Data | 61\\nwww.it-ebooks.infoNow, whatever units our data is in (e.g., “friends”), all of our measures of central ten‐\\ndency are in that same unit. The range will similarly be in that same unit. The var‐\\niance, on the other hand, has units that are the square  of the original units (e.g.,\\n“friends squared”). As it can be hard to make sense of these, we often  look instead at\\nthe standard deviation :\\ndef standard_deviation (x):\\n    return math.sqrt(variance (x))\\nstandard_deviation (num_friends ) # 9.03\\nBoth the range and the standard deviation have the same outlier problem that we saw\\nearlier for the mean. Using the same example, if our friendliest user had instead 200\\nfriends, the standard deviation would be 14.89, more than 60% higher!\\nA more robust alternative computes the difference between the 75th percentile value\\nand the 25th percentile value:\\ndef interquartile_range (x):\\n    return quantile (x, 0.75) - quantile (x, 0.25)\\ninterquartile_range (num_friends ) # 6\\nwhich is quite plainly unaffected by a small number of outliers.\\nCorrelation\\nDataSciencester’s VP of Growth has a theory that the amount of time people spend\\non the site is related to the number of friends they have on the site (she’s not a VP for\\nnothing), and she’s asked you to verify this.\\nAfter digging through traffic logs, you’ve come up with a list daily_minutes  that\\nshows how many minutes per day each user spends on DataSciencester, and you’ve\\nordered it so that its elements correspond to the elements of our previous\\nnum_friends  list. We’ d like to investigate the relationship between these two metrics.\\nWe’ll first look at covariance , the paired analogue of variance. Whereas variance\\nmeasures how a single variable deviates from its mean, covariance measures how two\\nvariables vary in tandem from their means:\\ndef covariance (x, y):\\n    n = len(x)\\n    return dot(de_mean(x), de_mean(y)) / (n - 1)\\ncovariance (num_friends , daily_minutes ) # 22.43\\nRecall that dot sums up the products of corresponding pairs of elements. When cor‐\\nresponding elements of x and y are either both above their means or both below their\\nmeans, a positive number enters the sum. When one is above its mean and the other\\n62 | Chapter 5: Statistics\\nwww.it-ebooks.infobelow, a negative number enters the sum. Accordingly, a “large” positive covariance\\nmeans that x tends to be large when y is large and small when y is small. A “large”\\nnegative covariance means the opposite—that x tends to be small when y is large and\\nvice versa. A covariance close to zero means that no such relationship exists.\\nNonetheless, this number can be hard to interpret, for a couple of reasons:\\n•Its units are the product of the inputs’ units (e.g., friend-minutes-per-day), which\\ncan be hard to make sense of. (What’s a “friend-minute-per-day”?)\\n•If each user had twice as many friends (but the same number of minutes), the\\ncovariance would be twice as large. But in a sense the variables would be just as\\ninterrelated. Said differently, it’s hard to say what counts as a “large” covariance.\\nFor this reason, it’s more common to look at the correlation , which divides out the\\nstandard deviations of both variables:\\ndef correlation (x, y):\\n    stdev_x = standard_deviation (x)\\n    stdev_y = standard_deviation (y)\\n    if stdev_x > 0 and stdev_y > 0:\\n        return covariance (x, y) / stdev_x / stdev_y\\n    else:\\n        return 0    # if no variation, correlation is zero\\ncorrelation (num_friends , daily_minutes ) # 0.25\\nThe correlation  is unitless and always lies between -1 (perfect anti-correlation) and\\n1 (perfect correlation). A number like 0.25 represents a relatively weak positive corre‐\\nlation.\\nHowever, one thing we neglected to do was examine our data. Check out Figure 5-2 .\\nCorrelation | 63\\nwww.it-ebooks.infoFigure 5-2. Correlation with an outlier\\nThe person with 100 friends (who spends only one minute per day on the site) is a\\nhuge outlier, and correlation can be very sensitive to outliers. What happens if we\\nignore him?\\noutlier = num_friends .index(100)    # index of outlier\\nnum_friends_good  = [x\\n                    for i, x in enumerate (num_friends )\\n                    if i != outlier]\\ndaily_minutes_good  = [x\\n                      for i, x in enumerate (daily_minutes )\\n                      if i != outlier]\\ncorrelation (num_friends_good , daily_minutes_good ) # 0.57\\nWithout the outlier, there is a much stronger correlation ( Figure 5-3 ).\\n64 | Chapter 5: Statistics\\nwww.it-ebooks.infoFigure 5-3. Correlation after  removing the outlier\\nY ou investigate further and discover that the outlier was actually an internal test\\naccount that no one ever bothered to remove. So you feel pretty justified in excluding\\nit.\\nSimpson’s Paradox\\nOne not uncommon surprise when analyzing data is Simpson’s Paradox, in which\\ncorrelations can be misleading when confounding  variables are ignored.\\nFor example, imagine that you can identify all of your members as either East Coast\\ndata scientists or West Coast data scientists. Y ou decide to examine which coast’s data\\nscientists are friendlier:\\ncoast # of members avg. # of friends\\nWest Coast 101 8.2\\nEast Coast 103 6.5\\nSimpson’s Paradox | 65\\nwww.it-ebooks.infoIt certainly looks like the West Coast data scientists are friendlier than the East Coast\\ndata scientists. Y our coworkers advance all sorts of theories as to why this might be:\\nmaybe it’s the sun, or the coffee, or the organic produce, or the laid-back Pacific vibe?\\nWhen playing with the data you discover something very strange. If you only look at\\npeople with PhDs, the East Coast data scientists have more friends on average. And if\\nyou only look at people without PhDs, the East Coast data scientists also have more\\nfriends on average!\\ncoast degree # of members avg. # of friends\\nWest Coast PhD 35 3.1\\nEast Coast PhD 70 3.2\\nWest Coast no PhD 66 10.9\\nEast Coast no PhD 33 13.4\\nOnce you account for the users’ degrees, the correlation goes in the opposite direc‐\\ntion! Bucketing the data as East Coast/West Coast disguised the fact that the East\\nCoast data scientists skew much more heavily toward PhD types.\\nThis phenomenon crops up in the real world with some regularity. The key issue is\\nthat correlation is measuring the relationship between your two variables all else being\\nequal . If your data classes are assigned at random, as they might be in a well-designed\\nexperiment, “all else being equal” might not be a terrible assumption. But when there\\nis a deeper pattern to class assignments, “all else being equal” can be an awful assump‐\\ntion.\\nThe only real way to avoid this is by knowing your data  and by doing what you can to\\nmake sure you’ve checked for possible confounding factors. Obviously, this is not\\nalways possible. If you didn’t have the educational attainment of these 200 data scien‐\\ntists, you might simply conclude that there was something inherently more sociable\\nabout the West Coast.\\nSome Other Correlational Caveats\\nA correlation of zero indicates that there is no linear relationship between the two\\nvariables. However, there may be other sorts of relationships. For example, if:\\nx = [-2, -1, 0, 1, 2]\\ny = [ 2,  1, 0, 1, 2]\\nthen x and y have zero correlation. But they certainly have a relationship—each ele‐\\nment of y equals the absolute value of the corresponding element of x. What they\\n66 | Chapter 5: Statistics\\nwww.it-ebooks.infodon’t have is a relationship in which knowing how x_i compares to mean(x)  gives us\\ninformation about how y_i compares to mean(y) . That is the sort of relationship that\\ncorrelation looks for.\\nIn addition, correlation tells you nothing about how large the relationship is. The\\nvariables:\\nx = [-2, 1, 0, 1, 2]\\ny = [99.98, 99.99, 100, 100.01, 100.02]\\nare perfectly correlated, but (depending on what you’re measuring) it’s quite possible\\nthat this relationship isn’t all that interesting.\\nCorrelation and Causation\\nY ou have probably heard at some point that “correlation is not causation, ” most likely \\nby someone looking at data that posed a challenge to parts of his worldview that he\\nwas reluctant to question. Nonetheless, this is an important point—if x and y are\\nstrongly correlated, that might mean that x causes y, that y causes x, that each causes\\nthe other, that some third factor causes both, or it might mean nothing.\\nConsider the relationship between num_friends  and daily_minutes . It’s possible that\\nhaving more friends on the site causes  DataSciencester users to spend more time on\\nthe site. This might be the case if each friend posts a certain amount of content each\\nday, which means that the more friends you have, the more time it takes to stay cur‐\\nrent with their updates.\\nHowever, it’s also possible that the more time you spend arguing in the DataSciences‐\\nter forums, the more you encounter and befriend like-minded people. That is, spend‐\\ning more time on the site causes  users to have more friends.\\nA third possibility is that the users who are most passionate about data science spend\\nmore time on the site (because they find it more interesting) and more actively collect\\ndata science friends (because they don’t want to associate with anyone else).\\nOne way to feel more confident about causality is by conducting randomized trials. If\\nyou can randomly split your users into two groups with similar demographics and\\ngive one of the groups a slightly different experience, then you can often feel pretty\\ngood that the different experiences are causing the different outcomes.\\nFor instance, if you don’t mind being angrily accused of experimenting on your users ,\\nyou could randomly choose a subset of your users and show them content from only\\na fraction of their friends. If this subset subsequently spent less time on the site, this\\nwould give you some confidence that having more friends causes  more time on the\\nsite.\\nCorrelation and Causation | 67\\nwww.it-ebooks.infoFor Further Exploration\\n•SciPy , pandas , and StatsModels  all come with a wide variety of statistical func‐\\ntions.\\n•Statistics is important . (Or maybe statistics are important?) If you want to be a\\ngood data scientist it would be a good idea to read a statistics textbook. Many are\\nfreely available online. A couple that I like are:\\n—OpenIntro Statistics\\n—OpenStax Introductory Statistics\\n68 | Chapter 5: Statistics\\nwww.it-ebooks.infoCHAPTER 6\\nProbability\\nThe laws of probability, so true in general, so fallacious in particular.\\n—Edward Gibbon\\nIt is hard to do data science without  some sort of understanding of probability  and its\\nmathematics. As with our treatment of statistics in Chapter 5 , we’ll wave our hands a\\nlot and elide many of the technicalities.\\nFor our purposes you should think of probability as a way of quantifying the uncer‐\\ntainty associated with events  chosen from a some universe  of events. Rather than get‐\\nting technical about what these terms mean, think of rolling a die. The universe\\nconsists of all possible outcomes. And any subset of these outcomes is an event; for\\nexample, “the die rolls a one” or “the die rolls an even number. ”\\nNotationally, we write PE to mean “the probability of the event E. ”\\nWe’ll use probability theory to build models. We’ll use probability theory to evaluate\\nmodels. We’ll use probability theory all over the place.\\nOne could, were one so inclined, get really deep into the philosophy of what probabil‐\\nity theory means . (This is best done over beers.) We won’t be doing that.\\nDependence and Independence\\nRoughly speaking, we say that two events E and F are dependent  if knowing some‐\\nthing about whether E happens gives us information about whether F happens (and\\nvice versa). Otherwise they are independent .\\nFor instance, if we flip a fair coin twice, knowing whether the first flip is Heads gives\\nus no information about whether the second flip is Heads. These events are inde‐\\npendent. On the other hand, knowing whether the first flip is Heads certainly gives us\\n69\\nwww.it-ebooks.infoinformation about whether both flips are Tails. (If the first flip is Heads, then defi‐\\nnitely it’s not the case that both flips are Tails.) These two events are dependent.\\nMathematically, we say that two events E and F are independent if the probability that\\nthey both happen is the product of the probabilities that each one happens:\\nPE,F=PEPF\\nIn the example above, the probability of “first flip Heads” is 1/2, and the probability of\\n“both flips Tails” is 1/4, but the probability of “first flip Heads and both flips Tails” is\\n0.\\nConditional Probability\\nWhen two events E and F are independent, then by definition we have:\\nPE,F=PEPF\\nIf they are not necessarily independent (and if the probability of F is not zero), then\\nwe define the probability of E “conditional on F” as:\\nPEF=PE,F/PF\\nY ou should think of this as the probability that E happens, given that we know that F\\nhappens.\\nWe often rewrite this as:\\nPE,F=PEFPF\\nWhen E and F are independent, you can check that this gives:\\nPEF=PE\\nwhich is the mathematical way of expressing that knowing F occurred gives us no\\nadditional information about whether E occurred.\\nOne common tricky example involves a family with two (unknown) children.\\nIf we assume that:\\n1.Each child is equally likely to be a boy or a girl\\n2.The gender of the second child is independent of the gender of the first child\\n70 | Chapter 6: Probability\\nwww.it-ebooks.infothen the event “no girls” has probability 1/4, the event “one girl, one boy” has proba‐\\nbility 1/2, and the event “two girls” has probability 1/4.\\nNow we can ask what is the probability of the event “both children are girls” ( B) con‐\\nditional on the event “the older child is a girl” ( G)? Using the definition of conditional\\nprobability:\\nPBG=PB,G/PG=PB/PG= 1/2\\nsince the event B and G (“both children are girls and the older child is a girl”) is just\\nthe event B. (Once you know that both children are girls, it’s necessarily true that the\\nolder child is a girl.)\\nMost likely this result accords with your intuition.\\nWe could also ask about the probability of the event “both children are girls” condi‐\\ntional on the event “at least one of the children is a girl” ( L). Surprisingly, the answer\\nis different from before!\\nAs before, the event B and L (“both children are girls and at least one of the children\\nis a girl”) is just the event B. This means we have:\\nPBL=PB,L/PL=PB/PL= 1/3\\nHow can this be the case? Well, if all you know is that at least one of the children is a\\ngirl, then it is twice as likely that the family has one boy and one girl than that it has\\nboth girls.\\nWe can check this by “generating” a lot of families:\\ndef random_kid ():\\n    return random.choice([\"boy\", \"girl\"])\\nboth_girls  = 0\\nolder_girl  = 0\\neither_girl  = 0\\nrandom.seed(0)\\nfor _ in range(10000):\\n    younger = random_kid ()\\n    older = random_kid ()\\n    if older == \"girl\":\\n        older_girl  += 1\\n    if older == \"girl\" and younger == \"girl\":\\n        both_girls  += 1\\n    if older == \"girl\" or younger == \"girl\":\\n        either_girl  += 1\\nConditional Probability | 71\\nwww.it-ebooks.infoprint \"P(both | older):\" , both_girls  / older_girl       # 0.514 ~ 1/2\\nprint \"P(both | either): \" , both_girls  / either_girl    # 0.342 ~ 1/3\\nBayes’s Theorem\\nOne of the data scientist’s best friends is Bayes’s Theorem, which is a way of “revers‐\\ning” conditional probabilities. Let’s say we need to know the probability of some event\\nE conditional on some other event F occurring. But we only have information about\\nthe probability of F conditional on E occurring. Using the definition of conditional\\nprobability twice tells us that:\\nPEF=PE,F/PF=PFEPE/PF\\nThe event F can be split into the two mutually exclusive events “ F and E” and “ F and\\nnot E. ” If we write ¬E for “not E” (i.e., “ E doesn’t happen”), then:\\nPF=PF,E+PF, ¬E\\nso that:\\nPEF=PFEPE/PFEPE+PF¬EP¬E\\nwhich is how Bayes’s Theorem is often stated.\\nThis theorem often gets used to demonstrate why data scientists are smarter than\\ndoctors. Imagine a certain disease that affects 1 in every 10,000 people. And imagine\\nthat there is a test for this disease that gives the correct result (“diseased” if you have\\nthe disease, “nondiseased” if you don’t) 99% of the time.\\nWhat does a positive test mean? Let’s use T for the event “your test is positive” and D\\nfor the event “you have the disease. ” Then Bayes’s Theorem says that the probability\\nthat you have the disease, conditional on testing positive, is:\\nPDT=PTDPD/PTDPD+PT¬DP¬D\\nHere we know that PTD, the probability that someone with the disease tests posi‐\\ntive, is 0.99. PD, the probability that any given person has the disease, is 1/10,000 =\\n0.0001. PT¬D, the probability that someone without the disease tests positive, is\\n0.01. And P¬D, the probability that any given person doesn’t have the disease, is\\n0.9999. If you substitute these numbers into Bayes’s Theorem you find\\nPDT= 0 . 98 %\\n72 | Chapter 6: Probability\\nwww.it-ebooks.infoThat is, less than 1% of the people who test positive actually have the disease.\\nThis assumes that people take the test more or less at random. If\\nonly people with certain symptoms take the test we would instead\\nhave to condition on the event “positive test and symptoms” and\\nthe number would likely be a lot higher.\\nWhile this is a simple calculation for a data scientist, most doctors will guess that\\nPDT is approximately 2.\\nA more intuitive way to see this is to imagine a population of 1 million people. Y ou’ d\\nexpect 100 of them to have the disease, and 99 of those 100 to test positive. On the\\nother hand, you’ d expect 999,900 of them not to have the disease, and 9,999 of those\\nto test positive. Which means that you’ d expect only 99 out of (99 + 9999) positive\\ntesters to actually have the disease.\\nRandom Variables\\nA random variable  is a variable whose possible values have an associated probability\\ndistribution. A very simple random variable equals 1 if a coin flip turns up heads and\\n0 if the flip turns up tails. A more complicated one might measure the number of\\nheads observed when flipping a coin 10 times or a value picked from range(10)\\nwhere each number is equally likely.\\nThe associated distribution gives the probabilities that the variable realizes each of its\\npossible values. The coin flip variable equals 0 with probability 0.5 and 1 with proba‐\\nbility 0.5. The range(10)  variable has a distribution that assigns probability 0.1 to\\neach of the numbers from 0 to 9.\\nWe will sometimes talk about the expected value  of a random variable, which is the\\naverage of its values weighted by their probabilities. The coin flip variable has an\\nexpected value of 1/2 (= 0 * 1/2 + 1 * 1/2), and the range(10)  variable has an\\nexpected value of 4.5.\\nRandom variables can be conditioned  on events just as other events can. Going back\\nto the two-child example from “Conditional Probability”  on page 70, if X is the ran‐\\ndom variable representing the number of girls, X equals 0 with probability 1/4, 1 with\\nprobability 1/2, and 2 with probability 1/4.\\nWe can define a new random variable Y that gives the number of girls conditional on\\nat least one of the children being a girl. Then Y equals 1 with probability 2/3 and 2\\nwith probability 1/3. And a variable Z that’s the number of girls conditional on the\\nolder child being a girl equals 1 with probability 1/2 and 2 with probability 1/2.\\nRandom Variables | 73\\nwww.it-ebooks.infoFor the most part, we will be using random variables implicitly  in what we do without\\ncalling special attention to them. But if you look deeply you’ll see them.\\nContinuous Distributions\\nA coin flip corresponds to a discrete distribution —one that associates positive proba‐\\nbility with discrete outcomes. Often we’ll want to model distributions across a contin‐\\nuum of outcomes. (For our purposes, these outcomes will always be real numbers,\\nalthough that’s not always the case in real life.) For example, the uniform distribution\\nputs equal weight  on all the numbers between 0 and 1.\\nBecause there are infinitely many numbers between 0 and 1, this means that the\\nweight it assigns to individual points must necessarily be zero. For this reason, we\\nrepresent a continuous distribution with a probability density function  (pdf) such that\\nthe probability of seeing a value in a certain interval equals the integral of the density\\nfunction over the interval.\\nIf your integral calculus is rusty, a simpler way of understanding\\nthis is that if a distribution has density function f, then the proba‐\\nbility of seeing a value between x and x+h is approximately\\nh*fx if h is small.\\nThe density function for the uniform distribution is just:\\ndef uniform_pdf (x):\\n    return 1 if x >= 0 and x < 1 else 0\\nThe probability that a random variable following that distribution is between 0.2 and\\n0.3 is 1/10, as you’ d expect. Python’s random.random()  is a [pseudo]random variable\\nwith a uniform density.\\nWe will often be more interested in the cumulative distribution function  (cdf), which\\ngives the probability that a random variable is less than or equal to a certain value. It’s\\nnot hard to create the cumulative distribution function for the uniform distribution\\n(Figure 6-1 ):\\ndef uniform_cdf (x):\\n    \"returns the probability that a uniform random variable is <= x\"\\n    if x < 0:   return 0    # uniform random is never less than 0\\n    elif x < 1: return x    # e.g. P(X <= 0.4) = 0.4\\n    else:       return 1    # uniform random is always less than 1\\n74 | Chapter 6: Probability\\nwww.it-ebooks.infoFigure 6-1. The uniform cdf\\nThe Normal Distribution\\nThe normal distribution is the king of distributions.  It is the classic bell curve–shaped\\ndistribution and is completely determined by two parameters: its mean μ (mu) and its\\nstandard deviation σ (sigma). The mean indicates where the bell is centered, and the\\nstandard deviation how “wide” it is.\\nIt has the distribution function:\\nfxμ,σ=1\\n2πσexp −x−μ2\\n2σ2\\nwhich we can implement as:\\ndef normal_pdf (x, mu=0, sigma=1):\\n    sqrt_two_pi  = math.sqrt(2 * math.pi)\\n    return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (sqrt_two_pi  * sigma))\\nIn Figure 6-2 , we plot some of these pdfs to see what they look like:\\nThe Normal Distribution | 75\\nwww.it-ebooks.infoxs = [x / 10.0 for x in range(-50, 50)]\\nplt.plot(xs,[normal_pdf (x,sigma=1) for x in xs],\\'-\\',label=\\'mu=0,sigma=1\\' )\\nplt.plot(xs,[normal_pdf (x,sigma=2) for x in xs],\\'--\\',label=\\'mu=0,sigma=2\\' )\\nplt.plot(xs,[normal_pdf (x,sigma=0.5) for x in xs],\\':\\',label=\\'mu=0,sigma=0.5\\' )\\nplt.plot(xs,[normal_pdf (x,mu=-1)   for x in xs],\\'-.\\',label=\\'mu=-1,sigma=1\\' )\\nplt.legend()\\nplt.title(\"Various Normal pdfs\" )\\nplt.show()\\nFigure 6-2. Various normal pdfs\\nWhen μ= 0 and σ= 1, it’s called the standard normal distribution . If Z is a standard\\nnormal random variable, then it turns out that:\\nX=σZ+μ\\nis also normal but with mean μ and standard deviation σ. Conversely, if X is a normal\\nrandom variable with mean μ and standard deviation σ,\\nZ=X−μ/σ\\nis a standard normal variable.\\n76 | Chapter 6: Probability\\nwww.it-ebooks.infoThe cumulative distribution function for the normal distribution cannot be written in\\nan “elementary” manner, but we can write it using Python’s math.erf :\\ndef normal_cdf (x, mu=0,sigma=1):\\n    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\\nAgain, in Figure 6-3 , we plot a few:\\nxs = [x / 10.0 for x in range(-50, 50)]\\nplt.plot(xs,[normal_cdf (x,sigma=1) for x in xs],\\'-\\',label=\\'mu=0,sigma=1\\' )\\nplt.plot(xs,[normal_cdf (x,sigma=2) for x in xs],\\'--\\',label=\\'mu=0,sigma=2\\' )\\nplt.plot(xs,[normal_cdf (x,sigma=0.5) for x in xs],\\':\\',label=\\'mu=0,sigma=0.5\\' )\\nplt.plot(xs,[normal_cdf (x,mu=-1) for x in xs],\\'-.\\',label=\\'mu=-1,sigma=1\\' )\\nplt.legend(loc=4) # bottom right\\nplt.title(\"Various Normal cdfs\" )\\nplt.show()\\nFigure 6-3. Various normal cdfs\\nSometimes we’ll need to invert normal_cdf  to find the value corresponding to a\\nspecified probability. There’s no simple way to compute its inverse, but normal_cdf  is \\ncontinuous and strictly increasing, so we can use a binary search :\\ndef inverse_normal_cdf (p, mu=0, sigma=1, tolerance =0.00001):\\n    \"\"\"find approximate inverse using binary search\"\"\"\\nThe Normal Distribution | 77\\nwww.it-ebooks.info    # if not standard, compute standard and rescale\\n    if mu != 0 or sigma != 1:\\n        return mu + sigma * inverse_normal_cdf (p, tolerance =tolerance )\\n    low_z, low_p = -10.0, 0            # normal_cdf(-10) is (very close to) 0\\n    hi_z,  hi_p  =  10.0, 1            # normal_cdf(10)  is (very close to) 1\\n    while hi_z - low_z > tolerance :\\n        mid_z = (low_z + hi_z) / 2     # consider the midpoint\\n        mid_p = normal_cdf (mid_z)      # and the cdf\\'s value there\\n        if mid_p < p:\\n            # midpoint is still too low, search above it\\n            low_z, low_p = mid_z, mid_p\\n        elif mid_p > p:\\n            # midpoint is still too high, search below it\\n            hi_z, hi_p = mid_z, mid_p\\n        else:\\n            break\\n    return mid_z\\nThe function repeatedly bisects intervals until it narrows in on a Z that’s close enough\\nto the desired probability.\\nThe Central Limit Theorem\\nOne reason the normal distribution is so useful is the central limit theorem , which\\nsays (in essence) that a random variable defined as the average of a large number of\\nindependent and identically distributed random variables is itself approximately nor‐\\nmally distributed.\\nIn particular, if x1, ...,xn are random variables with mean μ and standard deviation σ,\\nand if n is large, then:\\n1\\nnx1+ ... + xn\\nis approximately normally distributed with mean μ and standard deviation σ/n.\\nEquivalently (but often more usefully),\\nx1+ ... + xn−μn\\nσn\\nis approximately normally distributed with mean 0 and standard deviation 1.\\nAn easy way to illustrate this is by looking at binomial  random variables, which have\\ntwo parameters n and p. A Binomial(n,p) random variable is simply the sum of n\\n78 | Chapter 6: Probability\\nwww.it-ebooks.infoindependent Bernoulli(p) random variables, each of which equals 1 with probability p\\nand 0 with probability 1 −p:\\ndef bernoulli_trial (p):\\n    return 1 if random.random() < p else 0\\ndef binomial (n, p):\\n    return sum(bernoulli_trial (p) for _ in range(n))\\nThe mean of a Bernoulli(p) variable is p, and its standard deviation is p1 −p. The\\ncentral limit theorem says that as n gets large, a Binomial(n,p) variable is approxi‐\\nmately a normal random variable with mean μ=np and standard deviation\\nσ=np1 −p. If we plot both, you can easily see the resemblance:\\ndef make_hist (p, n, num_points ):\\n    data = [binomial (n, p) for _ in range(num_points )]\\n    # use a bar chart to show the actual binomial samples\\n    histogram  = Counter(data)\\n    plt.bar([x - 0.4 for x in histogram .keys()],\\n            [v / num_points  for v in histogram .values()],\\n            0.8,\\n            color=\\'0.75\\')\\n    mu = p * n\\n    sigma = math.sqrt(n * p * (1 - p))\\n    # use a line chart to show the normal approximation\\n    xs = range(min(data), max(data) + 1)\\n    ys = [normal_cdf (i + 0.5, mu, sigma) - normal_cdf (i - 0.5, mu, sigma)\\n          for i in xs]\\n    plt.plot(xs,ys)\\n    plt.title(\"Binomial Distribution vs. Normal Approximation\" )\\n    plt.show()\\nFor example, when you call make_hist(0.75, 100, 10000) , you get the graph in\\nFigure 6-4 .\\nThe Central Limit Theorem | 79\\nwww.it-ebooks.infoFigure 6-4. The output from make_hist\\nThe moral of this approximation is that if you want to know the probability that (say)\\na fair coin turns up more than 60 heads in 100 flips, you can estimate it as the proba‐\\nbility that a Normal(50,5) is greater than 60, which is easier than computing the Bino‐\\nmial(100,0.5) cdf. (Although in most applications you’ d probably be using statistical\\nsoftware that would gladly compute whatever probabilities you want.)\\nFor Further Exploration\\n•scipy.stats  contains pdf and cdf functions for most of the popular probability dis‐\\ntributions.\\n•Remember how, at the end of Chapter 5 , I said that it would be a good idea to\\nstudy a statistics textbook? It would also be a good idea to study a probability\\ntextbook. The best one I know that’s available online is Introduction to Probabil‐\\nity.\\n80 | Chapter 6: Probability\\nwww.it-ebooks.infoCHAPTER 7\\nHypothesis and Inference\\nIt is the mark of a truly intelligent person to be moved by statistics.\\n—George Bernard Shaw\\nWhat will we do with all this statistics and probability theory? The science  part of data\\nscience frequently involves forming and testing hypotheses  about our data and the\\nprocesses that generate it.\\nStatistical Hypothesis Testing\\nOften, as data scientists, we’ll want to test whether a certain hypothesis is likely to be\\ntrue. For our purposes, hypotheses are assertions like “this coin is fair” or “data scien‐\\ntists prefer Python to R” or “people are more likely to navigate away from the page\\nwithout ever reading the content if we pop up an irritating interstitial advertisement\\nwith a tiny, hard-to-find close button” that can be translated into statistics about data.\\nUnder various assumptions, those statistics can be thought of as observations of ran‐\\ndom variables from known distributions, which allows us to make statements about\\nhow likely those assumptions are to hold.\\nIn the classical setup, we have a null hypothesis  H0 that represents some default posi‐\\ntion, and some alternative hypothesis H1 that we’ d like to compare it with. We use sta‐\\ntistics to decide whether we can reject H0 as false or not. This will probably make\\nmore sense with an example.\\nExample: Flipping a Coin\\nImagine we have a coin and we want to test whether it’s fair. We’ll make the assump‐\\ntion that the coin has some probability p of landing heads, and so our null hypothesis\\n81\\nwww.it-ebooks.infois that the coin is fair—that is, that p= 0 . 5 . We’ll test this against the alternative\\nhypothesis p≠ 0 . 5 .\\nIn particular, our test will involve flipping the coin some number n times and count‐\\ning the number of heads X. Each coin flip is a Bernoulli trial, which means that X is a\\nBinomial(n,p) random variable, which (as we saw in Chapter 6 ) we can approximate\\nusing the normal distribution:\\ndef normal_approximation_to_binomial (n, p):\\n    \"\"\"finds mu and sigma corresponding to a Binomial(n, p)\"\"\"\\n    mu = p * n\\n    sigma = math.sqrt(p * (1 - p) * n)\\n    return mu, sigma\\nWhenever a random variable follows a normal distribution, we can use normal_cdf\\nto figure out the probability that its realized value lies within (or outside) a particular\\ninterval:\\n# the normal cdf _is_ the probability the variable is below a threshold\\nnormal_probability_below  = normal_cdf\\n# it\\'s above the threshold if it\\'s not below the threshold\\ndef normal_probability_above (lo, mu=0, sigma=1):\\n    return 1 - normal_cdf (lo, mu, sigma)\\n# it\\'s between if it\\'s less than hi, but not less than lo\\ndef normal_probability_between (lo, hi, mu=0, sigma=1):\\n    return normal_cdf (hi, mu, sigma) - normal_cdf (lo, mu, sigma)\\n# it\\'s outside if it\\'s not between\\ndef normal_probability_outside (lo, hi, mu=0, sigma=1):\\n    return 1 - normal_probability_between (lo, hi, mu, sigma)\\nWe can also do the reverse—find either the nontail region or the (symmetric) interval\\naround the mean that accounts for a certain level of likelihood. For example, if we\\nwant to find an interval centered at the mean and containing 60% probability, then\\nwe find the cutoffs where the upper and lower tails each contain 20% of the probabil‐\\nity (leaving 60%):\\ndef normal_upper_bound (probability , mu=0, sigma=1):\\n    \"\"\"returns the z for which P(Z <= z) = probability\"\"\"\\n    return inverse_normal_cdf (probability , mu, sigma)\\ndef normal_lower_bound (probability , mu=0, sigma=1):\\n    \"\"\"returns the z for which P(Z >= z) = probability\"\"\"\\n    return inverse_normal_cdf (1 - probability , mu, sigma)\\ndef normal_two_sided_bounds (probability , mu=0, sigma=1):\\n    \"\"\"returns the symmetric (about the mean) bounds\\n    that contain the specified probability\"\"\"\\n    tail_probability  = (1 - probability ) / 2\\n82 | Chapter 7: Hypothesis and Inference\\nwww.it-ebooks.info    # upper bound should have tail_probability above it\\n    upper_bound  = normal_lower_bound (tail_probability , mu, sigma)\\n    # lower bound should have tail_probability below it\\n    lower_bound  = normal_upper_bound (tail_probability , mu, sigma)\\n    return lower_bound , upper_bound\\nIn particular, let’s say that we choose to flip the coin n= 1000  times. If our hypothesis\\nof fairness is true, X should be distributed approximately normally with mean 50 and\\nstandard deviation 15.8:\\nmu_0, sigma_0 = normal_approximation_to_binomial (1000, 0.5)\\nWe need to make a decision about significance —how willing we are to make a type 1\\nerror  (“false positive”), in which  we reject H0 even though it’s true. For reasons lost to\\nthe annals of history, this willingness is often set at 5% or 1%. Let’s choose 5%.\\nConsider the test that rejects H0 if X falls outside the bounds given by:\\nnormal_two_sided_bounds (0.95, mu_0, sigma_0)   # (469, 531)\\nAssuming p really equals 0.5 (i.e., H0 is true), there is just a 5% chance we observe an\\nX that lies outside this interval, which is the exact significance we wanted. Said differ‐\\nently, if H0 is true, then, approximately 19 times out of 20, this test will give the cor‐\\nrect result.\\nWe are also often interested in the power  of a test, which is the probability of not\\nmaking a type 2 error , in which we fail to reject H0 even though it’s false. In order to\\nmeasure this, we have to specify what exactly H0 being false means . (Knowing merely\\nthat p is not 0.5 doesn’t give you a ton of information about the distribution of X.) In\\nparticular, let’s check what happens if p is really 0.55, so that the coin is slightly biased\\ntoward heads.\\nIn that case, we can calculate the power of the test with:\\n# 95% bounds based on assumption p is 0.5\\nlo, hi = normal_two_sided_bounds (0.95, mu_0, sigma_0)\\n# actual mu and sigma based on p = 0.55\\nmu_1, sigma_1 = normal_approximation_to_binomial (1000, 0.55)\\n# a type 2 error means we fail to reject the null hypothesis\\n# which will happen when X is still in our original interval\\ntype_2_probability  = normal_probability_between (lo, hi, mu_1, sigma_1)\\npower = 1 - type_2_probability       # 0.887\\nImagine instead that our null hypothesis was that the coin is not biased toward heads,\\nor that p≤ 0 . 5 . In that case we want a one-sided test  that rejects the null hypothesis\\nwhen X is much larger than 50 but not when X is smaller than 50. So a 5%-\\nExample: Flipping a Coin | 83\\nwww.it-ebooks.infosignificance test involves using normal_probability_below  to find the cutoff below\\nwhich 95% of the probability lies:\\nhi = normal_upper_bound (0.95, mu_0, sigma_0)\\n# is 526 (< 531, since we need more probability in the upper tail)\\ntype_2_probability  = normal_probability_below (hi, mu_1, sigma_1)\\npower = 1 - type_2_probability       # 0.936\\nThis is a more powerful test, since it no longer rejects H0 when X is below 469 (which\\nis very unlikely to happen if H1 is true) and instead rejects H0 when X is between 526\\nand 531 (which is somewhat likely to happen if H1 is true). === p-values\\nAn alternative way of thinking about the preceding test involves p-values . Instead of\\nchoosing bounds based on some probability cutoff, we compute the probability—\\nassuming H0 is true—that we would see a value at least as extreme as the one we\\nactually observed.\\nFor our two-sided test of whether the coin is fair, we compute:\\ndef two_sided_p_value (x, mu=0, sigma=1):\\n    if x >= mu:\\n        # if x is greater than the mean, the tail is what\\'s greater than x\\n        return 2 * normal_probability_above (x, mu, sigma)\\n    else:\\n        # if x is less than the mean, the tail is what\\'s less than x\\n        return 2 * normal_probability_below (x, mu, sigma)\\nIf we were to see 530 heads, we would compute:\\ntwo_sided_p_value (529.5, mu_0, sigma_0)   # 0.062\\nWhy did we use 529.5 instead of 530? This is what’s called a con‐\\ntinuity correction . It reflects the fact that normal_probabil\\nity_between(529.5, 530.5, mu_0, sigma_0)  is a better estimate\\nof the probability of seeing 530 heads than normal_probabil\\nity_between(530, 531, mu_0, sigma_0)  is.\\nCorrespondingly, normal_probability_above(529.5, mu_0,\\nsigma_0)  is a better estimate of the probability of seeing at least\\n530 heads. Y ou may have noticed that we also used this in the code\\nthat produced Figure 6-4 .\\nOne way to convince yourself that this is a sensible estimate is with a simulation:\\nextreme_value_count  = 0\\nfor _ in range(100000):\\n    num_heads  = sum(1 if random.random() < 0.5 else 0    # count # of heads\\n                    for _ in range(1000))                # in 1000 flips\\n    if num_heads  >= 530 or num_heads  <= 470:             # and count how often\\n84 | Chapter 7: Hypothesis and Inference\\nwww.it-ebooks.info        extreme_value_count  += 1                         # the # is \\'extreme\\'\\nprint extreme_value_count  / 100000   # 0.062\\nSince the p-value is greater than our 5% significance, we don’t reject the null. If we\\ninstead saw 532 heads, the p-value would be:\\ntwo_sided_p_value (531.5, mu_0, sigma_0)   # 0.0463\\nwhich is smaller than the 5% significance, which means we would reject the null. It’s\\nthe exact same test as before. It’s just a different way of approaching the statistics.\\nSimilarly, we would have:\\nupper_p_value  = normal_probability_above\\nlower_p_value  = normal_probability_below\\nFor our one-sided test, if we saw 525 heads we would compute:\\nupper_p_value (524.5, mu_0, sigma_0) # 0.061\\nwhich means we wouldn’t reject the null. If we saw 527 heads, the computation would\\nbe:\\nupper_p_value (526.5, mu_0, sigma_0) # 0.047\\nand we would reject the null.\\nMake sure your data is roughly normally distributed before using\\nnormal_probability_above  to compute p-values. The annals of\\nbad data science are filled with examples of people opining that the\\nchance of some observed event occurring at random is one in a\\nmillion, when what they really mean is “the chance, assuming the\\ndata is distributed normally, ” which is pretty meaningless if the data\\nisn’t.\\nThere are various statistical tests for normality, but even plotting\\nthe data is a good start.\\nConfidence  Intervals\\nWe’ve been testing hypotheses about the value of the heads probability p, which is a\\nparameter  of the unknown “heads” distribution. When this is the case, a third\\napproach is to construct a confidence  interval  around the observed value of the\\nparameter.\\nFor example, we can estimate the probability of the unfair coin by looking at the aver‐\\nage value of the Bernoulli variables corresponding to each flip—1 if heads, 0 if tails. If\\nwe observe 525 heads out of 1,000 flips, then we estimate p equals 0.525.\\nConfidence  Intervals | 85\\nwww.it-ebooks.infoHow confident  can we be about this estimate? Well, if we knew the exact value of p,\\nthe central limit theorem (recall “The Central Limit Theorem”  on page 78) tells us\\nthat the average of those Bernoulli variables should be approximately normal, with\\nmean p and standard deviation:\\nmath.sqrt(p * (1 - p) / 1000)\\nHere we don’t know p, so instead we use our estimate:\\np_hat = 525 / 1000\\nmu = p_hat\\nsigma = math.sqrt(p_hat * (1 - p_hat) / 1000)   # 0.0158\\nThis is not entirely justified, but people seem to do it anyway. Using the normal\\napproximation, we conclude that we are “95% confident” that the following interval\\ncontains the true parameter p:\\nnormal_two_sided_bounds (0.95, mu, sigma)        # [0.4940, 0.5560]\\nThis is a statement about the interval , not about p. Y ou should\\nunderstand it as the assertion that if you were to repeat the experi‐\\nment many times, 95% of the time the “true” parameter (which is\\nthe same every time) would lie within the observed confidence\\ninterval (which might be different every time).\\nIn particular, we do not conclude that the coin is unfair, since 0.5 falls within our con‐\\nfidence interval.\\nIf instead we’ d seen 540 heads, then we’ d have:\\np_hat = 540 / 1000\\nmu = p_hat\\nsigma = math.sqrt(p_hat * (1 - p_hat) / 1000) # 0.0158\\nnormal_two_sided_bounds (0.95, mu, sigma) # [0.5091, 0.5709]\\nHere, “fair coin” doesn’t lie in the confidence interval. (The “fair coin” hypothesis\\ndoesn’t pass a test that you’ d expect it to pass 95% of the time if it were true.)\\nP-hacking\\nA procedure that erroneously rejects the null hypothesis  only 5% of the time will—by\\ndefinition—5% of the time erroneously reject the null hypothesis:\\ndef run_experiment ():\\n    \"\"\"flip a fair coin 1000 times, True = heads, False = tails\"\"\"\\n    return [random.random() < 0.5 for _ in range(1000)]\\ndef reject_fairness (experiment ):\\n    \"\"\"using the 5% significance levels\"\"\"\\n    num_heads  = len([flip for flip in experiment  if flip])\\n86 | Chapter 7: Hypothesis and Inference\\nwww.it-ebooks.info    return num_heads  < 469 or num_heads  > 531\\nrandom.seed(0)\\nexperiments  = [run_experiment () for _ in range(1000)]\\nnum_rejections  = len([experiment\\n                      for experiment  in experiments\\n                      if reject_fairness (experiment )])\\nprint num_rejections    # 46\\nWhat this means is that if you’re setting out to find “significant” results, you usually\\ncan. Test enough hypotheses against your data set, and one of them will almost cer‐\\ntainly appear significant. Remove the right outliers, and you can probably get your p\\nvalue below 0.05. (We did something vaguely similar in “Correlation” on page 62; did\\nyou notice?)\\nThis is sometimes called P-hacking  and is in some ways a consequence of the “infer‐\\nence from p-values framework. ” A good article criticizing this approach is “The Earth\\nIs Round. ”\\nIf you want to do good science , you should determine your hypotheses before looking\\nat the data, you should clean your data without the hypotheses in mind, and you\\nshould keep in mind that p-values are not substitutes for common sense. (An alterna‐\\ntive approach is “Bayesian Inference” on page 88 .)\\nExample: Running an A/B Test\\nOne of your primary responsibilities at DataSciencester is experience optimization,\\nwhich is a euphemism for trying to get people to click on advertisements. One of\\nyour advertisers has developed a new energy drink targeted at data scientists, and the\\nVP of Advertisements wants your help choosing between advertisement A (“tastes\\ngreat!”) and advertisement B (“less bias!”).\\nBeing a scientist , you decide to run an experiment  by randomly showing site visitors\\none of the two advertisements and tracking how many people click on each one.\\nIf 990 out of 1,000 A-viewers click their ad while only 10 out of 1,000 B-viewers click\\ntheir ad, you can be pretty confident that A is the better ad. But what if the differences\\nare not so stark? Here’s where you’ d use statistical inference.\\nLet’s say that NA people see ad A, and that nA of them click it. We can think of each ad\\nview as a Bernoulli trial where pA is the probability that someone clicks ad A. Then (if\\nNA is large, which it is here) we know that nA/NA is approximately a normal random\\nvariable with mean pA and standard deviation σA=pA1 −pA/NA.\\nSimilarly, nB/NB is approximately a normal random variable with mean pB and stan‐\\ndard deviation σB=pB1 −pB/NB:\\nExample: Running an A/B Test | 87\\nwww.it-ebooks.infodef estimated_parameters (N, n):\\n    p = n / N\\n    sigma = math.sqrt(p * (1 - p) / N)\\n    return p, sigma\\nIf we assume those two normals are independent (which seems reasonable, since the\\nindividual Bernoulli trials ought to be), then their difference should also be normal\\nwith mean pB−pA and standard deviation σA2+σB2.\\nThis is sort of cheating. The math only works out exactly like this if\\nyou know  the standard deviations. Here we’re estimating them\\nfrom the data, which means that we really should be using a t-\\ndistribution. But for large enough data sets, it’s close enough that it\\ndoesn’t make much of a difference.\\nThis means we can test the null hypothesis  that pA and pB are the same (that is, that\\npA−pB is zero), using the statistic:\\ndef a_b_test_statistic (N_A, n_A, N_B, n_B):\\n    p_A, sigma_A = estimated_parameters (N_A, n_A)\\n    p_B, sigma_B = estimated_parameters (N_B, n_B)\\n    return (p_B - p_A) / math.sqrt(sigma_A ** 2 + sigma_B ** 2)\\nwhich should approximately be a standard normal.\\nFor example, if “tastes great” gets 200 clicks out of 1,000 views and “less bias” gets 180\\nclicks out of 1,000 views, the statistic equals:\\nz = a_b_test_statistic (1000, 200, 1000, 180)    # -1.14\\nThe probability of seeing such a large difference if the means were actually equal\\nwould be:\\ntwo_sided_p_value (z)                            # 0.254\\nwhich is large enough that you can’t conclude there’s much of a difference. On the\\nother hand, if “less bias” only got 150 clicks, we’ d have:\\nz = a_b_test_statistic (1000, 200, 1000, 150)    # -2.94\\ntwo_sided_p_value (z)                            # 0.003\\nwhich means there’s only a 0.003 probability you’ d see such a large difference if the\\nads were equally effective.\\nBayesian Inference\\nThe procedures we’ve looked at have involved making probability statements about\\nour tests: “there’s only a 3% chance you’ d observe such an extreme statistic if our null\\nhypothesis were true. ”\\n88 | Chapter 7: Hypothesis and Inference\\nwww.it-ebooks.infoAn alternative approach to inference involves treating the unknown parameters\\nthemselves as random variables. The analyst (that’s you) starts with a prior distribu‐\\ntion for the parameters and then uses the observed data and Bayes’s Theorem to get\\nan updated posterior distribution  for the parameters.  Rather than making probability\\njudgments about the tests, you make probability judgments about the parameters\\nthemselves.\\nFor example, when the unknown parameter is a probability (as in our coin-flipping\\nexample), we often use a prior from the Beta distribution , which puts all its probabil‐\\nity between 0 and 1:\\ndef B(alpha, beta):\\n    \"\"\"a normalizing constant so that the total probability is 1\"\"\"\\n    return math.gamma(alpha) * math.gamma(beta) / math.gamma(alpha + beta)\\ndef beta_pdf (x, alpha, beta):\\n    if x < 0 or x > 1:          # no weight outside of [0, 1]\\n        return 0\\n    return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta)\\nGenerally speaking, this distribution centers its weight at:\\nalpha / (alpha + beta)\\nand the larger alpha  and beta  are, the “tighter” the distribution is.\\nFor example, if alpha  and beta  are both 1, it’s just the uniform distribution (centered\\nat 0.5, very dispersed). If alpha  is much larger than beta , most of the weight is near 1.\\nAnd if alpha  is much smaller than beta , most of the weight is near zero. Figure 7-1\\nshows several different Beta distributions.\\nSo let’s say we assume a prior distribution on p. Maybe we don’t want to take a stand\\non whether the coin is fair, and we choose alpha  and beta  to both equal 1. Or maybe\\nwe have a strong belief that it lands heads 55% of the time, and we choose alpha\\nequals 55, beta  equals 45.\\nThen we flip our coin a bunch of times and see h heads and t tails. Bayes’s Theorem\\n(and some mathematics that’s too tedious for us to go through here) tells us that the\\nposterior distribution for p is again a Beta distribution but with parameters alpha +\\nh and beta + t .\\nIt is no coincidence that the posterior distribution was again a Beta\\ndistribution. The number of heads is given by a Binomial distribu‐\\ntion, and the Beta is the conjugate prior  to the Binomial distribu‐\\ntion. This means that whenever you update a Beta prior using\\nobservations from the corresponding binomial, you will get back a\\nBeta posterior.\\nBayesian Inference | 89\\nwww.it-ebooks.infoFigure 7-1. Example Beta distributions\\nLet’s say you flip the coin 10 times and see only 3 heads.\\nIf you started with the uniform prior (in some sense refusing to take a stand about\\nthe coin’s fairness), your posterior distribution would be a Beta(4, 8), centered around\\n0.33. Since you considered all probabilities equally likely, your best guess is some‐\\nthing pretty close to the observed probability.\\nIf you started with a Beta(20, 20) (expressing the belief that the coin was roughly\\nfair), your posterior distribution would be a Beta(23, 27), centered around 0.46, indi‐\\ncating a revised belief that maybe the coin is slightly biased toward tails.\\nAnd if you started with a Beta(30, 10) (expressing a belief that the coin was biased to\\nflip 75% heads), your posterior distribution would be a Beta(33, 17), centered around\\n0.66. In that case you’ d still believe in a heads bias, but less strongly than you did ini‐\\ntially. These three different posteriors are plotted in Figure 7-2 .\\n90 | Chapter 7: Hypothesis and Inference\\nwww.it-ebooks.infoFigure 7-2. Posteriors arising from different  priors\\nIf you flipped the coin more and more times, the prior would matter less and less\\nuntil eventually you’ d have (nearly) the same posterior distribution no matter which\\nprior you started with.\\nFor example, no matter how biased you initially thought the coin was, it would be\\nhard to maintain that belief after seeing 1,000 heads out of 2,000 flips (unless you are\\na lunatic who picks something like a Beta(1000000,1) prior).\\nWhat’s interesting is that this allows us to make probability statements about hypoth‐\\neses: “Based on the prior and the observed data, there is only a 5% likelihood the\\ncoin’s heads probability is between 49% and 51%. ” This is philosophically very differ‐\\nent from a statement like “if the coin were fair we would expect to observe data so\\nextreme only 5% of the time. ”\\nUsing Bayesian inference to test hypotheses is considered somewhat controversial—\\nin part because its mathematics can get somewhat complicated, and in part because of\\nthe subjective nature of choosing a prior. We won’t use it any further in this book, but\\nit’s good to know about.\\nBayesian Inference | 91\\nwww.it-ebooks.infoFor Further Exploration\\n•We’ve barely scratched the surface of what you should know about statistical\\ninference. The books recommended at the end of Chapter 5  go into a lot more\\ndetail.\\n•Coursera offers a Data Analysis and Statistical Inference  course that covers many\\nof these topics.\\n92 | Chapter 7: Hypothesis and Inference\\nwww.it-ebooks.infoCHAPTER 8\\nGradient Descent\\nThose  who boast of their descent, brag on what they owe to others.\\n—Seneca\\nFrequently when doing data science, we’ll be trying to the find the best model for a\\ncertain situation. And usually “best” will mean something like “minimizes the error\\nof the model” or “maximizes the likelihood of the data. ” In other words, it will repre‐\\nsent the solution to some sort of optimization problem.\\nThis means we’ll need to solve a number of optimization problems. And in particular,\\nwe’ll need to solve them from scratch. Our approach will be a technique called gradi‐\\nent descent , which lends itself pretty well to a from-scratch treatment. Y ou might not\\nfind it super exciting in and of itself, but it will enable us to do exciting things\\nthroughout the book, so bear with me.\\nThe Idea Behind Gradient Descent\\nSuppose we have some function f that takes as input a vector of real numbers and\\noutputs a single real number. One simple such function is:\\ndef sum_of_squares (v):\\n    \"\"\"computes the sum of squared elements in v\"\"\"\\n    return sum(v_i ** 2 for v_i in v)\\nWe’ll frequently need to maximize (or minimize) such functions. That is, we need to\\nfind the input v that produces the largest (or smallest) possible value.\\nFor functions like ours,  the gradient  (if you remember your calculus, this is the vector\\nof partial derivatives) gives the input direction in which the function most quickly\\nincreases. (If you don’t remember your calculus, take my word for it or look it up on\\nthe Internet.)\\n93\\nwww.it-ebooks.infoAccordingly, one approach to maximizing a function is to pick a random starting\\npoint, compute the gradient, take a small step in the direction of the gradient (i.e., the\\ndirection that causes the function to increase the most), and repeat with the new\\nstarting point. Similarly, you can try to minimize a function by taking small steps in\\nthe opposite  direction, as shown in Figure 8-1 .\\nFigure 8-1. Finding a minimum using gradient descent\\nIf a function has a unique global minimum, this procedure is likely\\nto find it. If a function has multiple (local) minima, this procedure\\nmight “find” the wrong one of them, in which case you might re-\\nrun the procedure from a variety of starting points. If a function\\nhas no minimum, then it’s possible the procedure might go on for‐\\never.\\nEstimating the Gradient\\nIf f is a function of one variable, its derivative at a point x measures how f(x)\\nchanges when we make a very small change to x. It is defined as the limit of the dif‐\\nference quotients:\\ndef difference_quotient (f, x, h):\\n    return (f(x + h) - f(x)) / h\\nas h approaches zero.\\n(Many a would-be calculus student has been stymied by the mathematical definition\\nof limit. Here we’ll cheat and simply say that it means what you think it means.)\\n94 | Chapter 8: Gradient Descent\\nwww.it-ebooks.infoFigure 8-2. Approximating a derivative with a difference  quotient\\nThe derivative is the slope of the tangent line at x,fx, while the difference quo‐\\ntient is the slope of the not-quite-tangent line that runs through x+h,fx+h. As h\\ngets smaller and smaller, the not-quite-tangent line gets closer and closer to the tan‐\\ngent line ( Figure 8-2 ).\\nFor many functions it’s easy to exactly calculate derivatives. For example, the square\\nfunction:\\ndef square(x):\\n    return x * x\\nhas the derivative:\\ndef derivative (x):\\n    return 2 * x\\nwhich you can check—if you are so inclined—by explicitly computing the difference\\nquotient and taking the limit.\\nWhat if you couldn’t (or didn’t want to) find the gradient? Although we can’t take lim‐\\nits in Python, we can estimate derivatives by evaluating the difference quotient for a\\nvery small e. Figure 8-3  shows the results of one such estimation:\\nEstimating the Gradient | 95\\nwww.it-ebooks.infoderivative_estimate  = partial(difference_quotient , square, h=0.00001)\\n# plot to show they\\'re basically the same\\nimport matplotlib.pyplot  as plt\\nx = range(-10,10)\\nplt.title(\"Actual Derivatives vs. Estimates\" )\\nplt.plot(x, map(derivative , x), \\'rx\\', label=\\'Actual\\' )             # red  x\\nplt.plot(x, map(derivative_estimate , x), \\'b+\\', label=\\'Estimate\\' )  # blue +\\nplt.legend(loc=9)\\nplt.show()\\nFigure 8-3. Goodness of difference  quotient approximation\\nWhen f is a function of many variables, it has multiple partial derivatives , each indi‐\\ncating how f changes when we make small changes in just one of the input variables.\\nWe calculate its ith partial derivative by treating it as a function of just its ith variable,\\nholding the other variables fixed:\\ndef partial_difference_quotient (f, v, i, h):\\n    \"\"\"compute the ith partial difference quotient of f at v\"\"\"\\n    w = [v_j + (h if j == i else 0)    # add h to just the ith element of v\\n         for j, v_j in enumerate (v)]\\n    return (f(w) - f(v)) / h\\n96 | Chapter 8: Gradient Descent\\nwww.it-ebooks.infoafter which we can estimate the gradient the same way:\\ndef estimate_gradient (f, v, h=0.00001):\\n    return [partial_difference_quotient (f, v, i, h)\\n            for i, _ in enumerate (v)]\\nA major drawback to this “estimate using difference quotients”\\napproach is that it’s computationally expensive. If v has length n,\\nestimate_gradient  has to evaluate f on 2n different inputs. If\\nyou’re repeatedly estimating gradients, you’re doing a whole lot of\\nextra work.\\nUsing the Gradient\\nIt’s easy to see that the sum_of_squares  function is smallest when its input v is a vec‐\\ntor of zeroes. But imagine we didn’t know that. Let’s use gradients to find the mini‐\\nmum among all three-dimensional vectors. We’ll just pick a random starting point\\nand then take tiny steps in the opposite direction of the gradient until we reach a\\npoint where the gradient is very small:\\ndef step(v, direction , step_size ):\\n    \"\"\"move step_size in the direction from v\"\"\"\\n    return [v_i + step_size  * direction_i\\n            for v_i, direction_i  in zip(v, direction )]\\ndef sum_of_squares_gradient (v):\\n    return [2 * v_i for v_i in v]\\n# pick a random starting point\\nv = [random.randint(-10,10) for i in range(3)]\\ntolerance  = 0.0000001\\nwhile True:\\n    gradient  = sum_of_squares_gradient (v)   # compute the gradient at v\\n    next_v = step(v, gradient , -0.01)       # take a negative gradient step\\n    if distance (next_v, v) < tolerance :     # stop if we\\'re converging\\n        break\\n    v = next_v                              # continue if we\\'re not\\nIf you run this, you’ll find that it always ends up with a v that’s very close to [0,0,0] .\\nThe smaller you make the tolerance , the closer it will get.\\nChoosing the Right Step Size\\nAlthough the rationale for moving against the gradient is clear, how far to move is\\nnot. Indeed, choosing the right step size is more of an art than a science. Popular\\noptions include:\\nUsing the Gradient | 97\\nwww.it-ebooks.info•Using a fixed step size\\n•Gradually shrinking the step size over time\\n•At each step, choosing the step size that minimizes the value of the objective\\nfunction\\nThe last sounds optimal but is, in practice, a costly computation. We can approximate\\nit by trying a variety of step sizes and choosing the one that results in the smallest\\nvalue of the objective function:\\nstep_sizes  = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\\nIt is possible that certain step sizes will result in invalid inputs for our function. So\\nwe’ll need to create a “safe apply” function that returns infinity (which should never\\nbe the minimum of anything) for invalid inputs:\\ndef safe(f):\\n    \"\"\"return a new function that\\'s the same as f,\\n    except that it outputs infinity whenever f produces an error\"\"\"\\n    def safe_f(*args, **kwargs):\\n        try:\\n            return f(*args, **kwargs)\\n        except:\\n            return float(\\'inf\\')         # this means \"infinity\" in Python\\n    return safe_f\\nPutting It All Together\\nIn the general case, we have some target_fn  that we want to minimize, and we also\\nhave its gradient_fn . For example, the target_fn  could represent the errors in a\\nmodel as a function of its parameters, and we might want to find the parameters that\\nmake the errors as small as possible.\\nFurthermore, let’s say we have (somehow) chosen a starting value for the parameters\\ntheta_0 . Then we can implement gradient descent as:\\ndef minimize_batch (target_fn , gradient_fn , theta_0, tolerance =0.000001 ):\\n    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\\n    step_sizes  = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\\n    theta = theta_0                           # set theta to initial value\\n    target_fn  = safe(target_fn )               # safe version of target_fn\\n    value = target_fn (theta)                  # value we\\'re minimizing\\n    while True:\\n        gradient  = gradient_fn (theta)\\n        next_thetas  = [step(theta, gradient , -step_size )\\n                       for step_size  in step_sizes ]\\n98 | Chapter 8: Gradient Descent\\nwww.it-ebooks.info        # choose the one that minimizes the error function\\n        next_theta  = min(next_thetas , key=target_fn )\\n        next_value  = target_fn (next_theta )\\n        # stop if we\\'re \"converging\"\\n        if abs(value - next_value ) < tolerance :\\n            return theta\\n        else:\\n            theta, value = next_theta , next_value\\nWe called it minimize_batch  because, for each gradient step, it looks at the entire data\\nset (because target_fn  returns the error on the whole data set). In the next section,\\nwe’ll see an alternative approach that only looks at one data point at a time.\\nSometimes we’ll instead want to maximize  a function, which we can do by minimiz‐\\ning its negative (which has a corresponding negative gradient):\\ndef negate(f):\\n    \"\"\"return a function that for any input x returns -f(x)\"\"\"\\n    return lambda *args, **kwargs: -f(*args, **kwargs)\\ndef negate_all (f):\\n    \"\"\"the same when f returns a list of numbers\"\"\"\\n    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\\ndef maximize_batch (target_fn , gradient_fn , theta_0, tolerance =0.000001 ):\\n    return minimize_batch (negate(target_fn ),\\n                          negate_all (gradient_fn ),\\n                          theta_0,\\n                          tolerance )\\nStochastic Gradient Descent\\nAs we mentioned before, often we’ll be using gradient descent to choose the parame‐\\nters of a model in a way that minimizes some notion of error. Using the previous\\nbatch approach, each gradient step requires us to make a prediction and compute the\\ngradient for the whole data set, which makes each step take a long time.\\nNow, usually these error functions are additive , which means that the predictive error\\non the whole data set is simply the sum of the predictive errors for each data point.\\nWhen this is the case, we can instead apply a technique called stochastic gradient\\ndescent , which computes the gradient (and takes a step) for only one point at a time.\\nIt cycles over our data repeatedly until it reaches a stopping point.\\nDuring each cycle, we’ll want to iterate through our data in a random order:\\ndef in_random_order (data):\\n    \"\"\"generator that returns the elements of data in random order\"\"\"\\n    indexes = [i for i, _ in enumerate (data)]  # create a list of indexes\\n    random.shuffle(indexes)                    # shuffle them\\nStochastic Gradient Descent | 99\\nwww.it-ebooks.info    for i in indexes:                          # return the data in that order\\n        yield data[i]\\nAnd we’ll want to take a gradient step for each data point. This approach leaves the\\npossibility that we might circle around near a minimum forever, so whenever we stop\\ngetting improvements we’ll decrease the step size and eventually quit:\\ndef minimize_stochastic (target_fn , gradient_fn , x, y, theta_0, alpha_0=0.01):\\n    data = zip(x, y)\\n    theta = theta_0                             # initial guess\\n    alpha = alpha_0                             # initial step size\\n    min_theta , min_value  = None, float(\"inf\")   # the minimum so far\\n    iterations_with_no_improvement  = 0\\n    # if we ever go 100 iterations with no improvement, stop\\n    while iterations_with_no_improvement  < 100:\\n        value = sum( target_fn (x_i, y_i, theta) for x_i, y_i in data )\\n        if value < min_value :\\n            # if we\\'ve found a new minimum, remember it\\n            # and go back to the original step size\\n            min_theta , min_value  = theta, value\\n            iterations_with_no_improvement  = 0\\n            alpha = alpha_0\\n        else:\\n            # otherwise we\\'re not improving, so try shrinking the step size\\n            iterations_with_no_improvement  += 1\\n            alpha *= 0.9\\n        # and take a gradient step for each of the data points\\n        for x_i, y_i in in_random_order (data):\\n            gradient_i  = gradient_fn (x_i, y_i, theta)\\n            theta = vector_subtract (theta, scalar_multiply (alpha, gradient_i ))\\n    return min_theta\\nThe stochastic version will typically be a lot faster than the batch version. Of course,\\nwe’ll want a version that maximizes as well:\\ndef maximize_stochastic (target_fn , gradient_fn , x, y, theta_0, alpha_0=0.01):\\n    return minimize_stochastic (negate(target_fn ),\\n                               negate_all (gradient_fn ),\\n                               x, y, theta_0, alpha_0)\\nFor Further Exploration\\n•Keep reading! We’ll be using gradient descent to solve problems throughout the\\nrest of the book.\\n100 | Chapter 8: Gradient Descent\\nwww.it-ebooks.info•At this point, you’re undoubtedly sick of me recommending that you read text‐\\nbooks. If it’s any consolation, Active Calculus  seems nicer than the calculus text‐\\nbooks I learned from.\\n•scikit-learn has a Stochastic Gradient Descent module  that is not as general as\\nours in some ways and more general in other ways. Really, though, in most real-\\nworld situations you’ll be using libraries in which the optimization is already\\ntaken care of behind the scenes, and you won’t have to worry about it yourself\\n(other than when it doesn’t work correctly, which one day, inevitably, it won’t).\\nFor Further Exploration | 101\\nwww.it-ebooks.infowww.it-ebooks.infoCHAPTER 9\\nGetting Data\\nTo write it, it took three months; to conceive it, three minutes; to collect the data in it, all my\\nlife.\\n—F. Scott Fitzgerald\\nIn order to be a data scientist you need data. In fact, as a data scientist you will spend\\nan embarrassingly large fraction of your time acquiring, cleaning, and transforming\\ndata. In a pinch, you can always type the data in yourself (or if you have minions,\\nmake them do it), but usually this is not a good use of your time. In this chapter, we’ll\\nlook at different ways of getting data into Python and into the right formats.\\nstdin and stdout\\nIf you run your Python scripts at the command line, you can pipe data through them\\nusing sys.stdin  and sys.stdout . For example, here is a script that reads in lines of\\ntext and spits back out the ones that match a regular expression:\\n# egrep.py\\nimport sys, re\\n# sys.argv is the list of command-line arguments\\n# sys.argv[0] is the name of the program itself\\n# sys.argv[1] will be the regex specified at the command line\\nregex = sys.argv[1]\\n# for every line passed into the script\\nfor line in sys.stdin:\\n    # if it matches the regex, write it to stdout\\n    if re.search(regex, line):\\n        sys.stdout.write(line)\\nAnd here’s one that counts the lines it receives and then writes out the count:\\n103\\nwww.it-ebooks.info# line_count.py\\nimport sys\\ncount = 0\\nfor line in sys.stdin:\\n    count += 1\\n# print goes to sys.stdout\\nprint count\\nY ou could then use these to count how many lines of a file contain numbers. In Win‐\\ndows, you’ d use:\\ntype SomeFile.txt | python egrep.py \"[0-9]\" | python line_count.py\\nwhereas in a Unix system you’ d use:\\ncat SomeFile.txt | python egrep.py \"[0-9]\" | python line_count.py\\nThe | is the pipe character, which means “use the output of the left command as the\\ninput of the right command. ” Y ou can build pretty elaborate data-processing pipelines\\nthis way.\\nIf you are using Windows, you can probably leave out the python\\npart of this command:\\ntype SomeFile.txt | egrep.py \"[0-9]\" | line_count.py\\nIf you are on a Unix system, doing so might require a little more\\nwork .\\nSimilarly, here’s a script that counts the words in its input and writes out the most\\ncommon ones:\\n# most_common_words.py\\nimport sys\\nfrom collections  import Counter\\n# pass in number of words as first argument\\ntry:\\n    num_words  = int(sys.argv[1])\\nexcept:\\n    print \"usage: most_common_words.py num_words\"\\n    sys.exit(1)   # non-zero exit code indicates error\\ncounter = Counter(word.lower()                      # lowercase words\\n                  for line in sys.stdin             #\\n                  for word in line.strip().split()  # split on spaces\\n                  if word)                          # skip empty \\'words\\'\\nfor word, count in counter.most_common (num_words ):\\n    sys.stdout.write(str(count))\\n    sys.stdout.write(\"\\\\t\")\\n104 | Chapter 9: Getting Data\\nwww.it-ebooks.info    sys.stdout.write(word)\\n    sys.stdout.write(\"\\\\n\")\\nafter which you could do something like:\\nC:\\\\DataScience >type the_bible.txt | python most_common_words.py 10\\n64193   the\\n51380   and\\n34753   of\\n13643   to\\n12799   that\\n12560   in\\n10263   he\\n9840    shall\\n8987    unto\\n8836    for\\nIf you are a seasoned Unix programmer, you are probably familiar\\nwith a wide variety of command-line tools (for example, egrep )\\nthat are built into your operating system and that are probably\\npreferable to building your own from scratch. Still, it’s good to\\nknow you can if you need to.\\nReading Files\\nY ou can also explicitly read from and write to files directly in your code. Python\\nmakes working with files pretty simple.\\nThe Basics of Text Files\\nThe first step to working with a text file is to obtain a file object  using open :\\n# \\'r\\' means read-only\\nfile_for_reading  = open(\\'reading_file.txt\\' , \\'r\\')\\n# \\'w\\' is write -- will destroy the file if it already exists!\\nfile_for_writing  = open(\\'writing_file.txt\\' , \\'w\\')\\n# \\'a\\' is append -- for adding to the end of the file\\nfile_for_appending  = open(\\'appending_file.txt\\' , \\'a\\')\\n# don\\'t forget to close your files when you\\'re done\\nfile_for_writing .close()\\nBecause it is easy to forget to close your files, you should always use them in a with\\nblock, at the end of which they will be closed automatically:\\nwith open(filename ,\\'r\\') as f:\\n    data = function_that_gets_data_from (f)\\nReading Files | 105\\nwww.it-ebooks.info# at this point f has already been closed, so don\\'t try to use it\\nprocess(data)\\nIf you need to read a whole text file, you can just iterate over the lines of the file using\\nfor:\\nstarts_with_hash  = 0\\nwith open(\\'input.txt\\' ,\\'r\\') as f:\\n    for line in file:               # look at each line in the file\\n        if re.match(\"^#\",line):     # use a regex to see if it starts with \\'#\\'\\n            starts_with_hash  += 1   # if it does, add 1 to the count\\nEvery line you get this way ends in a newline character, so you’ll often want to\\nstrip()  it before doing anything with it.\\nFor example, imagine you have a file full of email addresses, one per line, and that\\nyou need to generate a histogram of the domains. The rules for correctly extracting\\ndomains are somewhat subtle (e.g., the Public Suffix List ), but a good first approxi‐\\nmation is to just take the parts of the email addresses that come after the @. (Which\\ngives the wrong answer for email addresses like joel@mail.datasciencester.com .)\\ndef get_domain (email_address ):\\n    \"\"\"split on \\'@\\' and return the last piece\"\"\"\\n    return email_address .lower().split(\"@\")[-1]\\nwith open(\\'email_addresses.txt\\' , \\'r\\') as f:\\n    domain_counts  = Counter(get_domain (line.strip())\\n                            for line in f\\n                            if \"@\" in line)\\nDelimited Files\\nThe hypothetical email addresses file we just processed had one address per line.\\nMore frequently you’ll work with files with lots of data on each line. These files are\\nvery  often either comma-separated  or tab-separated . Each line has several fields, with\\na comma (or a tab) indicating where one field ends and the next field starts.\\nThis starts to get complicated when you have fields with commas and tabs and new‐\\nlines in them (which you inevitably do). For this reason, it’s pretty much always a\\nmistake to try to parse them yourself. Instead, you should use Python’s csv module\\n(or the pandas  library). For technical reasons that you should feel free to blame on\\nMicrosoft, you should always work with csv files in binary  mode by including a b\\nafter the r or w (see Stack Overflow ).\\nIf your file has no headers (which means you probably want each row as a list , and\\nwhich places the burden on you to know what’s in each column), you can use\\ncsv.reader  to iterate over the rows, each of which will be an appropriately split list.\\n106 | Chapter 9: Getting Data\\nwww.it-ebooks.infoFor example, if we had a tab-delimited file of stock prices:\\n6/20/2014   AAPL    90.91\\n6/20/2014   MSFT    41.68\\n6/20/2014   FB  64.5\\n6/19/2014   AAPL    91.86\\n6/19/2014   MSFT    41.51\\n6/19/2014   FB  64.34\\nwe could process them with:\\nimport csv\\nwith open(\\'tab_delimited_stock_prices.txt\\' , \\'rb\\') as f:\\n    reader = csv.reader(f, delimiter =\\'\\\\t\\')\\n    for row in reader:\\n        date = row[0]\\n        symbol = row[1]\\n        closing_price  = float(row[2])\\n        process(date, symbol, closing_price )\\nIf your file has headers:\\ndate:symbol:closing_price\\n6/20/2014:AAPL:90.91\\n6/20/2014:MSFT:41.68\\n6/20/2014:FB:64.5\\nyou can either skip the header row (with an initial call to reader.next() ) or get each\\nrow as a dict  (with the headers as keys) by using csv.DictReader :\\nwith open(\\'colon_delimited_stock_prices.txt\\' , \\'rb\\') as f:\\n    reader = csv.DictReader (f, delimiter =\\':\\')\\n    for row in reader:\\n        date = row[\"date\"]\\n        symbol = row[\"symbol\" ]\\n        closing_price  = float(row[\"closing_price\" ])\\n        process(date, symbol, closing_price )\\nEven if your file doesn’t have headers you can still use DictReader  by passing it the\\nkeys as a fieldnames  parameter.\\nY ou can similarly write out delimited data using csv.writer :\\ntoday_prices  = { \\'AAPL\\' : 90.91, \\'MSFT\\' : 41.68, \\'FB\\' : 64.5 }\\nwith open(\\'comma_delimited_stock_prices.txt\\' ,\\'wb\\') as f:\\n    writer = csv.writer(f, delimiter =\\',\\')\\n    for stock, price in today_prices .items():\\n        writer.writerow ([stock, price])\\ncsv.writer  will do the right thing if your fields themselves have commas in them.\\nY our own hand-rolled writer probably won’t. For example, if you attempt:\\nReading Files | 107\\nwww.it-ebooks.inforesults = [[\"test1\", \"success\" , \"Monday\" ],\\n           [\"test2\", \"success, kind of\" , \"Tuesday\" ],\\n           [\"test3\", \"failure, kind of\" , \"Wednesday\" ],\\n           [\"test4\", \"failure, utter\" , \"Thursday\" ]]\\n# don\\'t do this!\\nwith open(\\'bad_csv.txt\\' , \\'wb\\') as f:\\n    for row in results:\\n        f.write(\",\".join(map(str, row))) # might have too many commas in it!\\n        f.write(\"\\\\n\")                    # row might have newlines as well!\\nY ou will end up with a csv file that looks like:\\ntest1,success,Monday\\ntest2,success, kind of,Tuesday\\ntest3,failure, kind of,Wednesday\\ntest4,failure, utter,Thursday\\nand that no one will ever be able to make sense of.\\nScraping the Web\\nAnother way to get data is by scraping it from web pages.  Fetching web pages, it turns\\nout, is pretty easy; getting meaningful structured information out of them less so.\\nHTML and the Parsing Thereof\\nPages on the Web are written in HTML, in which text is (ideally) marked up into ele‐\\nments and their attributes:\\n<html>\\n  <head>\\n    <title>A web page </title>\\n  </head>\\n  <body>\\n    <p id=\"author\" >Joel Grus </p>\\n    <p id=\"subject\" >Data Science </p>\\n  </body>\\n</html>\\nIn a perfect world, where all web pages are marked up semantically for our benefit,\\nwe would be able to extract data using rules like “find the <p> element whose id is\\nsubject  and return the text it contains. ” In the actual world, HTML is not generally\\nwell-formed, let alone annotated. This means we’ll need help making sense of it.\\nTo get data out of HTML, we will use the BeautifulSoup library , which builds a tree\\nout of the various elements on a web page and provides a simple interface for access‐\\ning them. As I write this, the latest version is Beautiful Soup 4.3.2 ( pip install beau\\ntifulsoup4 ), which is what we’ll be using. We’ll also be using the requests library\\n108 | Chapter 9: Getting Data\\nwww.it-ebooks.info(pip install requests ), which is a much nicer way of making HTTP requests than\\nanything that’s built into Python.\\nPython’s built-in HTML parser is not that lenient, which means that it doesn’t always\\ncope well with HTML that’s not perfectly formed. For that reason, we’ll use a different\\nparser, which we need to install:\\npip install html5lib\\nTo use Beautiful Soup, we’ll need to pass some HTML into the BeautifulSoup()\\nfunction. In our examples, this will be the result of a call to requests.get :\\nfrom bs4 import BeautifulSoup\\nimport requests\\nhtml = requests .get(\"http://www.example.com\" ).text\\nsoup = BeautifulSoup (html, \\'html5lib\\' )\\nafter which we can get pretty far using a few simple methods.\\nWe’ll typically work with Tag objects, which correspond to the tags representing the\\nstructure of an HTML page.\\nFor example, to find the first <p> tag (and its contents) you can use:\\nfirst_paragraph  = soup.find(\\'p\\')        # or just soup.p\\nY ou can get the text contents of a Tag using its text  property:\\nfirst_paragraph_text  = soup.p.text\\nfirst_paragraph_words  = soup.p.text.split()\\nAnd you can extract a tag’s attributes by treating it like a dict :\\nfirst_paragraph_id  = soup.p[\\'id\\']       # raises KeyError if no \\'id\\'\\nfirst_paragraph_id2  = soup.p.get(\\'id\\')  # returns None if no \\'id\\'\\nY ou can get multiple tags at once:\\nall_paragraphs  = soup.find_all (\\'p\\')  # or just soup(\\'p\\')\\nparagraphs_with_ids  = [p for p in soup(\\'p\\') if p.get(\\'id\\')]\\nFrequently you’ll want to find tags with a specific class :\\nimportant_paragraphs  = soup(\\'p\\', {\\'class\\' : \\'important\\' })\\nimportant_paragraphs2  = soup(\\'p\\', \\'important\\' )\\nimportant_paragraphs3  = [p for p in soup(\\'p\\')\\n                         if \\'important\\'  in p.get(\\'class\\', [])]\\nAnd you can combine these to implement more elaborate logic. For example, if you\\nwant to find every <span>  element that is contained inside a <div>  element, you\\ncould do this:\\n# warning, will return the same span multiple times\\n# if it sits inside multiple divs\\n# be more clever if that\\'s the case\\nScraping the Web | 109\\nwww.it-ebooks.infospans_inside_divs  = [span\\n                     for div in soup(\\'div\\')     # for each <div> on the page\\n                     for span in div(\\'span\\')]   # find each <span> inside it\\nJust this handful of features will allow us to do quite a lot. If you end up needing to do\\nmore-complicated things (or if you’re just curious), check the documentation.\\nOf course, whatever data is important won’t typically be labeled as class=\"impor\\ntant\" . Y ou’ll need to carefully inspect the source HTML, reason through your selec‐\\ntion logic, and worry about edge cases to make sure your data is correct. Let’s look at\\nan example.\\nExample: O’Reilly Books About Data\\nA potential investor in DataSciencester thinks data is just a fad. To prove him wrong,\\nyou decide to examine how many data books O’Reilly has published over time. After\\ndigging through its website, you find that it has many pages of data books (and vid‐\\neos), reachable through 30-items-at-a-time directory pages with URLs like:\\nhttp://shop.oreilly.com/category/browse-subjects/data.do?\\nsortby=publicationDate&page=1\\nUnless you want to be a jerk (and unless you want your scraper to get banned), when‐\\never you want to scrape data from a website you should first check to see if it has\\nsome sort of access policy. Looking at:\\nhttp://oreilly.com/terms/\\nthere seems to be nothing prohibiting this project. In order to be good citizens, we\\nshould also check for a robots.txt  file that tells webcrawlers how to behave. The\\nimportant lines in http://shop.oreilly.com/robots.txt  are:\\nCrawl-delay: 30\\nRequest-rate: 1/30\\nThe first tells us that we should wait 30 seconds between requests, the second that we\\nshould request only one page every 30 seconds. So basically they’re two different ways\\nof saying the same thing. (There are other lines that indicate directories not to scrape,\\nbut they don’t include our URL, so we’re OK there.)\\nThere’s always the possibility that O’Reilly will at some point\\nrevamp its website and break all the logic in this section. I will do\\nwhat I can to prevent that, of course, but I don’t have a ton of influ‐\\nence over there. Although, if every one of you were to convince\\neveryone you know to buy a copy of this book…\\nTo figure out how to extract the data, let’s download one of those pages and feed it to\\nBeautiful Soup:\\n110 | Chapter 9: Getting Data\\nwww.it-ebooks.info# you don\\'t have to split the url like this unless it needs to fit in a book\\nurl = \"http://shop.oreilly.com/category/browse-subjects/\"  + \\\\\\n      \"data.do?sortby=publicationDate&page=1\"\\nsoup = BeautifulSoup (requests .get(url).text, \\'html5lib\\' )\\nIf you view the source of the page (in your browser, right-click and select “View\\nsource” or “View page source” or whatever option looks the most like that), you’ll see\\nthat each book (or video) seems to be uniquely contained in a <td>  table cell element\\nwhose class  is thumbtext . Here is (an abridged version of) the relevant HTML for\\none book:\\n<td class=\"thumbtext\" >\\n  <div class=\"thumbcontainer\" >\\n    <div class=\"thumbdiv\" >\\n      <a href=\"/product/9781118903407.do\" >\\n        <img src=\"...\"/>\\n      </a>\\n    </div>\\n  </div>\\n  <div class=\"widthchange\" >\\n    <div class=\"thumbheader\" >\\n      <a href=\"/product/9781118903407.do\" >Getting a Big Data Job For Dummies </a>\\n    </div>\\n    <div class=\"AuthorName\" >By Jason Williamson </div>\\n    <span class=\"directorydate\" >        December 2014    </span>\\n    <div style=\"clear:both;\" >\\n      <div id=\"146350\" >\\n        <span class=\"pricelabel\" >\\n                            Ebook:\\n                            <span class=\"price\">&nbsp;$29.99</span>\\n        </span>\\n      </div>\\n    </div>\\n  </div>\\n</td>\\nA good first step is to find all of the td thumbtext  tag elements:\\ntds = soup(\\'td\\', \\'thumbtext\\' )\\nprint len(tds)\\n# 30\\nNext we’ d like to filter out the videos. (The would-be investor is only impressed by\\nbooks.) If we inspect the HTML further, we see that each td contains one or more\\nspan  elements whose class  is pricelabel , and whose text looks like Ebook:  or\\nVideo:  or Print: . It appears that the videos contain only one pricelabel , whose\\ntext  starts with Video  (after removing leading spaces). This means we can test for\\nvideos with:\\ndef is_video (td):\\n    \"\"\"it\\'s a video if it has exactly one pricelabel, and if\\nScraping the Web | 111\\nwww.it-ebooks.info    the stripped text inside that pricelabel starts with \\'Video\\'\"\"\"\\n    pricelabels  = td(\\'span\\', \\'pricelabel\\' )\\n    return (len(pricelabels ) == 1 and\\n            pricelabels [0].text.strip().startswith (\"Video\"))\\nprint len([td for td in tds if not is_video (td)])\\n# 21 for me, might be different for you\\nNow we’re ready to start pulling data out of the td elements. It looks like the book\\ntitle is the text inside the <a> tag inside the <div class=\"thumbheader\"> :\\ntitle = td.find(\"div\", \"thumbheader\" ).a.text\\nThe author(s) are in the text of the AuthorName  <div> . They are prefaced by a By\\n(which we want to get rid of) and separated by commas (which we want to split out,\\nafter which we’ll need to get rid of spaces):\\nauthor_name  = td.find(\\'div\\', \\'AuthorName\\' ).text\\nauthors = [x.strip() for x in re.sub(\"^By \", \"\", author_name ).split(\",\")]\\nThe ISBN seems to be contained in the link that’s in the thumbheader  <div> :\\nisbn_link  = td.find(\"div\", \"thumbheader\" ).a.get(\"href\")\\n# re.match captures the part of the regex in parentheses\\nisbn = re.match(\"/product/(.*)\\\\.do\" , isbn_link ).group(1)\\nAnd the date is just the contents of the <span class=\"directorydate\"> :\\ndate = td.find(\"span\", \"directorydate\" ).text.strip()\\nLet’s put this all together into a function:\\ndef book_info (td):\\n    \"\"\"given a BeautifulSoup <td> Tag representing a book,\\n    extract the book\\'s details and return a dict\"\"\"\\n    title = td.find(\"div\", \"thumbheader\" ).a.text\\n    by_author  = td.find(\\'div\\', \\'AuthorName\\' ).text\\n    authors = [x.strip() for x in re.sub(\"^By \", \"\", by_author ).split(\",\")]\\n    isbn_link  = td.find(\"div\", \"thumbheader\" ).a.get(\"href\")\\n    isbn = re.match(\"/product/(.*)\\\\.do\" , isbn_link ).groups()[0]\\n    date = td.find(\"span\", \"directorydate\" ).text.strip()\\n    return {\\n        \"title\" : title,\\n        \"authors\"  : authors,\\n        \"isbn\" : isbn,\\n        \"date\" : date\\n    }\\nAnd now we’re ready to scrape:\\nfrom bs4 import BeautifulSoup\\nimport requests\\n112 | Chapter 9: Getting Data\\nwww.it-ebooks.infofrom time import sleep\\nbase_url  = \"http://shop.oreilly.com/category/browse-subjects/\"  + \\\\\\n           \"data.do?sortby=publicationDate&page=\"\\nbooks = []\\nNUM_PAGES  = 31     # at the time of writing, probably more by now\\nfor page_num  in range(1, NUM_PAGES  + 1):\\n    print \"souping page\" , page_num , \",\", len(books), \" found so far\"\\n    url = base_url  + str(page_num )\\n    soup = BeautifulSoup (requests .get(url).text, \\'html5lib\\' )\\n    for td in soup(\\'td\\', \\'thumbtext\\' ):\\n        if not is_video (td):\\n            books.append(book_info (td))\\n    # now be a good citizen and respect the robots.txt!\\n    sleep(30)\\nExtracting data from HTML like this is more data art than data sci‐\\nence. There are countless other find-the-books and find-the-title\\nlogics that would have worked just as well.\\nNow that we’ve collected the data, we can plot the number of books published each\\nyear ( Figure 9-1 ):\\ndef get_year (book):\\n    \"\"\"book[\"date\"] looks like \\'November 2014\\' so we need to\\n    split on the space and then take the second piece\"\"\"\\n    return int(book[\"date\"].split()[1])\\n# 2014 is the last complete year of data (when I ran this)\\nyear_counts  = Counter(get_year (book) for book in books\\n                      if get_year (book) <= 2014)\\nimport matplotlib.pyplot  as plt\\nyears = sorted(year_counts )\\nbook_counts  = [year_counts [year] for year in years]\\nplt.plot(years, book_counts )\\nplt.ylabel(\"# of data books\" )\\nplt.title(\"Data is Big!\" )\\nplt.show()\\nScraping the Web | 113\\nwww.it-ebooks.infoFigure 9-1. Number of data books per year\\nUnfortunately, the would-be investor looks at the graph and decides that 2013 was\\n“peak data. ”\\nUsing APIs\\nMany websites and web services provide application programming interfaces (APIs),\\nwhich allow you to explicitly request data in a structured format.  This saves you the\\ntrouble of having to scrape them!\\nJSON (and XML)\\nBecause HTTP is a protocol for transferring text, the data you request through a web\\nAPI needs to be serialized  into a string format. Often this serialization uses JavaScript\\nObject Notation (JSON). JavaScript objects look quite similar to Python dict s, which\\nmakes their string representations easy to interpret:\\n{ \"title\" : \"Data Science Book\" ,\\n  \"author\"  : \"Joel Grus\" ,\\n  \"publicationYear\"  : 2014,\\n  \"topics\"  : [ \"data\", \"science\" , \"data science\" ] }\\n114 | Chapter 9: Getting Data\\nwww.it-ebooks.infoWe can parse JSON using Python’s json  module. In particular, we will use its loads\\nfunction, which deserializes a string representing a JSON object into a Python object:\\nimport json\\nserialized  = \"\"\"{ \"title\" : \"Data Science Book\",\\n                  \"author\" : \"Joel Grus\",\\n                  \"publicationYear\" : 2014,\\n                  \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\\n# parse the JSON to create a Python dict\\ndeserialized  = json.loads(serialized )\\nif \"data science\"  in deserialized [\"topics\" ]:\\n    print deserialized\\nSometimes an API provider hates you and only provides responses in XML:\\n<Book>\\n  <Title>Data Science Book </Title>\\n  <Author> Joel Grus </Author>\\n  <PublicationYear> 2014</PublicationYear>\\n  <Topics>\\n    <Topic>data</Topic>\\n    <Topic>science</Topic>\\n    <Topic>data science </Topic>\\n  </Topics>\\n</Book>\\nY ou can use BeautifulSoup  to get data from XML similarly to how we used it to get\\ndata from HTML; check its documentation for details.\\nUsing an Unauthenticated API\\nMost APIs these days require you to first authenticate yourself in order to use them.\\nWhile we don’t begrudge them this policy, it creates a lot of extra boilerplate that\\nmuddies up our exposition. Accordingly, we’ll first take a look at GitHub’s API , with\\nwhich you can do some simple things unauthenticated:\\nimport requests , json\\nendpoint  = \"https://api.github.com/users/joelgrus/repos\"\\nrepos = json.loads(requests .get(endpoint ).text)\\nAt this point repos  is a list  of Python dict s, each representing a public repository in\\nmy GitHub account. (Feel free to substitute your username and get your GitHub\\nrepository data instead. Y ou do have a GitHub account, right?)\\nWe can use this to figure out which months and days of the week I’m most likely to\\ncreate a repository. The only issue is that the dates in the response are (Unicode)\\nstrings:\\nu\\'created_at\\' : u\\'2013-07-05T02:02:28Z\\'\\nUsing APIs | 115\\nwww.it-ebooks.infoPython doesn’t come with a great date parser, so we’ll need to install one:\\npip install python-dateutil\\nfrom which you’ll probably only ever need the dateutil.parser.parse  function:\\nfrom dateutil.parser  import parse\\ndates = [parse(repo[\"created_at\" ]) for repo in repos]\\nmonth_counts  = Counter(date.month for date in dates)\\nweekday_counts  = Counter(date.weekday() for date in dates)\\nSimilarly, you can get the languages of my last five repositories:\\nlast_5_repositories  = sorted(repos,\\n                             key=lambda r: r[\"created_at\" ],\\n                             reverse=True)[:5]\\nlast_5_languages  = [repo[\"language\" ]\\n                    for repo in last_5_repositories ]\\nTypically we won’t be working with APIs at this low “make the requests and parse the\\nresponses ourselves” level. One of the benefits of using Python is that someone has\\nalready built a library for pretty much any API you’re interested in accessing. When\\nthey’re done well, these libraries can save you a lot of the trouble of figuring out the\\nhairier details of API access. (When they’re not done well, or when it turns out they’re\\nbased on defunct versions of the corresponding APIs, they can cause you enormous\\nheadaches.)\\nNonetheless, you’ll occasionally have to roll your own API-access library (or, more\\nlikely, debug why someone else’s isn’t working), so it’s good to know some of the\\ndetails.\\nFinding APIs\\nIf you need data from a specific site, look for a developers or API section of the site\\nfor details, and try searching the Web for “python __ api” to find a library. There is a\\nRotten Tomatoes API for Python. There are multiple Python wrappers for the Klout\\nAPI, for the Y elp API, for the IMDB API, and so on.\\nIf you’re looking for lists of APIs that have Python wrappers, two directories are at\\nPython API  and Python for Beginners .\\nIf you want a directory of web APIs more broadly (without Python wrappers neces‐\\nsarily), a good resource is Programmable Web , which has a huge directory of catego‐\\nrized APIs.\\nAnd if after all that you can’t find what you need, there’s always scraping, the last ref‐\\nuge of the data scientist.\\n116 | Chapter 9: Getting Data\\nwww.it-ebooks.infoExample: Using the Twitter APIs\\nTwitter is a fantastic source of data to work with. Y ou can use it to get real-time news.\\nY ou can use it to measure reactions to current events. Y ou can use it to find links\\nrelated to specific topics. Y ou can use it for pretty much anything you can imagine,\\njust as long as you can get access to its data. And you can get access to its data\\nthrough its API.\\nTo interact with the Twitter APIs we’ll be using the Twython library  (pip install\\ntwython ). There are quite a few Python Twitter libraries out there, but this is the one\\nthat I’ve had the most success working with. Y ou are encouraged to explore the others\\nas well!\\nGetting Credentials\\nIn order to use Twitter’s APIs, you need to get some credentials (for which you need a\\nTwitter account, which you should have anyway so that you can be part of the lively\\nand friendly Twitter #datascience  community). Like all instructions that relate to\\nwebsites that I don’t control, these may go obsolete at some point but will hopefully\\nwork for a while. (Although they have already changed at least once while I was writ‐\\ning this book, so good luck!)\\n1.Go to https://apps.twitter.com/ .\\n2.If you are not signed in, click Sign in and enter your Twitter username and pass‐\\nword.\\n3.Click Create New App.\\n4.Give it a name (such as “Data Science”) and a description, and put any URL as\\nthe website (it doesn’t matter which one).\\n5.Agree to the Terms of Service and click Create.\\n6.Take note of the consumer key and consumer secret.\\n7.Click “Create my access token. ”\\n8.Take note of the access token and access token secret (you may have to refresh\\nthe page).\\nThe consumer key and consumer secret tell Twitter what application is accessing its\\nAPIs, while the access token and access token secret tell Twitter who is accessing its\\nAPIs. If you have ever used your Twitter account to log in to some other site, the\\n“click to authorize” page was generating an access token for that site to use to con‐\\nvince Twitter that it was you (or, at least, acting on your behalf). As we don’t need this\\n“let anyone log in” functionality, we can get by with the statically generated access\\ntoken and access token secret.\\nExample: Using the Twitter APIs | 117\\nwww.it-ebooks.infoThe consumer key/secret and access token key/secret should be\\ntreated like passwords . Y ou shouldn’t share them, you shouldn’t\\npublish them in your book, and you shouldn’t check them into\\nyour public GitHub repository. One simple solution is to store\\nthem in a credentials.json  file that doesn’t get checked in, and to\\nhave your code use json.loads  to retrieve them.\\nUsing Twython\\nFirst we’ll look at the Search API , which requires only the consumer key and secret,\\nnot the access token or secret:\\nfrom twython import Twython\\ntwitter = Twython(CONSUMER_KEY , CONSUMER_SECRET )\\n# search for tweets containing the phrase \"data science\"\\nfor status in twitter.search(q=\\'\"data science\"\\' )[\"statuses\" ]:\\n    user = status[\"user\"][\"screen_name\" ].encode(\\'utf-8\\')\\n    text = status[\"text\"].encode(\\'utf-8\\')\\n    print user, \":\", text\\n    print\\nThe .encode(\"utf-8\")  is necessary to deal with the fact that tweets\\noften contain Unicode characters that print  can’t deal with. (If you\\nleave it out, you will very likely get a UnicodeEncodeError .)\\nIt is almost certain that at some point in your data science career\\nyou will run into some serious Unicode problems, at which point\\nyou will need to refer to the Python documentation  or else grudg‐\\ningly start using Python 3, which plays much more nicely with Uni‐\\ncode text.\\nIf you run this, you should get some tweets back like:\\nhaithemnyc: Data scientists with the technical savvy &amp; analytical chops to\\nderive meaning from big data are in demand. http://t.co/HsF9Q0dShP\\nRPubsRecent: Data Science http://t.co/6hcHUz2PHM\\nspleonard1: Using #dplyr in #R to work through a procrastinated assignment for\\n@rdpeng in @coursera data science specialization.  So easy and Awesome.\\nThis isn’t that interesting, largely because the Twitter Search API just shows you\\nwhatever handful of recent results it feels like. When you’re doing data science, more\\noften you want a lot of tweets. This is where the Streaming API  is useful. It allows you\\nto connect to (a sample of) the great Twitter firehose. To use it, you’ll need to authen‐\\nticate using your access tokens.\\n118 | Chapter 9: Getting Data\\nwww.it-ebooks.infoIn order to access the Streaming API with Twython, we need to define a class that\\ninherits from TwythonStreamer  and that overrides its on_success  method (and pos‐\\nsibly its on_error  method):\\nfrom twython import TwythonStreamer\\n# appending data to a global variable is pretty poor form\\n# but it makes the example much simpler\\ntweets = []\\nclass MyStreamer (TwythonStreamer ):\\n    \"\"\"our own subclass of TwythonStreamer that specifies\\n    how to interact with the stream\"\"\"\\n    def on_success (self, data):\\n        \"\"\"what do we do when twitter sends us data?\\n        here data will be a Python dict representing a tweet\"\"\"\\n        # only want to collect English-language tweets\\n        if data[\\'lang\\'] == \\'en\\':\\n            tweets.append(data)\\n            print \"received tweet #\" , len(tweets)\\n        # stop when we\\'ve collected enough\\n        if len(tweets) >= 1000:\\n            self.disconnect ()\\n    def on_error (self, status_code , data):\\n        print status_code , data\\n        self.disconnect ()\\nMyStreamer will connect to the Twitter stream and wait for Twitter to feed it data.\\nEach time it receives some data (here, a Tweet represented as a Python object) it\\npasses it to the on_success  method, which appends it to our tweets  list if its lan‐\\nguage is English, and then disconnects the streamer after it’s collected 1,000 tweets.\\nAll that’s left is to initialize it and start it running:\\nstream = MyStreamer (CONSUMER_KEY , CONSUMER_SECRET ,\\n                    ACCESS_TOKEN , ACCESS_TOKEN_SECRET )\\n# starts consuming public statuses that contain the keyword \\'data\\'\\nstream.statuses .filter(track=\\'data\\')\\n# if instead we wanted to start consuming a sample of *all* public statuses\\n# stream.statuses.sample()\\nThis will run until it collects 1,000 tweets (or until it encounters an error) and stop, at\\nwhich point you can start analyzing those tweets. For instance, you could find the\\nmost common hashtags with:\\nExample: Using the Twitter APIs | 119\\nwww.it-ebooks.infotop_hashtags  = Counter(hashtag[\\'text\\'].lower()\\n                       for tweet in tweets\\n                       for hashtag in tweet[\"entities\" ][\"hashtags\" ])\\nprint top_hashtags .most_common (5)\\nEach tweet contains a lot of data. Y ou can either poke around yourself or dig through\\nthe Twitter API documentation .\\nIn a non-toy project you probably wouldn’t want to rely on an in-\\nmemory list  for storing the tweets. Instead you’ d want to save\\nthem to a file or a database, so that you’ d have them permanently.\\nFor Further Exploration\\n•pandas  is the primary library that data science types use for working with (and,\\nin particular, importing) data.\\n•Scrapy  is a more full-featured library for building more complicated web scrapers\\nthat do things like follow unknown links.\\n120 | Chapter 9: Getting Data\\nwww.it-ebooks.infoCHAPTER 10\\nWorking with Data\\nExperts often  possess more data than judgment.\\n—Colin Powell\\nWorking with data is both an art and a science. We’ve mostly been talking about the\\nscience part, but in this chapter we’ll look at some of the art.\\nExploring Your Data\\nAfter you’ve identified the questions you’re trying to answer and have gotten your\\nhands on some data, you might be tempted to dive in and immediately start building\\nmodels and getting answers. But you should resist this urge. Y our first step should be\\nto explore  your data.\\nExploring One-Dimensional Data\\nThe simplest case is when you have a one-dimensional data set, which is just a collec‐\\ntion of numbers. For example, these could be the daily average number of minutes\\neach user spends on your site, the number of times each of a collection of data science\\ntutorial videos was watched, or the number of pages of each of the data science books\\nin your data science library.\\nAn obvious first step is to compute a few summary statistics. Y ou’ d like to know how\\nmany data points you have, the smallest, the largest, the mean, and the standard devi‐\\nation.\\nBut even these don’t necessarily give you a great understanding. A good next step is to\\ncreate a histogram, in which you group your data into discrete buckets  and count how\\nmany points fall into each bucket:\\n121\\nwww.it-ebooks.infodef bucketize (point, bucket_size ):\\n    \"\"\"floor the point to the next lower multiple of bucket_size\"\"\"\\n    return bucket_size  * math.floor(point / bucket_size )\\ndef make_histogram (points, bucket_size ):\\n    \"\"\"buckets the points and counts how many in each bucket\"\"\"\\n    return Counter(bucketize (point, bucket_size ) for point in points)\\ndef plot_histogram (points, bucket_size , title=\"\"):\\n    histogram  = make_histogram (points, bucket_size )\\n    plt.bar(histogram .keys(), histogram .values(), width=bucket_size )\\n    plt.title(title)\\n    plt.show()\\nFor example, consider the two following sets of data:\\nrandom.seed(0)\\n# uniform between -100 and 100\\nuniform = [200 * random.random() - 100 for _ in range(10000)]\\n# normal distribution with mean 0, standard deviation 57\\nnormal = [57 * inverse_normal_cdf (random.random())\\n          for _ in range(10000)]\\nBoth have means close to 0 and standard deviations close to 58. However, they have\\nvery different distributions. Figure 10-1  shows the distribution of uniform :\\nplot_histogram (uniform, 10, \"Uniform Histogram\" )\\nwhile Figure 10-2  shows the distribution of normal :\\nplot_histogram (normal, 10, \"Normal Histogram\" )\\nIn this case, both distributions had pretty different max and min, but even knowing\\nthat wouldn’t have been sufficient to understand how they differed.\\n122 | Chapter 10: Working with Data\\nwww.it-ebooks.infoFigure 10-1. Histogram of uniform\\nTwo Dimensions\\nNow imagine you have a data set with two dimensions. Maybe in addition to daily\\nminutes you have years of data science experience. Of course you’ d want to under‐\\nstand each dimension individually. But you probably also want to scatter the data.\\nFor example, consider another fake data set:\\ndef random_normal ():\\n    \"\"\"returns a random draw from a standard normal distribution\"\"\"\\n    return inverse_normal_cdf (random.random())\\nxs = [random_normal () for _ in range(1000)]\\nys1 = [ x + random_normal () / 2 for x in xs]\\nys2 = [-x + random_normal () / 2 for x in xs]\\nIf you were to run plot_histogram  on ys1 and ys2 you’ d get very similar looking\\nplots (indeed, both are normally distributed with the same mean and standard devia‐\\ntion).\\nExploring Your Data | 123\\nwww.it-ebooks.infoFigure 10-2. Histogram of normal\\nBut each has a very different joint distribution with xs, as shown in Figure 10-3 :\\nplt.scatter(xs, ys1, marker=\\'.\\', color=\\'black\\', label=\\'ys1\\')\\nplt.scatter(xs, ys2, marker=\\'.\\', color=\\'gray\\',  label=\\'ys2\\')\\nplt.xlabel(\\'xs\\')\\nplt.ylabel(\\'ys\\')\\nplt.legend(loc=9)\\nplt.title(\"Very Different Joint Distributions\" )\\nplt.show()\\n124 | Chapter 10: Working with Data\\nwww.it-ebooks.infoFigure 10-3. Scattering two different  ys\\nThis difference would also be apparent if you looked at the correlations:\\nprint correlation (xs, ys1)      #  0.9\\nprint correlation (xs, ys2)      # -0.9\\nMany Dimensions\\nWith many dimensions, you’ d like to know how all the dimensions relate to one\\nanother. A simple approach is to look at the correlation matrix , in which the entry in\\nrow i and column j is the correlation between the ith dimension and the jth dimen‐\\nsion of the data:\\ndef correlation_matrix (data):\\n    \"\"\"returns the num_columns x num_columns matrix whose (i, j)th entry\\n    is the correlation between columns i and j of data\"\"\"\\n    _, num_columns  = shape(data)\\n    def matrix_entry (i, j):\\n        return correlation (get_column (data, i), get_column (data, j))\\n    return make_matrix (num_columns , num_columns , matrix_entry )\\nExploring Your Data | 125\\nwww.it-ebooks.infoA more visual approach (if you don’t have too many dimensions) is to make a scatter‐\\nplot matrix  (Figure 10-4 ) showing all the pairwise scatterplots. To do that we’ll use\\nplt.subplots() , which allows us to create subplots of our chart. We give it the num‐\\nber of rows and the number of columns, and it returns a figure  object (which we\\nwon’t use) and a two-dimensional array of axes  objects (each of which we’ll plot to):\\nimport matplotlib.pyplot  as plt\\n_, num_columns  = shape(data)\\nfig, ax = plt.subplots (num_columns , num_columns )\\nfor i in range(num_columns ):\\n    for j in range(num_columns ):\\n        # scatter column_j on the x-axis vs column_i on the y-axis\\n        if i != j: ax[i][j].scatter(get_column (data, j), get_column (data, i))\\n        # unless i == j, in which case show the series name\\n        else: ax[i][j].annotate (\"series \"  + str(i), (0.5, 0.5),\\n                                xycoords =\\'axes fraction\\' ,\\n                                ha=\"center\" , va=\"center\" )\\n        # then hide axis labels except left and bottom charts\\n        if i < num_columns  - 1: ax[i][j].xaxis.set_visible (False)\\n        if j > 0: ax[i][j].yaxis.set_visible (False)\\n# fix the bottom right and top left axis labels, which are wrong because\\n# their charts only have text in them\\nax[-1][-1].set_xlim (ax[0][-1].get_xlim ())\\nax[0][0].set_ylim (ax[0][1].get_ylim ())\\nplt.show()\\n126 | Chapter 10: Working with Data\\nwww.it-ebooks.infoFigure 10-4. Scatterplot matrix\\nLooking at the scatterplots, you can see that series 1 is very negatively correlated with\\nseries 0, series 2 is positively correlated with series 1, and series 3 only takes on the\\nvalues 0 and 6, with 0 corresponding to small values of series 2 and 6 corresponding\\nto large values.\\nThis is a quick way to get a rough sense of which of your variables are correlated\\n(unless you spend hours tweaking matplotlib  to display things exactly the way you\\nwant them to, in which case it’s not a quick way).\\nCleaning and Munging\\nReal-world data is dirty . Often you’ll have to do some work on it before you can use\\nit. We’ve seen examples of this in Chapter 9 . We have to convert strings to float s or\\nints before we can use them. Previously, we did that right before using the data:\\n        closing_price  = float(row[2])\\nBut it’s probably less error-prone to do the parsing on the way in, which we can do by\\ncreating a function that wraps csv.reader . We’ll give it a list of parsers, each specify‐\\ning how to parse one of the columns. And we’ll use None  to represent “don’t do any‐\\nthing to this column”:\\nCleaning and Munging | 127\\nwww.it-ebooks.infodef parse_row (input_row , parsers):\\n    \"\"\"given a list of parsers (some of which may be None)\\n    apply the appropriate one to each element of the input_row\"\"\"\\n    return [parser(value) if parser is not None else value\\n            for value, parser in zip(input_row , parsers)]\\ndef parse_rows_with (reader, parsers):\\n    \"\"\"wrap a reader to apply the parsers to each of its rows\"\"\"\\n    for row in reader:\\n        yield parse_row (row, parsers)\\nWhat if there’s bad data? A “float” value that doesn’t actually represent a number?\\nWe’ d usually rather get a None  than crash our program. We can do this with a helper\\nfunction:\\ndef try_or_none (f):\\n    \"\"\"wraps f to return None if f raises an exception\\n    assumes f takes only one input\"\"\"\\n    def f_or_none (x):\\n        try: return f(x)\\n        except: return None\\n    return f_or_none\\nafter which we can rewrite parse_row  to use it:\\ndef parse_row (input_row , parsers):\\n    return [try_or_none (parser)(value) if parser is not None else value\\n            for value, parser in zip(input_row , parsers)]\\nFor example, if we have comma-delimited stock prices with bad data:\\n6/20/2014,AAPL,90.91\\n6/20/2014,MSFT,41.68\\n6/20/3014,FB,64.5\\n6/19/2014,AAPL,91.86\\n6/19/2014,MSFT,n/a\\n6/19/2014,FB,64.34\\nwe can now read and parse in a single step:\\nimport dateutil.parser\\ndata = []\\nwith open(\"comma_delimited_stock_prices.csv\" , \"rb\") as f:\\n    reader = csv.reader(f)\\n    for line in parse_rows_with (reader, [dateutil .parser.parse, None, float]):\\n        data.append(line)\\nafter which we just need to check for None  rows:\\nfor row in data:\\n    if any(x is None for x in row):\\n        print row\\n128 | Chapter 10: Working with Data\\nwww.it-ebooks.infoand decide what we want to do about them. (Generally speaking, the three options\\nare to get rid of them, to go back to the source and try to fix the bad/missing data, or\\nto do nothing and cross our fingers.)\\nWe could create similar helpers for csv.DictReader . In that case, you’ d probably\\nwant to supply a dict  of parsers by field name. For example:\\ndef try_parse_field (field_name , value, parser_dict ):\\n    \"\"\"try to parse value using the appropriate function from parser_dict\"\"\"\\n    parser = parser_dict .get(field_name )   # None if no such entry\\n    if parser is not None:\\n        return try_or_none (parser)(value)\\n    else:\\n        return value\\ndef parse_dict (input_dict , parser_dict ):\\n    return { field_name  : try_parse_field (field_name , value, parser_dict )\\n             for field_name , value in input_dict .iteritems () }\\nA good next step is to check for outliers, using techniques from “Exploring Y our\\nData” on page 121 or by ad hoc investigating. For example, did you notice that one of\\nthe dates in the stocks file had the year 3014? That won’t (necessarily) give you an\\nerror, but it’s quite plainly wrong, and you’ll get screwy results if you don’t catch it.\\nReal-world data sets have missing decimal points, extra zeroes, typographical errors,\\nand countless other problems that it’s your job to catch. (Maybe it’s not officially your\\njob, but who else is going to do it?)\\nManipulating Data\\nOne of the most important skills of a data scientist is manipulating data . It’s more of a\\ngeneral approach than a specific technique, so we’ll just work through a handful of\\nexamples to give you the flavor of it.\\nImagine we’re working with dict s of stock prices that look like:\\ndata = [\\n    {\\'closing_price\\' : 102.06,\\n     \\'date\\': datetime .datetime (2014, 8, 29, 0, 0),\\n     \\'symbol\\' : \\'AAPL\\'},\\n    # ...\\n]\\nConceptually we’ll think of them as rows (as in a spreadsheet).\\nLet’s start asking questions about this data. Along the way we’ll try to notice patterns\\nin what we’re doing and abstract out some tools to make the manipulation easier.\\nFor instance, suppose we want to know the highest-ever closing price for AAPL. Let’s\\nbreak this down into concrete steps:\\nManipulating Data | 129\\nwww.it-ebooks.info1.Restrict ourselves to AAPL rows.\\n2.Grab the closing_price  from each row.\\n3.Take the max of those prices.\\nWe can do all three at once using a list comprehension:\\nmax_aapl_price  = max(row[\"closing_price\" ]\\n                     for row in data\\n                     if row[\"symbol\" ] == \"AAPL\")\\nMore generally, we might want to know the highest-ever closing price for each stock\\nin our data set. One way to do this is:\\n1.Group together all the rows with the same symbol .\\n2.Within each group, do the same as before:\\n# group rows by symbol\\nby_symbol  = defaultdict (list)\\nfor row in data:\\n    by_symbol [row[\"symbol\" ]].append(row)\\n# use a dict comprehension to find the max for each symbol\\nmax_price_by_symbol  = { symbol : max(row[\"closing_price\" ]\\n                                     for row in grouped_rows )\\n                        for symbol, grouped_rows  in by_symbol .iteritems () }\\nThere are some patterns here already. In both examples, we needed to pull the clos\\ning_price  value out of every dict . So let’s create a function to pick a field out of a\\ndict , and another function to pluck the same field out of a collection of dict s:\\ndef picker(field_name ):\\n    \"\"\"returns a function that picks a field out of a dict\"\"\"\\n    return lambda row: row[field_name ]\\ndef pluck(field_name , rows):\\n    \"\"\"turn a list of dicts into the list of field_name values\"\"\"\\n    return map(picker(field_name ), rows)\\nWe can also create a function to group rows by the result of a grouper  function and\\nto optionally apply some sort of value_transform  to each group:\\ndef group_by (grouper, rows, value_transform =None):\\n    # key is output of grouper, value is list of rows\\n    grouped = defaultdict (list)\\n    for row in rows:\\n        grouped[grouper(row)].append(row)\\n    if value_transform  is None:\\n        return grouped\\n    else:\\n130 | Chapter 10: Working with Data\\nwww.it-ebooks.info        return { key : value_transform (rows)\\n                 for key, rows in grouped.iteritems () }\\nThis allows us to rewrite our previous examples quite simply. For example:\\nmax_price_by_symbol  = group_by (picker(\"symbol\" ),\\n                               data,\\n                               lambda rows: max(pluck(\"closing_price\" , rows)))\\nWe can now start to ask more complicated things, like what are the largest and small‐\\nest one-day percent changes in our data set. The percent change is price_today /\\nprice_yesterday - 1 , which means we need some way of associating today’s price\\nand yesterday’s price. One approach is to group the prices by symbol, then, within\\neach group:\\n1.Order the prices by date.\\n2.Use zip to get pairs (previous, current).\\n3.Turn the pairs into new “percent change” rows.\\nWe’ll start by writing a function to do all the within-each-group work:\\ndef percent_price_change (yesterday , today):\\n    return today[\"closing_price\" ] / yesterday [\"closing_price\" ] - 1\\ndef day_over_day_changes (grouped_rows ):\\n    # sort the rows by date\\n    ordered = sorted(grouped_rows , key=picker(\"date\"))\\n    # zip with an offset to get pairs of consecutive days\\n    return [{ \"symbol\"  : today[\"symbol\" ],\\n              \"date\" : today[\"date\"],\\n              \"change\"  : percent_price_change (yesterday , today) }\\n            for yesterday , today in zip(ordered, ordered[1:])]\\nThen we can just use this as the value_transform  in a group_by :\\n# key is symbol, value is list of \"change\" dicts\\nchanges_by_symbol  = group_by (picker(\"symbol\" ), data, day_over_day_changes )\\n# collect all \"change\" dicts into one big list\\nall_changes  = [change\\n               for changes in changes_by_symbol .values()\\n               for change in changes]\\nAt which point it’s easy to find the largest and smallest:\\nmax(all_changes , key=picker(\"change\" ))\\n# {\\'change\\': 0.3283582089552237,\\n#  \\'date\\': datetime.datetime(1997, 8, 6, 0, 0),\\n#  \\'symbol\\': \\'AAPL\\'}\\n# see, e.g. http://news.cnet.com/2100-1001-202143.html\\nManipulating Data | 131\\nwww.it-ebooks.infomin(all_changes , key=picker(\"change\" ))\\n# {\\'change\\': -0.5193370165745856,\\n#  \\'date\\': datetime.datetime(2000, 9, 29, 0, 0),\\n#  \\'symbol\\': \\'AAPL\\'}\\n# see, e.g. http://money.cnn.com/2000/09/29/markets/techwrap/\\nWe can now use this new all_changes  data set to find which month is the best to\\ninvest in tech stocks. First we group the changes by month; then we compute the\\noverall change within each group.\\nOnce again, we write an appropriate value_transform  and then use group_by :\\n# to combine percent changes, we add 1 to each, multiply them, and subtract 1\\n# for instance, if we combine +10% and -20%, the overall change is\\n#    (1 + 10%) * (1 - 20%) - 1 = 1.1 * .8 - 1 = -12%\\ndef combine_pct_changes (pct_change1 , pct_change2 ):\\n    return (1 + pct_change1 ) * (1 + pct_change2 ) - 1\\ndef overall_change (changes):\\n    return reduce(combine_pct_changes , pluck(\"change\" , changes))\\noverall_change_by_month  = group_by (lambda row: row[\\'date\\'].month,\\n                                   all_changes ,\\n                                   overall_change )\\nWe’ll be doing these sorts of manipulations throughout the book, usually without\\ncalling too much explicit attention to them.\\nRescaling\\nMany techniques are sensitive to the scale  of your data. For example, imagine that you\\nhave a data set consisting of the heights and weights of hundreds of data scientists,\\nand that you are trying to identify clusters  of body sizes.\\nIntuitively, we’ d like clusters to represent points near each other, which means that we\\nneed some notion of distance between points. We already have a Euclidean distance\\nfunction,  so a natural approach might be to treat (height, weight) pairs as points in\\ntwo-dimensional space. Consider the people listed in Table 10-1 .\\nTable 10-1. Heights and Weights\\nPerson Height (inches) Height (centimeters) Weight\\nA 63 inches 160 cm 150 pounds\\nB 67 inches 170.2 cm 160 pounds\\nC 70 inches 177.8 cm 171 pounds\\n132 | Chapter 10: Working with Data\\nwww.it-ebooks.infoIf we measure height in inches, then B’s nearest neighbor is A:\\na_to_b = distance ([63, 150], [67, 160])        # 10.77\\na_to_c = distance ([63, 150], [70, 171])        # 22.14\\nb_to_c = distance ([67, 160], [70, 171])        # 11.40\\nHowever, if we measure height in centimeters, then B’s nearest neighbor is instead C:\\na_to_b = distance ([160, 150], [170.2, 160])    # 14.28\\na_to_c = distance ([160, 150], [177.8, 171])    # 27.53\\nb_to_c = distance ([170.2, 160], [177.8, 171])  # 13.37\\nObviously it’s problematic if changing units can change results like this. For this rea‐\\nson, when dimensions aren’t comparable with one another, we will sometimes rescale\\nour data so that each dimension has mean 0 and standard deviation 1. This effectively\\ngets rid of the units, converting each dimension to “standard deviations from the\\nmean. ”\\nTo start with, we’ll need to compute the mean  and the standard_deviation  for each\\ncolumn:\\ndef scale(data_matrix ):\\n    \"\"\"returns the means and standard deviations of each column\"\"\"\\n    num_rows , num_cols  = shape(data_matrix )\\n    means = [mean(get_column (data_matrix ,j))\\n             for j in range(num_cols )]\\n    stdevs = [standard_deviation (get_column (data_matrix ,j))\\n              for j in range(num_cols )]\\n    return means, stdevs\\nAnd then use them to create a new data matrix:\\ndef rescale(data_matrix ):\\n    \"\"\"rescales the input data so that each column\\n    has mean 0 and standard deviation 1\\n    leaves alone columns with no deviation\"\"\"\\n    means, stdevs = scale(data_matrix )\\n    def rescaled (i, j):\\n        if stdevs[j] > 0:\\n            return (data_matrix [i][j] - means[j]) / stdevs[j]\\n        else:\\n            return data_matrix [i][j]\\n    num_rows , num_cols  = shape(data_matrix )\\n    return make_matrix (num_rows , num_cols , rescaled )\\nAs always, you need to use your judgment. If you were to take a huge data set of\\nheights and weights and filter it down to only the people with heights between 69.5\\ninches and 70.5 inches, it’s quite likely (depending on the question you’re trying to\\nanswer) that the variation remaining is simply noise , and you might not want to put\\nits standard deviation on equal footing with other dimensions’ deviations.\\nRescaling | 133\\nwww.it-ebooks.infoDimensionality Reduction\\nSometimes the “actual” (or useful) dimensions of the data might not correspond to\\nthe dimensions we have. For example, consider the data set pictured in Figure 10-5 .\\nFigure 10-5. Data with the “wrong” axes\\nMost of the variation in the data seems to be along a single dimension that doesn’t\\ncorrespond to either the x-axis or the y-axis.\\nWhen this is the case, we can use a technique called principal component analysis  to\\nextract one or more dimensions that capture as much of the variation in the data as\\npossible.\\nIn practice, you wouldn’t use this technique on such a low-\\ndimensional data set. Dimensionality reduction is mostly useful\\nwhen your data set has a large number of dimensions and you want\\nto find a small subset that captures most of the variation. Unfortu‐\\nnately, that case is difficult to illustrate in a two-dimensional book\\nformat.\\nAs a first step, we’ll need to translate the data so that each dimension has mean zero:\\n134 | Chapter 10: Working with Data\\nwww.it-ebooks.infodef de_mean_matrix (A):\\n    \"\"\"returns the result of subtracting from every value in A the mean\\n    value of its column. the resulting matrix has mean 0 in every column\"\"\"\\n    nr, nc = shape(A)\\n    column_means , _ = scale(A)\\n    return make_matrix (nr, nc, lambda i, j: A[i][j] - column_means [j])\\n(If we don’t do this, our techniques are likely to identify the mean itself rather than\\nthe variation in the data.)\\nFigure 10-6  shows the example data after de-meaning.\\nFigure 10-6. Data after  de-meaning\\nNow, given a de-meaned matrix X, we can ask which is the direction that captures the\\ngreatest variance in the data?\\nSpecifically, given a direction d (a vector of magnitude 1), each row x in the matrix\\nextends dot(x, d)  in the d direction. And every nonzero vector w determines a\\ndirection if we rescale it to have magnitude 1:\\ndef direction (w):\\n    mag = magnitude (w)\\n    return [w_i / mag for w_i in w]\\nDimensionality Reduction | 135\\nwww.it-ebooks.infoTherefore, given a nonzero vector w, we can compute the variance of our data set in\\nthe direction determined by w:\\ndef directional_variance_i (x_i, w):\\n    \"\"\"the variance of the row x_i in the direction determined by w\"\"\"\\n    return dot(x_i, direction (w)) ** 2\\ndef directional_variance (X, w):\\n    \"\"\"the variance of the data in the direction determined w\"\"\"\\n    return sum(directional_variance_i (x_i, w)\\n               for x_i in X)\\nWe’ d like to find the direction that maximizes this variance. We can do this using gra‐\\ndient descent, as soon as we have the gradient function:\\ndef directional_variance_gradient_i (x_i, w):\\n    \"\"\"the contribution of row x_i to the gradient of\\n    the direction-w variance\"\"\"\\n    projection_length  = dot(x_i, direction (w))\\n    return [2 * projection_length  * x_ij for x_ij in x_i]\\ndef directional_variance_gradient (X, w):\\n    return vector_sum (directional_variance_gradient_i (x_i,w)\\n                      for x_i in X)\\nThe first principal component is just the direction that maximizes the direc\\ntional_variance  function:\\ndef first_principal_component (X):\\n    guess = [1 for _ in X[0]]\\n    unscaled_maximizer  = maximize_batch (\\n        partial(directional_variance , X),           # is now a function of w\\n        partial(directional_variance_gradient , X),  # is now a function of w\\n        guess)\\n    return direction (unscaled_maximizer )\\nOr, if you’ d rather use stochastic gradient descent:\\n# here there is no \"y\", so we just pass in a vector of Nones\\n# and functions that ignore that input\\ndef first_principal_component_sgd (X):\\n    guess = [1 for _ in X[0]]\\n    unscaled_maximizer  = maximize_stochastic (\\n        lambda x, _, w: directional_variance_i (x, w),\\n        lambda x, _, w: directional_variance_gradient_i (x, w),\\n        X,\\n        [None for _ in X],   # the fake \"y\"\\n        guess)\\n    return direction (unscaled_maximizer )\\nOn the de-meaned data set, this returns the direction [0.924, 0.383] , which does\\nappear to capture the primary axis along which our data varies ( Figure 10-7 ).\\n136 | Chapter 10: Working with Data\\nwww.it-ebooks.infoFigure 10-7. First principal component\\nOnce we’ve found the direction that’s the first principal component, we can project\\nour data onto it to find the values of that component:\\ndef project(v, w):\\n    \"\"\"return the projection of v onto the direction w\"\"\"\\n    projection_length  = dot(v, w)\\n    return scalar_multiply (projection_length , w)\\nIf we want to find further components, we first remove the projections from the data:\\ndef remove_projection_from_vector (v, w):\\n    \"\"\"projects v onto w and subtracts the result from v\"\"\"\\n    return vector_subtract (v, project(v, w))\\ndef remove_projection (X, w):\\n    \"\"\"for each row of X\\n    projects the row onto w, and subtracts the result from the row\"\"\"\\n    return [remove_projection_from_vector (x_i, w) for x_i in X]\\nBecause this example data set is only two-dimensional, after we remove the first com‐\\nponent, what’s left will be effectively one-dimensional ( Figure 10-8 ).\\nDimensionality Reduction | 137\\nwww.it-ebooks.infoFigure 10-8. Data after  removing first principal component\\nAt that point, we can find the next principal component by repeating the process on\\nthe result of remove_projection  (Figure 10-9 ).\\nOn a higher-dimensional data set, we can iteratively find as many components as we\\nwant:\\ndef principal_component_analysis (X, num_components ):\\n    components  = []\\n    for _ in range(num_components ):\\n        component  = first_principal_component (X)\\n        components .append(component )\\n        X = remove_projection (X, component )\\n    return components\\nWe can then transform  our data into the lower-dimensional space spanned by the\\ncomponents:\\ndef transform_vector (v, components ):\\n    return [dot(v, w) for w in components ]\\ndef transform (X, components ):\\n    return [transform_vector (x_i, components ) for x_i in X]\\n138 | Chapter 10: Working with Data\\nwww.it-ebooks.infoThis technique is valuable for a couple of reasons. First, it can help us clean our data\\nby eliminating noise dimensions and consolidating dimensions that are highly corre‐\\nlated.\\nFigure 10-9. First two principal components\\nSecond, after extracting a low-dimensional representation of our data, we can use a\\nvariety of techniques that don’t work as well on high-dimensional data. We’ll see\\nexamples of such techniques throughout the book.\\nAt the same time, while it can help you build better models, it can also make those\\nmodels harder to interpret. It’s easy to understand conclusions like “every extra year\\nof experience adds an average of $10k in salary. ” It’s much harder to make sense of\\n“every increase of 0.1 in the third principal component adds an average of $10k in\\nsalary. ”\\nFor Further Exploration\\n•As we mentioned at the end of Chapter 9 , pandas  is probably the primary Python\\ntool for cleaning, munging, manipulating, and working with data. All the exam‐\\nples we did by hand in this chapter could be done much more simply using pan‐\\ndas. Python for Data Analysis  (O’Reilly) is probably the best way to learn pandas.\\nFor Further Exploration | 139\\nwww.it-ebooks.info•scikit-learn has a wide variety of matrix decomposition  functions, including\\nPCA.\\n140 | Chapter 10: Working with Data\\nwww.it-ebooks.infoCHAPTER 11\\nMachine Learning\\nI am always ready to learn although I do not always like being taught.\\n—Winston Churchill\\nMany people imagine that data science is mostly machine learning and that data sci‐\\nentists mostly build and train and tweak machine-learning models all day long. (Then\\nagain, many of those people don’t actually know what machine learning is.) In fact,\\ndata science is mostly turning business problems into data problems and collecting\\ndata and understanding data and cleaning data and formatting data, after which\\nmachine learning is almost an afterthought. Even so, it’s an interesting and essential\\nafterthought that you pretty much have to know about in order to do data science.\\nModeling\\nBefore we can talk about machine learning we need to talk about models .\\nWhat is a model? It’s simply a specification of a mathematical (or probabilistic) rela‐\\ntionship that exists between different variables.\\nFor instance, if you’re trying to raise money for your social networking site, you\\nmight  build a business model  (likely in a spreadsheet) that takes inputs like “number\\nof users” and “ad revenue per user” and “number of employees” and outputs your\\nannual profit for the next several years. A cookbook recipe entails a model that relates\\ninputs like “number of eaters” and “hungriness” to quantities of ingredients needed.\\nAnd if you’ve ever watched poker on television, you know that they estimate each\\nplayer’s “win probability” in real time based on a model that takes into account the\\ncards that have been revealed so far and the distribution of cards in the deck.\\nThe business model is probably based on simple mathematical relationships: profit is\\nrevenue minus expenses, revenue is units sold times average price, and so on. The\\n141\\nwww.it-ebooks.inforecipe model is probably based on trial and error—someone went in a kitchen and\\ntried different combinations of ingredients until they found one they liked. And the\\npoker model is based on probability theory, the rules of poker, and some reasonably\\ninnocuous assumptions about the random process by which cards are dealt.\\nWhat Is Machine Learning?\\nEveryone has her own exact definition, but we’ll use machine learning  to refer to cre‐\\nating and using models that are learned from data . In other contexts this might be\\ncalled predictive modeling  or data mining , but we will stick with machine learning.\\nTypically, our goal will be to use existing data to develop models that we can use to\\npredict  various outcomes for new data, such as:\\n•Predicting whether an email message is spam or not\\n•Predicting whether a credit card transaction is fraudulent\\n•Predicting which advertisement a shopper is most likely to click on\\n•Predicting which football team is going to win the Super Bowl\\nWe’ll look at both supervised  models (in which there is a set of data labeled with the\\ncorrect answers to learn from), and unsupervised  models (in which there are no such\\nlabels).  There are various other types like semisupervised  (in which only some of the\\ndata are labeled) and online  (in which the model needs to continuously adjust to\\nnewly arriving data) that we won’t cover in this book.\\nNow, in even the simplest situation there are entire universes of models that might\\ndescribe the relationship we’re interested in. In most cases we will ourselves choose a\\nparameterized  family of models and then use data to learn parameters that are in\\nsome way optimal.\\nFor instance, we might assume that a person’s height is (roughly) a linear function of\\nhis weight and then use data to learn what that linear function is. Or we might\\nassume that a decision tree is a good way to diagnose what diseases our patients have\\nand then use data to learn the “optimal” such tree. Throughout the rest of the book\\nwe’ll be investigating different families of models that we can learn.\\nBut before we can do that, we need to better understand the fundamentals of machine\\nlearning. For the rest of the chapter, we’ll discuss some of those basic concepts, before\\nwe move on to the models themselves.\\nOverfitting  and Underfitting\\nA common danger in machine learning is overfitting —producing a model that per‐\\nforms well on the data you train it on but that generalizes poorly to any new data. \\n142 | Chapter 11: Machine Learning\\nwww.it-ebooks.infoThis could involve learning noise  in the data. Or it could involve learning to identify\\nspecific inputs rather than whatever factors are actually predictive for the desired out‐\\nput.\\nThe other side of this is underfitting , producing a model that doesn’t perform well\\neven on the training data, although typically when this happens you decide your\\nmodel isn’t good enough and keep looking for a better one.\\nFigure 11-1. Overfitting  and underfitting\\nIn Figure 11-1 , I’ve fit three polynomials to a sample of data. (Don’t worry about how;\\nwe’ll get to that in later chapters.)\\nThe horizontal line shows the best fit degree 0 (i.e., constant) polynomial. It severely\\nunderfits  the training data. The best fit degree 9 (i.e., 10-parameter) polynomial goes\\nthrough every training data point exactly, but it very severely overfits —if we were to\\npick a few more data points it would quite likely miss them by a lot. And the degree 1\\nline strikes a nice balance—it’s pretty close to every point, and (if these data are repre‐\\nsentative) the line will likely be close to new data points as well.\\nClearly models that are too complex lead to overfitting and don’t generalize well\\nbeyond the data they were trained on. So how do we make sure our models aren’t too\\nOverfitting  and Underfitting  | 143\\nwww.it-ebooks.infocomplex? The most fundamental approach involves using different data to train the\\nmodel and to test the model.\\nThe simplest way to do this is to split your data set, so that (for example) two-thirds\\nof it is used to train the model, after which we measure the model’s performance on\\nthe remaining third:\\ndef split_data (data, prob):\\n    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\\n    results = [], []\\n    for row in data:\\n        results[0 if random.random() < prob else 1].append(row)\\n    return results\\nOften, we’ll have a matrix x of input variables and a vector y of output variables. In\\nthat case, we need to make sure to put corresponding values together in either the\\ntraining data or the test data:\\ndef train_test_split (x, y, test_pct ):\\n    data = zip(x, y)                              # pair corresponding values\\n    train, test = split_data (data, 1 - test_pct )  # split the data set of pairs\\n    x_train, y_train = zip(*train)                # magical un-zip trick\\n    x_test, y_test = zip(*test)\\n    return x_train, x_test, y_train, y_test\\nso that you might do something like:\\nmodel = SomeKindOfModel ()\\nx_train, x_test, y_train, y_test = train_test_split (xs, ys, 0.33)\\nmodel.train(x_train, y_train)\\nperformance  = model.test(x_test, y_test)\\nIf the model was overfit to the training data, then it will hopefully perform really\\npoorly on the (completely separate) test data. Said differently, if it performs well on\\nthe test data, then you can be more confident that it’s fitting  rather than overfitting .\\nHowever, there are a couple of ways this can go wrong.\\nThe first is if there are common patterns in the test and train data that wouldn’t gen‐\\neralize to a larger data set.\\nFor example, imagine that your data set consists of user activity, one row per user per\\nweek. In such a case, most users will appear in both the training data and the test\\ndata, and certain models might learn to identify  users rather than discover relation‐\\nships involving attributes . This isn’t a huge worry, although it did happen to me once.\\nA bigger problem is if you use the test/train split not just to judge a model but also to\\nchoose  from among many models. In that case, although each individual model may\\nnot be overfit, the “choose a model that performs best on the test set” is a meta-\\ntraining that makes the test set function as a second training set. (Of course the\\nmodel that performed best on the test set is going to perform well on the test set.)\\n144 | Chapter 11: Machine Learning\\nwww.it-ebooks.infoIn such a situation, you should split the data into three parts: a training  set for build‐\\ning models, a validation  set for choosing among trained models, and a test set for\\njudging the final model.\\nCorrectness\\nWhen I’m not doing data science, I dabble in medicine. And in my spare time I’ve\\ncome up with a cheap, noninvasive test that can be given to a newborn baby that pre‐\\ndicts—with greater than 98% accuracy  — whether the newborn will ever develop leu‐\\nkemia. My lawyer has convinced me the test is unpatentable, so I’ll share with you the\\ndetails here: predict leukemia if and only if the baby is named Luke (which sounds\\nsort of like “leukemia”).\\nAs we’ll see below, this test is indeed more than 98% accurate. Nonetheless, it’s an\\nincredibly stupid test, and a good illustration of why we don’t typically use “accuracy”\\nto measure how good a model is.\\nImagine building a model to make a binary  judgment. Is this email spam? Should we\\nhire this candidate? Is this air traveler secretly a terrorist?\\nGiven a set of labeled data and such a predictive model, every data point lies in one of\\nfour categories:\\n•True positive: “This message is spam, and we correctly predicted spam. ”\\n•False positive (Type 1 Error): “This message is not spam, but we predicted spam. ”\\n•False negative (Type 2 Error): “This message is spam, but we predicted not\\nspam. ”\\n•True negative: “This message is not spam, and we correctly predicted not spam. ”\\nWe often represent these as counts in a confusion matrix :\\nSpam not Spam\\npredict “Spam” True Positive False Positive\\npredict “Not Spam” False Negative True Negative\\nLet’s see how my leukemia test fits into this framework. These days approximately 5\\nbabies out of 1,000 are named Luke . And the lifetime prevalence of leukemia is about\\n1.4%, or 14 out of every 1,000 people .\\nIf we believe these two factors are independent and apply my “Luke is for leukemia”\\ntest to 1 million people, we’ d expect to see a confusion matrix like:\\nCorrectness | 145\\nwww.it-ebooks.infoleukemia no leukemia total\\n“Luke” 70 4,930 5,000\\nnot “Luke” 13,930 981,070 995,000\\ntotal 14,000 986,000 1,000,000\\nWe can then use these to compute various statistics about model performance. For\\nexample, accuracy  is defined as the fraction of correct predictions:\\ndef accuracy (tp, fp, fn, tn):\\n    correct = tp + tn\\n    total = tp + fp + fn + tn\\n    return correct / total\\nprint accuracy (70, 4930, 13930, 981070)     # 0.98114\\nThat seems like a pretty impressive number. But clearly this is not a good test, which\\nmeans that we probably shouldn’t put a lot of credence in raw accuracy.\\nIt’s common to look at the combination of precision  and recall . Precision measures\\nhow accurate our positive  predictions were:\\ndef precision (tp, fp, fn, tn):\\n    return tp / (tp + fp)\\nprint precision (70, 4930, 13930, 981070)    # 0.014\\nAnd recall measures what fraction of the positives our model identified:\\ndef recall(tp, fp, fn, tn):\\n    return tp / (tp + fn)\\nprint recall(70, 4930, 13930, 981070)       # 0.005\\nThese are both terrible numbers, reflecting that this is a terrible model.\\nSometimes precision and recall are combined into the F1 score , which is defined as:\\ndef f1_score (tp, fp, fn, tn):\\n    p = precision (tp, fp, fn, tn)\\n    r = recall(tp, fp, fn, tn)\\n    return 2 * p * r / (p + r)\\nThis is the harmonic mean  of precision and recall and necessarily lies between them.\\nUsually the choice of a model involves a trade-off between precision and recall. A\\nmodel that predicts “yes” when it’s even a little bit confident will probably have a high\\nrecall but a low precision; a model that predicts “yes” only when it’s extremely confi‐\\ndent is likely to have a low recall and a high precision.\\n146 | Chapter 11: Machine Learning\\nwww.it-ebooks.infoAlternatively, you can think of this as a trade-off between false positives and false\\nnegatives. Saying “yes” too often will give you lots of false positives; saying “no” too\\noften will give you lots of false negatives.\\nImagine that there were 10 risk factors for leukemia, and that the more of them you\\nhad the more likely you were to develop leukemia. In that case you can imagine a\\ncontinuum of tests: “predict leukemia if at least one risk factor, ” “predict leukemia if at\\nleast two risk factors, ” and so on. As you increase the threshhold, you increase the\\ntest’s precision (since people with more risk factors are more likely to develop the dis‐\\nease), and you decrease the test’s recall (since fewer and fewer of the eventual disease-\\nsufferers will meet the threshhold). In cases like this, choosing the right threshhold is\\na matter of finding the right trade-off.\\nThe Bias-Variance Trade-off\\nAnother way of thinking about the overfitting problem is as a trade-off between bias\\nand variance.\\nBoth are measures of what would happen if you were to retrain your model many\\ntimes on different sets of training data (from the same larger population).\\nFor example, the degree 0 model in “Overfitting and Underfitting” on page 142 will\\nmake a lot of mistakes for pretty much any training set (drawn from the same popu‐\\nlation), which means that it has a high bias. However, any two randomly chosen\\ntraining sets should give pretty similar models (since any two randomly chosen train‐\\ning sets should have pretty similar average values). So we say that it has a low var‐\\niance . High bias and low variance typically correspond to underfitting.\\nOn the other hand, the degree 9 model fit the training set perfectly. It has very low\\nbias but very high variance (since any two training sets would likely give rise to very\\ndifferent models). This corresponds to overfitting.\\nThinking about model problems this way can help you figure out what do when your\\nmodel doesn’t work so well.\\nIf your model has high bias (which means it performs poorly even on your training\\ndata) then one thing to try is adding more features. Going from the degree 0 model in\\n“Overfitting and Underfitting”  on page 142 to the degree 1 model was a big improve‐\\nment.\\nIf your model has high variance, then you can similarly remove  features. But another\\nsolution is to obtain more data (if you can).\\nThe Bias-Variance Trade-off  | 147\\nwww.it-ebooks.infoFigure 11-2. Reducing variance with more data\\nIn Figure 11-2 , we fit a degree 9 polynomial to different size samples. The model fit\\nbased on 10 data points is all over the place, as we saw before. If we instead trained on\\n100 data points, there’s much less overfitting. And the model trained from 1,000 data\\npoints looks very similar to the degree 1 model.\\nHolding model complexity constant, the more data you have, the harder it is to over‐\\nfit.\\nOn the other hand, more data won’t help with bias.  If your model doesn’t use enough\\nfeatures to capture regularities in the data, throwing more data at it won’t help.\\nFeature Extraction and Selection\\nAs we mentioned, when your data doesn’t have enough features, your model is likely\\nto underfit. And when your data has too many features, it’s easy to overfit. But what\\nare features and where do they come from?\\nFeatures  are whatever inputs we provide to our model.\\n148 | Chapter 11: Machine Learning\\nwww.it-ebooks.infoIn the simplest case, features are simply given to you. If you want to predict some‐\\none’s salary based on her years of experience, then years of experience is the only fea‐\\nture you have.\\n(Although, as we saw in “Overfitting and Underfitting” on page 142, you might also\\nconsider adding years of experience squared, cubed, and so on if that helps you build\\na better model.)\\nThings become more interesting as your data becomes more complicated. Imagine\\ntrying to build a spam filter to predict whether an email is junk or not. Most models\\nwon’t know what to do with a raw email, which is just a collection of text.  Y ou’ll have\\nto extract features. For example:\\n•Does the email contain the word “Viagra”?\\n•How many times does the letter d appear?\\n•What was the domain of the sender?\\nThe first is simply a yes or no, which we typically encode as a 1 or 0. The second is a\\nnumber. And the third is a choice from a discrete set of options.\\nPretty much always, we’ll extract features from our data that fall into one of these\\nthree categories. What’s more, the type of features we have constrains the type of\\nmodels we can use.\\nThe Naive Bayes classifier we’ll build in Chapter 13  is suited to yes-or-no features, like\\nthe first one in the preceding list.\\nRegression models, as we’ll study in Chapter 14  and Chapter 16 , require numeric fea‐\\ntures (which could include dummy variables that are 0s and 1s).\\nAnd decision trees, which we’ll look at in Chapter 17 , can deal with numeric or cate‐\\ngorical data.\\nAlthough in the spam filter example we looked for ways to create features, sometimes\\nwe’ll instead look for ways to remove features.\\nFor example, your inputs might be vectors of several hundred numbers. Depending\\non the situation, it might be appropriate to distill these down to handful of important\\ndimensions (as in “Dimensionality Reduction” on page 134) and use only those small\\nnumber of features. Or it might be appropriate to use a technique (like regularization,\\nwhich we’ll look at in “Regularization”  on page 186) that penalizes models the more\\nfeatures they use.\\nHow do we choose features? That’s where a combination of experience  and domain\\nexpertise  comes into play. If you’ve received lots of emails, then you probably have a\\nsense that the presence of certain words might be a good indicator of spamminess.\\nAnd you might also have a sense that the number of d’s is likely not a good indicator\\nFeature Extraction and Selection | 149\\nwww.it-ebooks.infoof spamminess. But in general you’ll have to try different things, which is part of the\\nfun.\\nFor Further Exploration\\n•Keep reading! The next several chapters are about different families of machine-\\nlearning models.\\n•The Coursera Machine Learning  course is the original MOOC and is a good\\nplace to get a deeper understanding of the basics of machine learning. The Cal‐\\ntech Machine Learning  MOOC is also good.\\n•The Elements of Statistical Learning  is a somewhat canonical textbook that can be\\ndownloaded online for free . But be warned: it’s very mathy.\\n150 | Chapter 11: Machine Learning\\nwww.it-ebooks.infoCHAPTER 12\\nk-Nearest Neighbors\\nIf you want to annoy your neighbors, tell the truth about them.\\n—Pietro Aretino\\nImagine that you’re trying to predict how I’m going to vote in the next presidential\\nelection. If you know nothing else about me (and if you have the data), one sensible\\napproach is to look at how my neighbors  are planning to vote. Living in downtown\\nSeattle, as I do, my neighbors are invariably planning to vote for the Democratic can‐\\ndidate, which suggests that “Democratic candidate” is a good guess for me as well.\\nNow imagine you know more about me than just geography—perhaps you know my\\nage, my income, how many kids I have, and so on. To the extent my behavior is influ‐\\nenced (or characterized) by those things, looking just at my neighbors who are close\\nto me among all those dimensions seems likely to be an even better predictor than\\nlooking at all my neighbors. This is the idea behind nearest neighbors classification .\\nThe Model\\nNearest neighbors is one of the simplest predictive models there is.  It makes no math‐\\nematical assumptions, and it doesn’t require any sort of heavy machinery. The only\\nthings it requires are:\\n•Some notion of distance\\n•An assumption that points that are close to one another are similar\\nMost of the techniques we’ll look at in this book look at the data set as a whole in\\norder to learn patterns in the data. Nearest neighbors, on the other hand, quite con‐\\nsciously neglects a lot of information, since the prediction for each new point\\ndepends only on the handful of points closest to it.\\n151\\nwww.it-ebooks.infoWhat’s more, nearest neighbors is probably not going to help you understand the\\ndrivers of whatever phenomenon you’re looking at. Predicting my votes based on my\\nneighbors’ votes doesn’t tell you much about what causes me to vote the way I do,\\nwhereas some alternative model that predicted my vote based on (say) my income\\nand marital status very well might.\\nIn the general situation, we have some data points and we have a corresponding set of\\nlabels. The labels could be True  and False , indicating whether each input satisfies\\nsome condition like “is spam?” or “is poisonous?” or “would be enjoyable to watch?”\\nOr they could be categories, like movie ratings (G, PG, PG-13, R, NC-17). Or they\\ncould be the names of presidential candidates. Or they could be favorite program‐\\nming languages.\\nIn our case, the data points will be vectors, which means that we can use the distance\\nfunction from Chapter 4 .\\nLet’s say we’ve picked a number k like 3 or 5. Then when we want to classify some\\nnew data point, we find the k nearest labeled points and let them vote on the new\\noutput.\\nTo do this, we’ll need a function that counts votes. One possibility is:\\ndef raw_majority_vote (labels):\\n    votes = Counter(labels)\\n    winner, _ = votes.most_common (1)[0]\\n    return winner\\nBut this doesn’t do anything intelligent with ties. For example, imagine we’re rating\\nmovies and the five nearest movies are rated G, G, PG, PG, and R. Then G has two\\nvotes and PG also has two votes. In that case, we have several options:\\n•Pick one of the winners at random.\\n•Weight the votes by distance and pick the weighted winner.\\n•Reduce k until we find a unique winner.\\nWe’ll implement the third:\\ndef majority_vote (labels):\\n    \"\"\"assumes that labels are ordered from nearest to farthest\"\"\"\\n    vote_counts  = Counter(labels)\\n    winner, winner_count  = vote_counts .most_common (1)[0]\\n    num_winners  = len([count\\n                       for count in vote_counts .values()\\n                       if count == winner_count ])\\n    if num_winners  == 1:\\n        return winner                     # unique winner, so return it\\n    else:\\n        return majority_vote (labels[:-1]) # try again without the farthest\\n152 | Chapter 12: k-Nearest Neighbors\\nwww.it-ebooks.infoThis approach is sure to work eventually, since in the worst case we go all the way\\ndown to just one label, at which point that one label wins.\\nWith this function it’s easy to create a classifier:\\ndef knn_classify (k, labeled_points , new_point ):\\n    \"\"\"each labeled point should be a pair (point, label)\"\"\"\\n    # order the labeled points from nearest to farthest\\n    by_distance  = sorted(labeled_points ,\\n                         key=lambda (point, _): distance (point, new_point ))\\n    # find the labels for the k closest\\n    k_nearest_labels  = [label for _, label in by_distance [:k]]\\n    # and let them vote\\n    return majority_vote (k_nearest_labels )\\nLet’s take a look at how this works.\\nExample: Favorite Languages\\nThe results of the first DataSciencester user survey are back, and we’ve found the pre‐\\nferred programming languages of our users in a number of large cities:\\n# each entry is ([longitude, latitude], favorite_language)\\ncities = [([-122.3 , 47.53], \"Python\" ),  # Seattle\\n          ([ -96.85, 32.85], \"Java\"),    # Austin\\n          ([ -89.33, 43.13], \"R\"),       # Madison\\n          # ... and so on\\n]\\nThe VP of Community Engagement wants to know if we can use these results to pre‐\\ndict the favorite programming languages for places that weren’t part of our survey.\\nAs usual, a good first step is plotting the data ( Figure 12-1 ):\\n# key is language, value is pair (longitudes, latitudes)\\nplots = { \"Java\" : ([], []), \"Python\"  : ([], []), \"R\" : ([], []) }\\n# we want each language to have a different marker and color\\nmarkers = { \"Java\" : \"o\", \"Python\"  : \"s\", \"R\" : \"^\" }\\ncolors  = { \"Java\" : \"r\", \"Python\"  : \"b\", \"R\" : \"g\" }\\nfor (longitude , latitude ), language  in cities:\\n    plots[language ][0].append(longitude )\\n    plots[language ][1].append(latitude )\\n# create a scatter series for each language\\nfor language , (x, y) in plots.iteritems ():\\n    plt.scatter(x, y, color=colors[language ], marker=markers[language ],\\nExample: Favorite Languages | 153\\nwww.it-ebooks.info                      label=language , zorder=10)\\nplot_state_borders (plt)      # pretend we have a function that does this\\nplt.legend(loc=0)            # let matplotlib choose the location\\nplt.axis([-130,-60,20,55])   # set the axes\\nplt.title(\"Favorite Programming Languages\" )\\nplt.show()\\nFigure 12-1. Favorite programming languages\\nY ou may have noticed the call to plot_state_borders() , a func‐\\ntion that we haven’t actually defined. There’s an implementation on\\nthe book’s GitHub page , but it’s a good exercise to try to do it your‐\\nself:\\n1.Search the Web for something like state boundaries latitude\\nlongitude .\\n2.Convert whatever data you can find into a list of segments\\n[(long1, lat1), (long2, lat2)].\\n3.Use plt.plot()  to draw the segments.\\n154 | Chapter 12: k-Nearest Neighbors\\nwww.it-ebooks.infoSince it looks like nearby places tend to like the same language, k-nearest neighbors\\nseems like a reasonable choice for a predictive model.\\nTo start with, let’s look at what happens if we try to predict each city’s preferred lan‐\\nguage using its neighbors other than itself:\\n# try several different values for k\\nfor k in [1, 3, 5, 7]:\\n    num_correct  = 0\\n    for city in cities:\\n        location , actual_language  = city\\n        other_cities  = [other_city\\n                        for other_city  in cities\\n                        if other_city  != city]\\n        predicted_language  = knn_classify (k, other_cities , location )\\n        if predicted_language  == actual_language :\\n            num_correct  += 1\\n    print k, \"neighbor[s]:\" , num_correct , \"correct out of\" , len(cities)\\nIt looks like 3-nearest neighbors performs the best, giving the correct result about\\n59% of the time:\\n1 neighbor[s]: 40 correct out of 75\\n3 neighbor[s]: 44 correct out of 75\\n5 neighbor[s]: 41 correct out of 75\\n7 neighbor[s]: 35 correct out of 75\\nNow we can look at what regions would get classified to which languages under each\\nnearest neighbors scheme. We can do that by classifying an entire grid worth of\\npoints, and then plotting them as we did the cities:\\nplots = { \"Java\" : ([], []), \"Python\"  : ([], []), \"R\" : ([], []) }\\nk = 1 # or 3, or 5, or ...\\nfor longitude  in range(-130, -60):\\n    for latitude  in range(20, 55):\\n        predicted_language  = knn_classify (k, cities, [longitude , latitude ])\\n        plots[predicted_language ][0].append(longitude )\\n        plots[predicted_language ][1].append(latitude )\\nFor instance, Figure 12-2  shows what happens when we look at just the nearest neigh‐\\nbor ( k = 1).\\nWe see lots of abrupt changes from one language to another with sharp boundaries.\\nAs we increase the number of neighbors to three, we see smoother regions for each\\nlanguage ( Figure 12-3 ).\\nExample: Favorite Languages | 155\\nwww.it-ebooks.infoAnd as we increase the neighbors to five, the boundaries get smoother still\\n(Figure 12-4 ).\\nHere our dimensions are roughly comparable, but if they weren’t you might want to\\nrescale the data as we did in “Rescaling” on page 132 .\\nFigure 12-2. 1-Nearest neighbor programming languages\\nThe Curse of Dimensionality\\nk-nearest neighbors runs into trouble in higher dimensions thanks to the “curse of\\ndimensionality, ” which boils down to the fact that high-dimensional spaces are vast.\\nPoints in high-dimensional spaces tend not to be close to one another at all. One way\\nto see this is by randomly generating pairs of points in the d-dimensional “unit cube”\\nin a variety of dimensions, and calculating the distances between them.\\n156 | Chapter 12: k-Nearest Neighbors\\nwww.it-ebooks.infoFigure 12-3. 3-Nearest neighbor programming languages\\nGenerating random points should be second nature by now:\\ndef random_point (dim):\\n    return [random.random() for _ in range(dim)]\\nas is writing a function to generate the distances:\\ndef random_distances (dim, num_pairs ):\\n    return [distance (random_point (dim), random_point (dim))\\n            for _ in range(num_pairs )]\\nThe Curse of Dimensionality | 157\\nwww.it-ebooks.infoFigure 12-4. 5-Nearest neighbor programming languages\\nFor every dimension from 1 to 100, we’ll compute 10,000 distances and use those to\\ncompute the average distance between points and the minimum distance between\\npoints in each dimension ( Figure 12-5 ):\\ndimensions  = range(1, 101)\\navg_distances  = []\\nmin_distances  = []\\nrandom.seed(0)\\nfor dim in dimensions :\\n  distances  = random_distances (dim, 10000)  # 10,000 random pairs\\n  avg_distances .append(mean(distances ))     # track the average\\n  min_distances .append(min(distances ))      # track the minimum\\n158 | Chapter 12: k-Nearest Neighbors\\nwww.it-ebooks.infoFigure 12-5. The curse of dimensionality\\nAs the number of dimensions increases, the average distance between points increa‐\\nses. But what’s more problematic is the ratio between the closest distance and the\\naverage distance ( Figure 12-6 ):\\nmin_avg_ratio  = [min_dist  / avg_dist\\n                 for min_dist , avg_dist  in zip(min_distances , avg_distances )]\\nThe Curse of Dimensionality | 159\\nwww.it-ebooks.infoFigure 12-6. The curse of dimensionality again\\nIn low-dimensional data sets, the closest points tend to be much closer than average.\\nBut two points are close only if they’re close in every dimension, and every extra\\ndimension—even if just noise—is another opportunity for each point to be further\\naway from every other point. When you have a lot of dimensions, it’s likely that the\\nclosest points aren’t much closer than average, which means that two points being\\nclose doesn’t mean very much (unless there’s a lot of structure in your data that makes\\nit behave as if it were much lower-dimensional).\\nA different way of thinking about the problem involves the sparsity of higher-\\ndimensional spaces.\\nIf you pick 50 random numbers between 0 and 1, you’ll probably get a pretty good\\nsample of the unit interval ( Figure 12-7 ).\\n160 | Chapter 12: k-Nearest Neighbors\\nwww.it-ebooks.infoFigure 12-7. Fifty  random points in one dimension\\nIf you pick 50 random points in the unit square, you’ll get less coverage ( Figure 12-8 ).\\nThe Curse of Dimensionality | 161\\nwww.it-ebooks.infoFigure 12-8. Fifty  random points in two dimensions\\nAnd in three dimensions less still ( Figure 12-9 ).\\nmatplotlib  doesn’t graph four dimensions well, so that’s as far as we’ll go, but you\\ncan see already that there are starting to be large empty spaces with no points near\\nthem. In more dimensions—unless you get exponentially more data—those large\\nempty spaces represent regions far from all the points you want to use in your predic‐\\ntions.\\nSo if you’re trying to use nearest neighbors in higher dimensions, it’s probably a good\\nidea to do some kind of dimensionality reduction first.\\n162 | Chapter 12: k-Nearest Neighbors\\nwww.it-ebooks.infoFigure 12-9. Fifty  random points in three dimensions\\nFor Further Exploration\\nscikit-learn has many nearest neighbor  models.\\nFor Further Exploration | 163\\nwww.it-ebooks.infowww.it-ebooks.infoCHAPTER 13\\nNaive Bayes\\nIt is well for the heart to be naive and for the mind not to be.\\n—Anatole France\\nA social network isn’t much good if people can’t network.  Accordingly, DataSciences‐\\nter has a popular feature that allows members to send messages to other members.\\nAnd while most of your members are responsible citizens who send only well-\\nreceived “how’s it going?” messages, a few miscreants persistently spam other mem‐\\nbers about get-rich schemes, no-prescription-required pharmaceuticals, and for-\\nprofit data science credentialing programs. Y our users have begun to complain, and\\nso the VP of Messaging has asked you to use data science to figure out a way to filter\\nout these spam messages.\\nA Really Dumb Spam Filter\\nImagine a “universe” that consists of receiving a message chosen randomly from all\\npossible messages. Let S be the event “the message is spam” and V be the event “the\\nmessage contains the word viagra . ” Then Bayes’s Theorem tells us that the probability\\nthat the message is spam conditional on containing the word viagra  is:\\nPSV=PVSPS/PVSPS+PV¬SP¬S\\nThe numerator is the probability that a message is spam and contains viagra , while\\nthe denominator is just the probability that a message contains viagra . Hence you can\\nthink of this calculation as simply representing the proportion of viagra  messages that\\nare spam.\\nIf we have a large collection of messages we know are spam, and a large collection of\\nmessages we know are not spam, then we can easily estimate PVS and PV¬S. If\\n165\\nwww.it-ebooks.infowe further assume that any message is equally likely to be spam or not-spam (so that\\nPS=P¬S= 0 . 5 ), then:\\nPSV=PVS/PVS+PV¬S\\nFor example, if 50% of spam messages have the word viagra , but only 1% of nonspam\\nmessages do, then the probability that any given viagra -containing email is spam is:\\n0 . 5/ 0 . 5 + 0 . 01 = 98 %\\nA More Sophisticated Spam Filter\\nImagine now that we have a vocabulary of many words w1, ...,wn. To move this into\\nthe realm of probability theory, we’ll write Xi for the event “a message contains the\\nword wi. ” Also imagine that (through some unspecified-at-this-point process) we’ve\\ncome up with an estimate PXiS for the probability that a spam message contains\\nthe ith word, and a similar estimate PXi¬S for the probability that a nonspam\\nmessage contains the ith word.\\nThe key to Naive Bayes is making the (big) assumption that the presences (or absen‐\\nces) of each word are independent of one another, conditional on a message being\\nspam or not. Intuitively, this assumption means that knowing whether a certain spam\\nmessage contains the word “viagra” gives you no information about whether that\\nsame message contains the word “rolex. ” In math terms, this means that:\\nPX1=x1, . . . , Xn=xnS=PX1=x1S×⋯×PXn=xnS\\nThis is an extreme assumption. (There’s a reason the technique has “naive” in its\\nname.) Imagine that our vocabulary consists only of the words “viagra” and “rolex, ”\\nand that half of all spam messages are for “cheap viagra” and that the other half are for\\n“authentic rolex. ” In this case, the Naive Bayes estimate that a spam message contains\\nboth “viagra” and “rolex” is:\\nPX1= 1, X2= 1 S=PX1= 1 SPX2= 1 S= . 5 × . 5 = . 25\\nsince we’ve assumed away the knowledge that “viagra” and “rolex” actually never\\noccur together. Despite the unrealisticness of this assumption, this model often per‐\\nforms well and is used in actual spam filters.\\nThe same Bayes’s Theorem reasoning we used for our “viagra-only” spam filter tells\\nus that we can calculate the probability a message is spam using the equation:\\n166 | Chapter 13: Naive Bayes\\nwww.it-ebooks.infoPSX=x=PX=xS/PX=xS+PX=x¬S\\nThe Naive Bayes assumption allows us to compute each of the probabilities on the\\nright simply by multiplying together the individual probability estimates for each\\nvocabulary word.\\nIn practice, you usually want to avoid multiplying lots of probabilities together, to\\navoid a problem called underflow , in which computers don’t deal well with floating-\\npoint numbers that are too close to zero. Recalling from algebra that\\nlog ab= log a+ log b and that exp logx=x, we usually compute p1*⋯*pn as\\nthe equivalent (but floating-point-friendlier):\\nexp log p1+⋯+ log pn\\nThe only challenge left is coming up with estimates for PXiS and PXi¬S, the\\nprobabilities that a spam message (or nonspam message) contains the word wi. If we\\nhave a fair number of “training” messages labeled as spam and not-spam, an obvious\\nfirst try is to estimate PXiS simply as the fraction of spam messages containing\\nword wi.\\nThis causes a big problem, though. Imagine that in our training set the vocabulary\\nword “data” only occurs in nonspam messages. Then we’ d estimate P“data” S= 0.\\nThe result is that our Naive Bayes classifier would always assign spam probability 0 to\\nany message containing the word “data, ” even a message like “data on cheap viagra\\nand authentic rolex watches. ” To avoid this problem, we usually use some kind of\\nsmoothing.\\nIn particular, we’ll choose a pseudocount —k—and estimate the probability of seeing\\nthe ith word in a spam as:\\nPXiS=k+ number of spams containing wi/2k+ number of spams\\nSimilarly for PXi¬S. That is, when computing the spam probabilities for the ith\\nword, we assume we also saw k additional spams containing the word and k addi‐\\ntional spams not containing the word.\\nFor example, if “data” occurs in 0/98 spam documents, and if k is 1, we estimate\\nP“data” S as 1/100 = 0.01, which allows our classifier to still assign some nonzero\\nspam probability to messages that contain the word “data. ”\\nA More Sophisticated Spam Filter | 167\\nwww.it-ebooks.infoImplementation\\nNow we have all the pieces we need to build our classifier. First, let’s create a simple\\nfunction to tokenize messages  into distinct words. We’ll first convert each message to\\nlowercase; use re.findall()  to extract “words” consisting of letters, numbers, and\\napostrophes; and finally use set()  to get just the distinct words:\\ndef tokenize (message):\\n    message = message.lower()                       # convert to lowercase\\n    all_words  = re.findall(\"[a-z0-9\\']+\" , message)   # extract the words\\n    return set(all_words )                           # remove duplicates\\nOur second function will count the words in a labeled training set of messages. We’ll\\nhave it return a dictionary whose keys are words, and whose values are two-element\\nlists [spam_count, non_spam_count]  corresponding to how many times we saw that\\nword in both spam and nonspam messages:\\ndef count_words (training_set ):\\n    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\\n    counts = defaultdict (lambda: [0, 0])\\n    for message, is_spam in training_set :\\n        for word in tokenize (message):\\n            counts[word][0 if is_spam else 1] += 1\\n    return counts\\nOur next step is to turn these counts into estimated probabilities using the smoothing\\nwe described before. Our function will return a list of triplets containing each word,\\nthe probability of seeing that word in a spam message, and the probability of seeing\\nthat word in a nonspam message:\\ndef word_probabilities (counts, total_spams , total_non_spams , k=0.5):\\n    \"\"\"turn the word_counts into a list of triplets\\n    w, p(w | spam) and p(w | ~spam)\"\"\"\\n    return [(w,\\n             (spam + k) / (total_spams  + 2 * k),\\n             (non_spam  + k) / (total_non_spams  + 2 * k))\\n             for w, (spam, non_spam ) in counts.iteritems ()]\\nThe last piece is to use these word probabilities (and our Naive Bayes assumptions) to\\nassign probabilities to messages:\\ndef spam_probability (word_probs , message):\\n    message_words  = tokenize (message)\\n    log_prob_if_spam  = log_prob_if_not_spam  = 0.0\\n    # iterate through each word in our vocabulary\\n    for word, prob_if_spam , prob_if_not_spam  in word_probs :\\n        # if *word* appears in the message,\\n        # add the log probability of seeing it\\n        if word in message_words :\\n168 | Chapter 13: Naive Bayes\\nwww.it-ebooks.info            log_prob_if_spam  += math.log(prob_if_spam )\\n            log_prob_if_not_spam  += math.log(prob_if_not_spam )\\n        # if *word* doesn\\'t appear in the message\\n        # add the log probability of _not_ seeing it\\n        # which is log(1 - probability of seeing it)\\n        else:\\n            log_prob_if_spam  += math.log(1.0 - prob_if_spam )\\n            log_prob_if_not_spam  += math.log(1.0 - prob_if_not_spam )\\n    prob_if_spam  = math.exp(log_prob_if_spam )\\n    prob_if_not_spam  = math.exp(log_prob_if_not_spam )\\n    return prob_if_spam  / (prob_if_spam  + prob_if_not_spam )\\nWe can put this all together into our Naive Bayes Classifier:\\nclass NaiveBayesClassifier :\\n    def __init__ (self, k=0.5):\\n        self.k = k\\n        self.word_probs  = []\\n    def train(self, training_set ):\\n        # count spam and non-spam messages\\n        num_spams  = len([is_spam\\n                         for message, is_spam in training_set\\n                         if is_spam])\\n        num_non_spams  = len(training_set ) - num_spams\\n        # run training data through our \"pipeline\"\\n        word_counts  = count_words (training_set )\\n        self.word_probs  = word_probabilities (word_counts ,\\n                                             num_spams ,\\n                                             num_non_spams ,\\n                                             self.k)\\n    def classify (self, message):\\n        return spam_probability (self.word_probs , message)\\nTesting Our Model\\nA good (if somewhat old) data set is the SpamAssassin public corpus . We’ll look at\\nthe files prefixed with 20021010 . (On Windows, you might need a program like 7-Zip\\nto decompress and extract them.)\\nAfter extracting the data (to, say, C:\\\\spam ) you should have three folders: spam ,\\neasy_ham , and hard_ham . Each folder contains many emails, each contained in a sin‐\\ngle file. To keep things really  simple, we’ll just look at the subject lines of each email.\\nTesting Our Model | 169\\nwww.it-ebooks.infoHow do we identify the subject line? Looking through the files, they all seem to start\\nwith “Subject:” . So we’ll look for that:\\nimport glob, re\\n# modify the path with wherever you\\'ve put the files\\npath = r\"C:\\\\spam\\\\*\\\\*\"\\ndata = []\\n# glob.glob returns every filename that matches the wildcarded path\\nfor fn in glob.glob(path):\\n    is_spam = \"ham\" not in fn\\n    with open(fn,\\'r\\') as file:\\n        for line in file:\\n            if line.startswith (\"Subject:\" ):\\n                # remove the leading \"Subject: \" and keep what\\'s left\\n                subject = re.sub(r\"^Subject: \" , \"\", line).strip()\\n                data.append((subject, is_spam))\\nNow we can split the data into training data and test data, and then we’re ready to\\nbuild a classifier:\\nrandom.seed(0)      # just so you get the same answers as me\\ntrain_data , test_data  = split_data (data, 0.75)\\nclassifier  = NaiveBayesClassifier ()\\nclassifier .train(train_data )\\nAnd then we can check how our model does:\\n# triplets (subject, actual is_spam, predicted spam probability)\\nclassified  = [(subject, is_spam, classifier .classify (subject))\\n              for subject, is_spam in test_data ]\\n# assume that spam_probability > 0.5 corresponds to spam prediction\\n# and count the combinations of (actual is_spam, predicted is_spam)\\ncounts = Counter((is_spam, spam_probability  > 0.5)\\n                 for _, is_spam, spam_probability  in classified )\\nThis gives 101 true positives (spam classified as “spam”), 33 false positives (ham clas‐\\nsified as “spam”), 704 true negatives (ham classified as “ham”), and 38 false negatives\\n(spam classified as “ham”). This means our precision is 101 / (101 + 33) = 75%, and\\nour recall is 101 / (101 + 38) = 73%, which are not bad numbers for such a simple\\nmodel.\\nIt’s also interesting to look at the most misclassified:\\n# sort by spam_probability from smallest to largest\\nclassified .sort(key=lambda row: row[2])\\n# the highest predicted spam probabilities among the non-spams\\n170 | Chapter 13: Naive Bayes\\nwww.it-ebooks.infospammiest_hams  = filter(lambda row: not row[1], classified )[-5:]\\n# the lowest predicted spam probabilities among the actual spams\\nhammiest_spams  = filter(lambda row: row[1], classified )[:5]\\nThe two spammiest hams both have the words “needed” (77 times more likely to\\nappear in spam), “insurance” (30 times more likely to appear in spam), and “impor‐\\ntant” (10 times more likely to appear in spam).\\nThe hammiest spam is too short (“Re: girls”) to make much of a judgment, and the\\nsecond-hammiest is a credit card solicitation most of whose words weren’t in the\\ntraining set.\\nWe can similarly look at the spammiest words :\\ndef p_spam_given_word (word_prob ):\\n    \"\"\"uses bayes\\'s theorem to compute p(spam | message contains word)\"\"\"\\n    # word_prob is one of the triplets produced by word_probabilities\\n    word, prob_if_spam , prob_if_not_spam  = word_prob\\n    return prob_if_spam  / (prob_if_spam  + prob_if_not_spam )\\nwords = sorted(classifier .word_probs , key=p_spam_given_word )\\nspammiest_words  = words[-5:]\\nhammiest_words  = words[:5]\\nThe spammiest words are “money, ” “systemworks, ” “rates, ” “sale, ” and “year, ” all of\\nwhich seem related to trying to get people to buy things. And the hammiest words are\\n“spambayes, ” “users, ” “razor, ” “zzzzteana, ” and “sadev, ” most of which seem related to\\nspam prevention, oddly enough.\\nHow could we get better performance? One obvious way would be to get more data\\nto train on. There are a number of ways to improve the model as well. Here are some\\npossibilities that you might try:\\n•Look at the message content, not just the subject line. Y ou’ll have to be careful\\nhow you deal with the message headers.\\n•Our classifier takes into account every word that appears in the training set, even\\nwords that appear only once. Modify the classifier to accept an optional\\nmin_count  threshhold and ignore tokens that don’t appear at least that many\\ntimes.\\n•The tokenizer has no notion of similar words (e.g., “cheap” and “cheapest”). Mod‐\\nify the classifier to take an optional stemmer  function that converts words to\\nequivalence classes  of words. For example, a really simple stemmer function might\\nbe:\\ndef drop_final_s (word):\\n    return re.sub(\"s$\", \"\", word)\\nTesting Our Model | 171\\nwww.it-ebooks.infoCreating a good stemmer function is hard. People frequently use the Porter\\nStemmer .\\n•Although our features are all of the form “message contains word wi, ” there’s no\\nreason why this has to be the case. In our implementation, we could add extra\\nfeatures like “message contains a number” by creating phony tokens like con‐\\ntains:number  and modifying the tokenizer  to emit them when appropriate.\\nFor Further Exploration\\n•Paul Graham’s articles “ A Plan for Spam”  and “Better Bayesian Filtering”  (are\\ninteresting and) give more insight into the ideas behind building spam filters.\\n•scikit-learn  contains a BernoulliNB  model that implements the same Naive\\nBayes algorithm we implemented here, as well as other variations on the model.\\n172 | Chapter 13: Naive Bayes\\nwww.it-ebooks.infoCHAPTER 14\\nSimple Linear Regression\\nArt, like morality, consists in drawing the line somewhere.\\n—G. K. Chesterton\\nIn Chapter 5 , we used the correlation  function to measure the strength of the linear\\nrelationship between two variables. For most applications, knowing that such a linear\\nrelationship exists isn’t enough. We’ll want to be able to understand the nature of the\\nrelationship. This is where we’ll use simple linear regression.\\nThe Model\\nRecall that we were investigating the relationship between a DataSciencester user’s\\nnumber of friends and the amount of time he spent on the site each day.  Let’s assume\\nthat you’ve convinced yourself that having more friends causes  people to spend more\\ntime on the site, rather than one of the alternative explanations we discussed.\\nThe VP of Engagement asks you to build a model describing this relationship. Since\\nyou found a pretty strong linear relationship, a natural place to start is a linear model.\\nIn particular, you hypothesize that there are constants α (alpha) and β (beta) such\\nthat:\\nyi=βxi+α+εi\\nwhere yi is the number of minutes user i spends on the site daily, xi is the number of\\nfriends user i has, and εi is a (hopefully small) error term representing the fact that\\nthere are other factors not accounted for by this simple model.\\n173\\nwww.it-ebooks.infoAssuming we’ve determined such an alpha  and beta , then we make predictions sim‐\\nply with:\\ndef predict(alpha, beta, x_i):\\n    return beta * x_i + alpha\\nHow do we choose alpha  and beta ? Well, any choice of alpha  and beta  gives us a\\npredicted output for each input x_i. Since we know the actual output y_i we can\\ncompute the error for each pair:\\ndef error(alpha, beta, x_i, y_i):\\n    \"\"\"the error from predicting beta * x_i + alpha\\n    when the actual value is y_i\"\"\"\\n    return y_i - predict(alpha, beta, x_i)\\nWhat we’ d really like to know is the total error over the entire data set.  But we don’t\\nwant to just add the errors—if the prediction for x_1 is too high and the prediction\\nfor x_2 is too low, the errors may just cancel out.\\nSo instead we add up the squared  errors:\\ndef sum_of_squared_errors (alpha, beta, x, y):\\n    return sum(error(alpha, beta, x_i, y_i) ** 2\\n               for x_i, y_i in zip(x, y))\\nThe least squares solution  is to choose the alpha  and beta  that make\\nsum_of_squared_errors  as small as possible.\\nUsing calculus (or tedious algebra), the error-minimizing alpha and beta are given by:\\ndef least_squares_fit (x, y):\\n    \"\"\"given training values for x and y,\\n    find the least-squares values of alpha and beta\"\"\"\\n    beta = correlation (x, y) * standard_deviation (y) / standard_deviation (x)\\n    alpha = mean(y) - beta * mean(x)\\n    return alpha, beta\\nWithout going through the exact mathematics, let’s think about why this might be a\\nreasonable solution. The choice of alpha  simply says that when we see the average\\nvalue of the independent variable x, we predict the average value of the dependent\\nvariable y.\\nThe choice of beta  means that when the input value increases by standard_devia\\ntion(x) , the prediction increases by correlation(x, y) * standard_devia\\ntion(y) . In the case when x and y are perfectly correlated, a one standard deviation\\nincrease in x results in a one-standard-deviation-of- y increase in the prediction.\\nWhen they’re perfectly anticorrelated, the increase in x results in a decrease  in the\\nprediction. And when the correlation is zero, beta  is zero, which means that changes\\nin x don’t affect the prediction at all.\\nIt’s easy to apply this to the outlierless data from Chapter 5 :\\n174 | Chapter 14: Simple Linear Regression\\nwww.it-ebooks.infoalpha, beta = least_squares_fit (num_friends_good , daily_minutes_good )\\nThis gives values of alpha = 22.95 and beta = 0.903. So our model says that we expect\\na user with n friends to spend 22.95 + n * 0.903  minutes on the site each day. That\\nis, we predict that a user with no friends on DataSciencester would still spend about\\n23 minutes a day on the site. And for each additional friend, we expect a user to\\nspend almost a minute more on the site each day.\\nIn Figure 14-1 , we plot the prediction line to get a sense of how well the model fits the\\nobserved data.\\nFigure 14-1. Our simple linear model\\nOf course, we need a better way to figure out how well we’ve fit the data than staring\\nat the graph. A common measure is the coefficient  of determination  (or R-squared ),\\nwhich measures the fraction of the total variation in the dependent variable that is\\ncaptured by the model:\\ndef total_sum_of_squares (y):\\n    \"\"\"the total squared variation of y_i\\'s from their mean\"\"\"\\n    return sum(v ** 2 for v in de_mean(y))\\ndef r_squared (alpha, beta, x, y):\\nThe Model | 175\\nwww.it-ebooks.info    \"\"\"the fraction of variation in y captured by the model, which equals\\n    1 - the fraction of variation in y not captured by the model\"\"\"\\n    return 1.0 - (sum_of_squared_errors (alpha, beta, x, y) /\\n                  total_sum_of_squares (y))\\nr_squared (alpha, beta, num_friends_good , daily_minutes_good )      # 0.329\\nNow, we chose the alpha  and beta  that minimized the sum of the squared prediction\\nerrors. One linear model we could have chosen is “always predict mean(y) ” (corre‐\\nsponding to alpha = mean(y)  and beta = 0 ), whose sum of squared errors exactly\\nequals its total sum of squares. This means an R-squared of zero, which indicates a\\nmodel that (obviously, in this case) performs no better than just predicting the mean.\\nClearly, the least squares model must be at least as good as that one, which means that\\nthe sum of the squared errors is at most  the total sum of squares, which means that\\nthe R-squared must be at least zero. And the sum of squared errors must be at least 0,\\nwhich means that the R-squared can be at most 1.\\nThe higher the number, the better our model fits the data. Here we calculate an R-\\nsquared of 0.329, which tells us that our model is only sort of okay at fitting the data,\\nand that clearly there are other factors at play.\\nUsing Gradient Descent\\nIf we write theta = [alpha, beta] , then we can also solve this using gradient\\ndescent:\\ndef squared_error (x_i, y_i, theta):\\n    alpha, beta = theta\\n    return error(alpha, beta, x_i, y_i) ** 2\\ndef squared_error_gradient (x_i, y_i, theta):\\n    alpha, beta = theta\\n    return [-2 * error(alpha, beta, x_i, y_i),       # alpha partial derivative\\n            -2 * error(alpha, beta, x_i, y_i) * x_i] # beta partial derivative\\n# choose random value to start\\nrandom.seed(0)\\ntheta = [random.random(), random.random()]\\nalpha, beta = minimize_stochastic (squared_error ,\\n                                  squared_error_gradient ,\\n                                  num_friends_good ,\\n                                  daily_minutes_good ,\\n                                  theta,\\n                                  0.0001)\\nprint alpha, beta\\nUsing the same data we get alpha = 22.93, beta = 0.905, which are very close to the\\nexact answers.\\n176 | Chapter 14: Simple Linear Regression\\nwww.it-ebooks.infoMaximum Likelihood Estimation\\nWhy choose least squares? One justification involves maximum likelihood estimation .\\nImagine that we have a sample of data v1, ...,vn that comes from a distribution that\\ndepends on some unknown parameter θ:\\npv1, ...,vnθ\\nIf we didn’t know theta, we could turn around and think of this quantity as the likeli‐\\nhood  of θ given the sample:\\nLθv1, ...,vn\\nUnder this approach, the most likely θ is the value that maximizes this likelihood\\nfunction; that is, the value that makes the observed data the most probable. In the\\ncase of a continuous distribution, in which we have a probability distribution func‐\\ntion rather than a probability mass function, we can do the same thing.\\nBack to regression. One assumption that’s often made about the simple regression\\nmodel is that the regression errors are normally distributed with mean 0 and some\\n(known) standard deviation σ. If that’s the case, then the likelihood based on seeing a\\npair (x_i, y_i)  is:\\nLα,βxi,yi,σ=1\\n2πσexp −yi−α−βxi2/2σ2\\nThe likelihood based on the entire data set is the product of the individual likeli‐\\nhoods, which is largest precisely when alpha  and beta  are chosen to minimize the\\nsum of squared errors. That is, in this case (and with these assumptions), minimizing\\nthe sum of squared errors is equivalent to maximizing the likelihood of the observed\\ndata.\\nFor Further Exploration\\nContinue reading about multiple regression in Chapter 15 !\\nMaximum Likelihood Estimation | 177\\nwww.it-ebooks.infowww.it-ebooks.infoCHAPTER 15\\nMultiple Regression\\nI don’t look at a problem and put variables in there that don’t affect  it.\\n—Bill Parcells\\nAlthough the VP is pretty impressed with your predictive model, she thinks you can\\ndo better. To that end, you’ve collected additional data: for each of your users, you\\nknow how many hours he works each day, and whether he has a PhD. Y ou’ d like to\\nuse this additional data to improve your model.\\nAccordingly, you hypothesize a linear model with more independent variables:\\nminutes = α+β1friends + β2work hours + β3phd + ε\\nObviously, whether a user has a PhD is not a number, but—as we mentioned in\\nChapter 11 —we can introduce a dummy variable  that equals 1 for users with PhDs\\nand 0 for users without, after which it’s just as numeric as the other variables.\\nThe Model\\nRecall that in Chapter 14  we fit a model of the form:\\nyi=α+βxi+εi\\nNow imagine that each input xi is not a single number but rather a vector of k num‐\\nbers xi1, ...,xik. The multiple regression model assumes that:\\nyi=α+β1xi1+ . . . + βkxik+εi\\n179\\nwww.it-ebooks.infoIn multiple regression the vector of parameters is usually called β. We’ll want this to\\ninclude the constant term as well, which we can achieve by adding a column of ones\\nto our data:\\nbeta = [alpha, beta_1, ..., beta_k]\\nand:\\nx_i = [1, x_i1, ..., x_ik]\\nThen our model is just:\\ndef predict(x_i, beta):\\n    \"\"\"assumes that the first element of each x_i is 1\"\"\"\\n    return dot(x_i, beta)\\nIn this particular case, our independent variable x will be a list of vectors, each of\\nwhich looks like this:\\n[1,    # constant term\\n 49,   # number of friends\\n 4,    # work hours per day\\n 0]    # doesn\\'t have PhD\\nFurther Assumptions of the Least Squares Model\\nThere are a couple of further assumptions that are required for this model (and our\\nsolution) to make sense.\\nThe first is that the columns of x are linearly independent —that there’s no way to write\\nany one as a weighted sum of some of the others. If this assumption fails, it’s impossi‐\\nble to estimate beta . To see this in an extreme case, imagine we had an extra field\\nnum_acquaintances  in our data that for every user was exactly equal to num_friends .\\nThen, starting with any beta , if we add any amount to the num_friends  coefficient\\nand subtract that same amount from the num_acquaintances  coefficient, the model’s\\npredictions will remain unchanged. Which means that there’s no way to find the coef‐\\nficient for num_friends . (Usually violations of this assumption won’t be so obvious.)\\nThe second important assumption is that the columns of x are all uncorrelated with\\nthe errors ε. If this fails to be the case, our estimates of beta  will be systematically\\nwrong.\\nFor instance, in Chapter 14 , we built a model that predicted that each additional\\nfriend was associated with an extra 0.90 daily minutes on the site.\\nImagine that it’s also the case that:\\n•People who work more hours spend less time on the site.\\n180 | Chapter 15: Multiple Regression\\nwww.it-ebooks.info•People with more friends tend to work more hours.\\nThat is, imagine that the “actual” model is:\\nminutes = α+β1friends + β2work hours + ε\\nand that work hours and friends are positively correlated. In that case, when we mini‐\\nmize the errors of the single variable model:\\nminutes = α+β1friends + ε\\nwe will underestimate β1.\\nThink about what would happen if we made predictions using the single variable\\nmodel with the “actual” value of β1. (That is, the value that arises from minimizing\\nthe errors of what we called the “actual” model.) The predictions would tend to be too\\nsmall for users who work many hours and too large for users who work few hours,\\nbecause β2> 0 and we “forgot” to include it. Because work hours is positively correla‐\\nted with number of friends, this means the predictions tend to be too small for users\\nwith many friends and too large for users with few friends.\\nThe result of this is that we can reduce the errors (in the single-variable model) by\\ndecreasing our estimate of β1, which means that the error-minimizing β1 is smaller\\nthan the “actual” value. That is, in this case the single-variable least squares solution is\\nbiased to underestimate β1. And, in general, whenever the independent variables are\\ncorrelated with the errors like this, our least squares solution will give us a biased esti‐\\nmate of β.\\nFitting the Model\\nAs we did in the simple linear model, we’ll choose beta  to minimize the sum of\\nsquared errors. Finding an exact solution is not simple to do by hand, which means\\nwe’ll need to use gradient descent. We’ll start by creating an error function to mini‐\\nmize. For stochastic gradient descent, we’ll just want the squared error corresponding\\nto a single prediction:\\ndef error(x_i, y_i, beta):\\n    return y_i - predict(x_i, beta)\\ndef squared_error (x_i, y_i, beta):\\n    return error(x_i, y_i, beta) ** 2\\nIf you know calculus, you can compute:\\nFitting the Model | 181\\nwww.it-ebooks.infodef squared_error_gradient (x_i, y_i, beta):\\n    \"\"\"the gradient (with respect to beta)\\n    corresponding to the ith squared error term\"\"\"\\n    return [-2 * x_ij * error(x_i, y_i, beta)\\n            for x_ij in x_i]\\nOtherwise, you’ll need to take my word for it.\\nAt this point, we’re ready to find the optimal beta using stochastic gradient descent:\\ndef estimate_beta (x, y):\\n    beta_initial  = [random.random() for x_i in x[0]]\\n    return minimize_stochastic (squared_error ,\\n                               squared_error_gradient ,\\n                               x, y,\\n                               beta_initial ,\\n                               0.001)\\nrandom.seed(0)\\nbeta = estimate_beta (x, daily_minutes_good ) # [30.63, 0.972, -1.868, 0.911]\\nThis means our model looks like:\\nminutes = 30 . 63 + 0 . 972 friends − 1 . 868 work hours + 0 . 911 phd\\nInterpreting the Model\\nY ou should think of the coefficients of the model as representing all-else-being-equal\\nestimates of the impacts of each factor. All else being equal, each additional friend\\ncorresponds to an extra minute spent on the site each day. All else being equal, each\\nadditional hour in a user’s workday corresponds to about two fewer minutes spent on\\nthe site each day. All else being equal, having a PhD is associated with spending an\\nextra minute on the site each day.\\nWhat this doesn’t (directly) tell us is anything about the interactions among the vari‐\\nables. It’s possible that the effect of work hours is different for people with many\\nfriends than it is for people with few friends. This model doesn’t capture that. One\\nway to handle this case is to introduce a new variable that is the product  of “friends”\\nand “work hours. ” This effectively allows the “work hours” coefficient to increase (or\\ndecrease) as the number of friends increases.\\nOr it’s possible that the more friends you have, the more time you spend on the site\\nup to a point , after which further friends cause you to spend less time on the site.\\n(Perhaps with too many friends the experience is just too overwhelming?) We could\\ntry to capture this in our model by adding another variable that’s the square  of the\\nnumber of friends.\\n182 | Chapter 15: Multiple Regression\\nwww.it-ebooks.infoOnce we start adding variables, we need to worry about whether their coefficients\\n“matter. ” There are no limits to the numbers of products, logs, squares, and higher\\npowers we could add.\\nGoodness of Fit\\nAgain we can look at the R-squared, which has now increased to 0.68:\\ndef multiple_r_squared (x, y, beta):\\n    sum_of_squared_errors  = sum(error(x_i, y_i, beta) ** 2\\n                                for x_i, y_i in zip(x, y))\\n    return 1.0 - sum_of_squared_errors  / total_sum_of_squares (y)\\nKeep in mind, however, that adding new variables to a regression will necessarily\\nincrease the R-squared. After all, the simple regression model is just the special case\\nof the multiple regression model where the coefficients on “work hours” and “PhD”\\nboth equal 0. The optimal multiple regression model will necessarily have an error at\\nleast as small as that one.\\nBecause of this, in a multiple regression, we also need to look at the standard errors  of\\nthe coefficients, which measure how certain we are about our estimates of each βi.\\nThe regression as a whole may fit our data very well, but if some of the independent\\nvariables are correlated (or irrelevant), their coefficients might not mean  much.\\nThe typical approach to measuring these errors starts with another assumption—that\\nthe errors εi are independent normal random variables with mean 0 and some shared\\n(unknown) standard deviation σ. In that case, we (or, more likely, our statistical soft‐\\nware) can use some linear algebra to find the standard error of each coefficient. The\\nlarger it is, the less sure our model is about that coefficient. Unfortunately, we’re not\\nset up to do that kind of linear algebra from scratch.\\nDigression: The Bootstrap\\nImagine we have a sample of n data points, generated by some (unknown to us) dis‐\\ntribution:\\ndata = get_sample (num_points =n)\\nIn Chapter 5 , we wrote a function to compute the median  of the observed data, which\\nwe can use as an estimate of the median of the distribution itself.\\nBut how confident can we be about our estimate? If all the data in the sample are very\\nclose to 100, then it seems likely that the actual median is close to 100. If approxi‐\\nmately half the data in the sample is close to 0 and the other half is close to 200, then\\nwe can’t be nearly as certain about the median.\\nGoodness of Fit | 183\\nwww.it-ebooks.infoIf we could repeatedly get new samples, we could compute the median of each and\\nlook at the distribution of those medians. Usually we can’t. What we can do instead is\\nbootstrap  new data sets  by choosing n data points with replacement  from our data and\\nthen compute the medians of those synthetic data sets:\\ndef bootstrap_sample (data):\\n    \"\"\"randomly samples len(data) elements with replacement\"\"\"\\n    return [random.choice(data) for _ in data]\\ndef bootstrap_statistic (data, stats_fn , num_samples ):\\n    \"\"\"evaluates stats_fn on num_samples bootstrap samples from data\"\"\"\\n    return [stats_fn (bootstrap_sample (data))\\n            for _ in range(num_samples )]\\nFor example, consider the two following data sets:\\n# 101 points all very close to 100\\nclose_to_100  = [99.5 + random.random() for _ in range(101)]\\n# 101 points, 50 of them near 0, 50 of them near 200\\nfar_from_100  = ([99.5 + random.random()] +\\n                [random.random() for _ in range(50)] +\\n                [200 + random.random() for _ in range(50)])\\nIf you compute the median  of each, both will be very close to 100. However, if you\\nlook at:\\nbootstrap_statistic (close_to_100 , median, 100)\\nyou will mostly see numbers really close to 100. Whereas if you look at:\\nbootstrap_statistic (far_from_100 , median, 100)\\nyou will see a lot of numbers close to 0 and a lot of numbers close to 200.\\nThe standard_deviation  of the first set of medians is close to 0, while the stan\\ndard_deviation  of the second set of medians is close to 100. (This extreme a case\\nwould be pretty easy to figure out by manually inspecting the data, but in general that\\nwon’t be true.)\\nStandard Errors of Regression Coefficients\\nWe can take the same approach to estimating the standard errors of our regression\\ncoefficients.  We repeatedly take a bootstrap_sample  of our data and estimate beta\\nbased on that sample. If the coefficient corresponding to one of the independent vari‐\\nables (say num_friends ) doesn’t vary much across samples, then we can be confident\\nthat our estimate is relatively tight. If the coefficient varies greatly across samples,\\nthen we can’t be at all confident in our estimate.\\n184 | Chapter 15: Multiple Regression\\nwww.it-ebooks.infoThe only subtlety is that, before sampling, we’ll need to zip our x data and y data to\\nmake sure that corresponding values of the independent and dependent variables are\\nsampled together. This means that bootstrap_sample  will return a list of pairs (x_i,\\ny_i) , which we’ll need to reassemble into an x_sample  and a y_sample :\\ndef estimate_sample_beta (sample):\\n    \"\"\"sample is a list of pairs (x_i, y_i)\"\"\"\\n    x_sample , y_sample  = zip(*sample) # magic unzipping trick\\n    return estimate_beta (x_sample , y_sample )\\nrandom.seed(0) # so that you get the same results as me\\nbootstrap_betas  = bootstrap_statistic (zip(x, daily_minutes_good ),\\n                                      estimate_sample_beta ,\\n                                      100)\\nAfter which we can estimate the standard deviation of each coefficient:\\nbootstrap_standard_errors  = [\\n    standard_deviation ([beta[i] for beta in bootstrap_betas ])\\n    for i in range(4)]\\n# [1.174,    # constant term, actual error = 1.19\\n#  0.079,    # num_friends,   actual error = 0.080\\n#  0.131,    # unemployed,    actual error = 0.127\\n#  0.990]    # phd,           actual error = 0.998\\nWe can use these to test hypotheses such as “does βi equal zero?” Under the null\\nhypothesis βi= 0 (and with our other assumptions about the distribution of εi ) the\\nstatistic:\\ntj=βj/σj\\nwhich is our estimate of βj divided by our estimate of its standard error, follows a\\nStudent’s t-distribution  with “ n−k degrees of freedom. ”\\nIf we had a students_t_cdf  function, we could compute p-values for each least-\\nsquares coefficient to indicate how likely we would be to observe such a value if the\\nactual coefficient were zero. Unfortunately, we don’t have such a function. (Although\\nwe would if we weren’t working from scratch.)\\nHowever, as the degrees of freedom get large, the t-distribution gets closer and closer\\nto a standard normal. In a situation like this, where n is much larger than k, we can\\nuse normal_cdf  and still feel good about ourselves:\\ndef p_value(beta_hat_j , sigma_hat_j ):\\n    if beta_hat_j  > 0:\\n        # if the coefficient is positive, we need to compute twice the\\n        # probability of seeing an even *larger* value\\nStandard Errors of Regression Coefficients  | 185\\nwww.it-ebooks.info        return 2 * (1 - normal_cdf (beta_hat_j  / sigma_hat_j ))\\n    else:\\n        # otherwise twice the probability of seeing a *smaller* value\\n        return 2 * normal_cdf (beta_hat_j  / sigma_hat_j )\\np_value(30.63, 1.174)    # ~0   (constant term)\\np_value(0.972, 0.079)    # ~0   (num_friends)\\np_value(-1.868, 0.131)   # ~0   (work_hours)\\np_value(0.911, 0.990)    # 0.36 (phd)\\n(In a situation not like this, we would probably be using statistical software that\\nknows how to compute the t-distribution, as well as how to compute the exact stan‐\\ndard errors.)\\nWhile most of the coefficients have very small p-values (suggesting that they are\\nindeed nonzero), the coefficient for “PhD” is not “significantly” different from zero,\\nwhich makes it likely that the coefficient for “PhD” is random rather than meaning‐\\nful.\\nIn more elaborate regression scenarios, you sometimes want to test more elaborate\\nhypotheses about the data, such as “at least one of the βj is non-zero” or “ β1 equals β2\\nand β3 equals β4, ” which you can do with an F-test , which, alas, falls outside the scope\\nof this book.\\nRegularization\\nIn practice, you’ d often like to apply linear regression to data sets with large numbers\\nof variables. This creates a couple of extra wrinkles. First, the more variables you use,\\nthe more likely you are to overfit your model to the training set. And second, the\\nmore nonzero coefficients you have, the harder it is to make sense of them. If the goal\\nis to explain  some phenomenon, a sparse model with three factors might be more\\nuseful than a slightly better model with hundreds.\\nRegularization  is an approach in which we add to the error term a penalty that gets\\nlarger as beta  gets larger. We then minimize the combined error and penalty. The\\nmore importance we place on the penalty term, the more we discourage large coeffi‐\\ncients.\\nFor example, in ridge regression , we add a penalty proportional to the sum of the\\nsquares of the beta_i . (Except that typically we don’t penalize beta_0 , the constant\\nterm.)\\n# alpha is a *hyperparameter* controlling how harsh the penalty is\\n# sometimes it\\'s called \"lambda\" but that already means something in Python\\ndef ridge_penalty (beta, alpha):\\n  return alpha * dot(beta[1:], beta[1:])\\ndef squared_error_ridge (x_i, y_i, beta, alpha):\\n186 | Chapter 15: Multiple Regression\\nwww.it-ebooks.info    \"\"\"estimate error plus ridge penalty on beta\"\"\"\\n    return error(x_i, y_i, beta) ** 2 + ridge_penalty (beta, alpha)\\nwhich you can then plug into gradient descent in the usual way:\\ndef ridge_penalty_gradient (beta, alpha):\\n    \"\"\"gradient of just the ridge penalty\"\"\"\\n    return [0] + [2 * alpha * beta_j for beta_j in beta[1:]]\\ndef squared_error_ridge_gradient (x_i, y_i, beta, alpha):\\n    \"\"\"the gradient corresponding to the ith squared error term\\n    including the ridge penalty\"\"\"\\n    return vector_add (squared_error_gradient (x_i, y_i, beta),\\n                      ridge_penalty_gradient (beta, alpha))\\ndef estimate_beta_ridge (x, y, alpha):\\n    \"\"\"use gradient descent to fit a ridge regression\\n    with penalty alpha\"\"\"\\n    beta_initial  = [random.random() for x_i in x[0]]\\n    return minimize_stochastic (partial(squared_error_ridge , alpha=alpha),\\n                               partial(squared_error_ridge_gradient ,\\n                                       alpha=alpha),\\n                               x, y,\\n                               beta_initial ,\\n                               0.001)\\nWith alpha  set to zero, there’s no penalty at all and we get the same results as before:\\nrandom.seed(0)\\nbeta_0 = estimate_beta_ridge (x, daily_minutes_good , alpha=0.0)\\n# [30.6, 0.97, -1.87, 0.91]\\ndot(beta_0[1:], beta_0[1:]) # 5.26\\nmultiple_r_squared (x, daily_minutes_good , beta_0) # 0.680\\nAs we increase alpha , the goodness of fit gets worse, but the size of beta  gets smaller:\\nbeta_0_01  = estimate_beta_ridge (x, daily_minutes_good , alpha=0.01)\\n# [30.6, 0.97, -1.86, 0.89]\\ndot(beta_0_01 [1:], beta_0_01 [1:])  # 5.19\\nmultiple_r_squared (x, daily_minutes_good , beta_0_01 )  # 0.680\\nbeta_0_1  = estimate_beta_ridge (x, daily_minutes_good , alpha=0.1)\\n# [30.8, 0.95, -1.84, 0.54]\\ndot(beta_0_1 [1:], beta_0_1 [1:])  # 4.60\\nmultiple_r_squared (x, daily_minutes_good , beta_0_1 )  # 0.680\\nbeta_1 = estimate_beta_ridge (x, daily_minutes_good , alpha=1)\\n# [30.7, 0.90, -1.69, 0.085]\\ndot(beta_1[1:], beta_1[1:])  # 3.69\\nmultiple_r_squared (x, daily_minutes_good , beta_1)  # 0.676\\nbeta_10 = estimate_beta_ridge (x, daily_minutes_good , alpha=10)\\n# [28.3, 0.72, -0.91, -0.017]\\nRegularization | 187\\nwww.it-ebooks.infodot(beta_10[1:], beta_10[1:])  # 1.36\\nmultiple_r_squared (x, daily_minutes_good , beta_10)  # 0.573\\nIn particular, the coefficient on “PhD” vanishes as we increase the penalty, which\\naccords with our previous result that it wasn’t significantly different from zero.\\nUsually you’ d want to rescale  your data before using this\\napproach. After all, if you changed years of experience to centuries\\nof experience, its least squares coefficient would increase by a fac‐\\ntor of 100 and suddenly get penalized much more, even though it’s\\nthe same model.\\nAnother approach is lasso  regression, which uses the penalty:\\ndef lasso_penalty (beta, alpha):\\n    return alpha * sum(abs(beta_i) for beta_i in beta[1:])\\nWhereas the ridge penalty shrank the coefficients overall, the lasso penalty tends to\\nforce coefficients to be zero, which makes it good for learning sparse models.\\nUnfortunately, it’s not amenable to gradient descent, which means that we won’t be\\nable to solve it from scratch.\\nFor Further Exploration\\n•Regression has a rich and expansive theory behind it. This is another place where\\nyou should consider reading a textbook or at least a lot of Wikipedia articles.\\n•scikit-learn has a linear_model module  that provides a LinearRegression  model\\nsimilar to ours, as well as Ridge  regression, Lasso  regression, and other types of\\nregularization too.\\n•Statsmodels  is another Python module that contains (among other things) linear\\nregression models.\\n188 | Chapter 15: Multiple Regression\\nwww.it-ebooks.infoCHAPTER 16\\nLogistic Regression\\nA lot of people say there’s a fine line between genius and insanity. I don’t think there’s a fine\\nline, I actually think there’s a yawning gulf.\\n—Bill Bailey\\nIn Chapter 1 , we briefly looked at the problem of trying to predict which DataScien‐\\ncester users paid for premium accounts. Here we’ll revisit that problem.\\nThe Problem\\nWe have an anonymized data set of  about 200 users, containing each user’s salary, her\\nyears of experience as a data scientist, and whether she paid for a premium account\\n(Figure 16-1 ). As is usual with categorical variables, we represent the dependent vari‐\\nable as either 0 (no premium account) or 1 (premium account).\\nAs usual, our data is in a matrix where each row is a list [experience, salary,\\npaid_account] . Let’s turn it into the format we need:\\nx = [[1] + row[:2] for row in data]  # each element is [1, experience, salary]\\ny = [row[2] for row in data]         # each element is paid_account\\nAn obvious first attempt is to use linear regression and find the best model:\\npaid account = β0+β1experience + β2salary + ε\\n189\\nwww.it-ebooks.infoFigure 16-1. Paid and unpaid users\\nAnd certainly, there’s nothing preventing us from modeling the problem this way.\\nThe results are shown in Figure 16-2 :\\nrescaled_x  = rescale(x)\\nbeta = estimate_beta (rescaled_x , y)  # [0.26, 0.43, -0.43]\\npredictions  = [predict(x_i, beta) for x_i in rescaled_x ]\\nplt.scatter(predictions , y)\\nplt.xlabel(\"predicted\" )\\nplt.ylabel(\"actual\" )\\nplt.show()\\n190 | Chapter 16: Logistic Regression\\nwww.it-ebooks.infoFigure 16-2. Using linear regression to predict premium accounts\\nBut this approach leads to a couple of immediate problems:\\n•We’ d like for our predicted outputs to be 0 or 1, to indicate class membership. It’s\\nfine if they’re between 0 and 1, since we can interpret these as probabilities—an\\noutput of 0.25 could mean 25% chance of being a paid member. But the outputs\\nof the linear model can be huge positive numbers or even negative numbers,\\nwhich it’s not clear how to interpret. Indeed, here a lot of our predictions were\\nnegative.\\n•The linear regression model assumed that the errors were uncorrelated with the\\ncolumns of x. But here, the regression coefficent for experience  is 0.43, indicat‐\\ning that more experience leads to a greater likelihood of a premium account. This\\nmeans that our model outputs very large values for people with lots of experi‐\\nence. But we know that the actual values must be at most 1, which means that\\nnecessarily very large outputs (and therefore very large values of experience )\\ncorrespond to very large negative values of the error term. Because this is the\\ncase, our estimate of beta  is biased.\\nThe Problem | 191\\nwww.it-ebooks.infoWhat we’ d like instead is for large positive values of dot(x_i, beta)  to correspond to\\nprobabilities close to 1, and for large negative values to correspond to probabilities\\nclose to 0. We can accomplish this by applying another function to the result.\\nThe Logistic Function\\nIn the case of logistic regression, we use the logistic function , pictured in Figure 16-3 :\\ndef logistic (x):\\n    return 1.0 / (1 + math.exp(-x))\\nFigure 16-3. The logistic function\\nAs its input gets large and positive, it gets closer and closer to 1. As its input gets large\\nand negative, it gets closer and closer to 0. Additionally, it has the convenient prop‐\\nerty that its derivative is given by:\\ndef logistic_prime (x):\\n    return logistic (x) * (1 - logistic (x))\\nwhich we’ll make use of in a bit. We’ll use this to fit a model:\\nyi=fxiβ+εi\\n192 | Chapter 16: Logistic Regression\\nwww.it-ebooks.infowhere f is the logistic  function.\\nRecall that for linear regression we fit the model by minimizing the sum of squared\\nerrors, which ended up choosing the β that maximized the likelihood of the data.\\nHere the two aren’t equivalent, so we’ll use gradient descent to maximize the likeli‐\\nhood directly. This means we need to calculate the likelihood function and its gradi‐\\nent.\\nGiven some β, our model says that each yi should equal 1 with probability fxiβ and\\n0 with probability 1 −fxiβ.\\nIn particular, the pdf for yi can be written as:\\npyixi,β=fxiβyi1 −fxiβ1 −yi\\nsince if yi is 0, this equals:\\n1 −fxiβ\\nand if yi is 1, it equals:\\nfxiβ\\nIt turns out that it’s actually simpler to maximize the log likelihood :\\nlogLβxi,yi=yilogfxiβ+1 −yilog 1 −fxiβ\\nBecause log is strictly increasing function, any beta  that maximizes the log likelihood\\nalso maximizes the likelihood, and vice versa.\\ndef logistic_log_likelihood_i (x_i, y_i, beta):\\n    if y_i == 1:\\n        return math.log(logistic (dot(x_i, beta)))\\n    else:\\n        return math.log(1 - logistic (dot(x_i, beta)))\\nIf we assume different data points are independent from one another, the overall like‐\\nlihood is just the product of the individual likelihoods. Which means the overall log\\nlikelihood is the sum of the individual log likelihoods:\\ndef logistic_log_likelihood (x, y, beta):\\n    return sum(logistic_log_likelihood_i (x_i, y_i, beta)\\n               for x_i, y_i in zip(x, y))\\nThe Logistic Function | 193\\nwww.it-ebooks.infoA little bit of calculus gives us the gradient:\\ndef logistic_log_partial_ij (x_i, y_i, beta, j):\\n    \"\"\"here i is the index of the data point,\\n    j the index of the derivative\"\"\"\\n    return (y_i - logistic (dot(x_i, beta))) * x_i[j]\\ndef logistic_log_gradient_i (x_i, y_i, beta):\\n    \"\"\"the gradient of the log likelihood\\n    corresponding to the ith data point\"\"\"\\n    return [logistic_log_partial_ij (x_i, y_i, beta, j)\\n            for j, _ in enumerate (beta)]\\ndef logistic_log_gradient (x, y, beta):\\n    return reduce(vector_add ,\\n                  [logistic_log_gradient_i (x_i, y_i, beta)\\n                   for x_i, y_i in zip(x,y)])\\nat which point we have all the pieces we need.\\nApplying the Model\\nWe’ll want to split our data into a training set and a test set:\\nrandom.seed(0)\\nx_train, x_test, y_train, y_test = train_test_split (rescaled_x , y, 0.33)\\n# want to maximize log likelihood on the training data\\nfn = partial(logistic_log_likelihood , x_train, y_train)\\ngradient_fn  = partial(logistic_log_gradient , x_train, y_train)\\n# pick a random starting point\\nbeta_0 = [random.random() for _ in range(3)]\\n# and maximize using gradient descent\\nbeta_hat  = maximize_batch (fn, gradient_fn , beta_0)\\nAlternatively, you could use stochastic gradient descent:\\nbeta_hat  = maximize_stochastic (logistic_log_likelihood_i ,\\n                               logistic_log_gradient_i ,\\n                               x_train, y_train, beta_0)\\nEither way we find approximately:\\nbeta_hat  = [-1.90, 4.05, -3.87]\\nThese are coefficients for the rescale d data, but we can transform them back to the\\noriginal data as well:\\nbeta_hat_unscaled  = [7.61, 1.42, -0.000249 ]\\n194 | Chapter 16: Logistic Regression\\nwww.it-ebooks.infoUnfortunately, these are not as easy to interpret as linear regression coefficients. All\\nelse being equal, an extra year of experience adds 1.42 to the input of logistic . All\\nelse being equal, an extra $10,000 of salary subtracts 2.49 from the input of logistic .\\nThe impact on the output, however, depends on the other inputs as well. If dot(beta,\\nx_i)  is already large (corresponding to a probability close to 1), increasing it even by\\na lot cannot affect the probability very much. If it’s close to 0, increasing it just a little\\nmight increase the probability quite a bit.\\nWhat we can say is that—all else being equal—people with more experience are more\\nlikely to pay for accounts. And that—all else being equal—people with higher salaries\\nare less likely to pay for accounts. (This was also somewhat apparent when we plotted\\nthe data.)\\nGoodness of Fit\\nWe haven’t yet used the test data that we held out.  Let’s see what happens if we predict\\npaid account  whenever the probability exceeds 0.5:\\ntrue_positives  = false_positives  = true_negatives  = false_negatives  = 0\\nfor x_i, y_i in zip(x_test, y_test):\\n    predict = logistic (dot(beta_hat , x_i))\\n    if y_i == 1 and predict >= 0.5:  # TP: paid and we predict paid\\n        true_positives  += 1\\n    elif y_i == 1:                   # FN: paid and we predict unpaid\\n        false_negatives  += 1\\n    elif predict >= 0.5:             # FP: unpaid and we predict paid\\n        false_positives  += 1\\n    else:                            # TN: unpaid and we predict unpaid\\n        true_negatives  += 1\\nprecision  = true_positives  / (true_positives  + false_positives )\\nrecall = true_positives  / (true_positives  + false_negatives )\\nThis gives a precision of 93% (“when we predict paid account  we’re right 93% of the\\ntime”) and a recall of 82% (“when a user has a paid account we predict paid account\\n82% of the time”), both of which are pretty respectable numbers.\\nWe can also plot the predictions versus the actuals ( Figure 16-4 ), which also shows\\nthat the model performs well:\\npredictions  = [logistic (dot(beta_hat , x_i)) for x_i in x_test]\\nplt.scatter(predictions , y_test)\\nplt.xlabel(\"predicted probability\" )\\nplt.ylabel(\"actual outcome\" )\\nplt.title(\"Logistic Regression Predicted vs. Actual\" )\\nplt.show()\\nGoodness of Fit | 195\\nwww.it-ebooks.infoFigure 16-4. Logistic regression predicted versus actual\\nSupport Vector Machines\\nThe set of points where dot(beta_hat, x_i)  equals 0 is the boundary between our\\nclasses. We can plot this to see exactly what our model is doing ( Figure 16-5 ).\\nThis boundary is a hyperplane  that splits the parameter space into two half-spaces\\ncorresponding to predict paid  and predict unpaid . We found it as a side-effect of find‐\\ning the most likely logistic model.\\nAn alternative approach to classification is to just look for the hyperplane that “best”\\nseparates the classes in the training data. This is the idea behind the support vector\\nmachine , which finds the hyperplane that maximizes the distance to the nearest point\\nin each class ( Figure 16-6 ).\\n196 | Chapter 16: Logistic Regression\\nwww.it-ebooks.infoFigure 16-5. Paid and unpaid users with decision boundary\\nFinding such a hyperplane is an optimization problem that involves techniques that\\nare too advanced for us. A different problem is that a separating hyperplane might\\nnot exist at all. In our “who pays?” data set there simply is no line that perfectly sepa‐\\nrates the paid users from the unpaid users.\\nWe can (sometimes) get around this by transforming the data into a higher-\\ndimensional space. For example, consider the simple one-dimensional data set shown\\nin Figure 16-7 .\\nSupport Vector Machines | 197\\nwww.it-ebooks.infoFigure 16-6. A separating hyperplane\\nIt’s clear that there’s no hyperplane that separates the positive examples from the neg‐\\native ones. However, look at what happens when we map this data set to two dimen‐\\nsions by sending the point x to (x, x**2) . Suddenly it’s possible to find a hyperplane\\nthat splits the data ( Figure 16-8 ).\\nThis is usually called the kernel trick  because rather than actually mapping the points\\ninto the higher-dimensional space (which could be expensive if there are a lot of\\npoints and the mapping is complicated), we can use a “kernel” function to compute\\ndot products in the higher-dimensional space and use those to find a hyperplane.\\n198 | Chapter 16: Logistic Regression\\nwww.it-ebooks.infoFigure 16-7. A nonseparable one-dimensional data set\\nIt’s hard (and probably not a good idea) to use support vector machines without rely‐\\ning on specialized optimization software written by people with the appropriate\\nexpertise, so we’ll have to leave our treatment here.\\nSupport Vector Machines | 199\\nwww.it-ebooks.infoFigure 16-8. Data set becomes separable in higher dimensions\\nFor Further Investigation\\n•scikit-learn has modules for both Logistic Regression  and Support Vector\\nMachines .\\n•libsvm  is the support vector machine implementation that scikit-learn is using\\nbehind the scenes. Its website has a variety of useful documentation about sup‐\\nport vector machines.\\n200 | Chapter 16: Logistic Regression\\nwww.it-ebooks.infoCHAPTER 17\\nDecision Trees\\nA tree is an incomprehensible mystery.\\n—Jim Woodring\\nDataSciencester’s VP of Talent has interviewed a number of job candidates from the\\nsite, with varying degrees of success. He’s collected a data set consisting of several\\n(qualitative) attributes of each candidate, as well as whether that candidate inter‐\\nviewed well or poorly. Could you, he asks, use this data to build a model identifying\\nwhich candidates will interview well, so that he doesn’t have to waste time conducting\\ninterviews?\\nThis seems like a good fit for a decision tree , another predictive modeling tool in the\\ndata scientist’s kit.\\nWhat Is a Decision Tree?\\nA decision tree uses a tree structure to represent a number of possible decision paths\\nand an outcome for each path.\\nIf you have ever played the game Twenty Questions , then it turns out you are familiar\\nwith decision trees. For example:\\n•“I am thinking of an animal. ”\\n•“Does it have more than five legs?”\\n•“No. ”\\n•“Is it delicious?”\\n•“No. ”\\n•“Does it appear on the back of the Australian five-cent coin?”\\n201\\nwww.it-ebooks.info•“Y es. ”\\n•“Is it an echidna?”\\n•“Y es, it is!”\\nThis corresponds to the path:\\n“Not more than 5 legs” → “Not delicious” → “On the 5-cent coin” → “Echidna!”\\nin an idiosyncratic (and not very comprehensive) “guess the animal” decision tree\\n(Figure 17-1 ).\\nFigure 17-1. A “guess the animal” decision tree\\nDecision trees have a lot to recommend them. They’re very easy to understand and\\ninterpret, and the process by which they reach a prediction is completely transparent.\\nUnlike the other models we’ve looked at so far, decision trees can easily handle a mix\\nof numeric (e.g., number of legs) and categorical (e.g., delicious/not delicious)\\nattributes and can even classify data for which attributes are missing.\\nAt the same time, finding an “optimal” decision tree for a set of training data is com‐\\nputationally a very hard problem. (We will get around this by trying to build a good-\\nenough tree rather than an optimal one, although for large data sets this can still be a\\nlot of work.) More important, it is very easy (and very bad) to build decision trees\\nthat are overfitted  to the training data, and that don’t generalize well to unseen data.\\nWe’ll look at ways to address this.\\n202 | Chapter 17: Decision Trees\\nwww.it-ebooks.infoMost people divide decision trees into classification  trees  (which produce categorical\\noutputs) and regression trees  (which produce numeric outputs). In this chapter, we’ll\\nfocus on classification trees, and we’ll work through the ID3 algorithm for learning a\\ndecision tree from a set of labeled data, which should help us understand how deci‐\\nsion trees actually work. To make things simple, we’ll restrict ourselves to problems\\nwith binary outputs like “should I hire this candidate?” or “should I show this website\\nvisitor advertisement A or advertisement B?” or “will eating this food I found in the\\noffice fridge make me sick?”\\nEntropy\\nIn order to build a decision tree, we will need to decide what questions to ask and in\\nwhat order. At each stage of the tree there are some possibilities we’ve eliminated and\\nsome that we haven’t. After learning that an animal doesn’t have more than five legs,\\nwe’ve eliminated the possibility that it’s a grasshopper. We haven’t eliminated the pos‐\\nsibility that it’s a duck. Every possible question partitions the remaining possibilities\\naccording to their answers.\\nIdeally, we’ d like to choose questions whose answers give a lot of information about\\nwhat our tree should predict. If there’s a single yes/no question for which “yes”\\nanswers always correspond to True  outputs and “no” answers to False  outputs (or\\nvice versa), this would be an awesome question to pick. Conversely, a yes/no question\\nfor which neither answer gives you much new information about what the prediction\\nshould be is probably not a good choice.\\nWe capture this notion of “how much information” with entropy . Y ou have probably\\nheard this used to mean disorder. We use it to represent the uncertainty associated\\nwith data.\\nImagine that we have a set S of data, each member of which is labeled as belonging to\\none of a finite number of classes C1, ...,Cn. If all the data points belong to a single\\nclass, then there is no real uncertainty, which means we’ d like there to be low entropy.\\nIf the data points are evenly spread across the classes, there is a lot of uncertainty and\\nwe’ d like there to be high entropy.\\nIn math terms, if pi is the proportion of data labeled as class ci, we define the entropy\\nas:\\nHS= − p1log2p1− ... − pnlog2pn\\nwith the (standard) convention that 0 log 0 = 0 .\\nEntropy | 203\\nwww.it-ebooks.infoWithout worrying too much about the grisly details, each term −pilog2pi is non-\\nnegative and is close to zero precisely when pi is either close to zero or close to one\\n(Figure 17-2 ).\\nFigure 17-2. A graph of -p log p\\nThis means the entropy will be small when every pi is close to 0 or 1 (i.e., when most\\nof the data is in a single class), and it will be larger when many of the pi’s are not close\\nto 0 (i.e., when the data is spread across multiple classes). This is exactly the behavior\\nwe desire.\\nIt is easy enough to roll all of this into a function:\\ndef entropy(class_probabilities ):\\n    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\\n    return sum(-p * math.log(p, 2)\\n               for p in class_probabilities\\n               if p)                         # ignore zero probabilities\\nOur data will consist of pairs (input, label) , which means that we’ll need to com‐\\npute the class probabilities ourselves. Observe that we don’t actually care which label\\nis associated with each probability, only what the probabilities are:\\n204 | Chapter 17: Decision Trees\\nwww.it-ebooks.infodef class_probabilities (labels):\\n    total_count  = len(labels)\\n    return [count / total_count\\n            for count in Counter(labels).values()]\\ndef data_entropy (labeled_data ):\\n    labels = [label for _, label in labeled_data ]\\n    probabilities  = class_probabilities (labels)\\n    return entropy(probabilities )\\nThe Entropy of a Partition\\nWhat we’ve done so far is compute the entropy (think “uncertainty”) of a single set of\\nlabeled data. Now, each stage of a decision tree involves asking a question whose\\nanswer partitions data into one or (hopefully) more subsets. For instance, our “does it\\nhave more than five legs?” question partitions animals into those who have more than\\nfive legs (e.g., spiders) and those that don’t (e.g., echidnas).\\nCorrespondingly, we’ d like some notion of the entropy that results from partitioning a\\nset of data in a certain way. We want a partition to have low entropy if it splits the\\ndata into subsets that themselves have low entropy (i.e., are highly certain), and high\\nentropy if it contains subsets that (are large and) have high entropy (i.e., are highly\\nuncertain).\\nFor example, my “ Australian five-cent coin” question was pretty dumb (albeit pretty\\nlucky!), as it partitioned the remaining animals at that point into S1 = {echidna} and\\nS2 = {everything else}, where S2 is both large and high-entropy. ( S1 has no entropy but\\nit represents a small fraction of the remaining “classes. ”)\\nMathematically, if we partition our data S into subsets S1, ...,Sm containing propor‐\\ntions q1, ...,qm of the data, then we compute the entropy of the partition as a weighted\\nsum:\\nH=q1HS1+ . . . + qmHSm\\nwhich we can implement as:\\ndef partition_entropy (subsets):\\n    \"\"\"find the entropy from this partition of data into subsets\\n    subsets is a list of lists of labeled data\"\"\"\\n    total_count  = sum(len(subset) for subset in subsets)\\n    return sum( data_entropy (subset) * len(subset) / total_count\\n                for subset in subsets )\\nThe Entropy of a Partition | 205\\nwww.it-ebooks.infoOne problem with this approach is that partitioning by an attribute\\nwith many different values will result in a very low entropy due to\\noverfitting. For example, imagine you work for a bank and are try‐\\ning to build a decision tree to predict which of your customers are\\nlikely to default on their mortgages, using some historical data as\\nyour training set. Imagine further that the data set contains each\\ncustomer’s Social Security number. Partitioning on SSN will pro‐\\nduce one-person subsets, each of which necessarily has zero\\nentropy. But a model that relies on SSN is certain  not to generalize\\nbeyond the training set. For this reason, you should probably try to\\navoid (or bucket, if appropriate) attributes with large numbers of\\npossible values when creating decision trees.\\nCreating a Decision Tree\\nThe VP provides you with the interviewee data, consisting of (per your specification)\\npairs (input, label) , where each input  is a dict  of candidate attributes, and each\\nlabel is either True  (the candidate interviewed well) or False  (the candidate inter‐\\nviewed poorly). In particular, you are provided with each candidate’s level, her prefer‐\\nred language, whether she is active on Twitter, and whether she has a PhD:\\ninputs = [\\n    ({\\'level\\':\\'Senior\\' , \\'lang\\':\\'Java\\', \\'tweets\\' :\\'no\\', \\'phd\\':\\'no\\'},    False),\\n    ({\\'level\\':\\'Senior\\' , \\'lang\\':\\'Java\\', \\'tweets\\' :\\'no\\', \\'phd\\':\\'yes\\'},   False),\\n    ({\\'level\\':\\'Mid\\', \\'lang\\':\\'Python\\' , \\'tweets\\' :\\'no\\', \\'phd\\':\\'no\\'},      True),\\n    ({\\'level\\':\\'Junior\\' , \\'lang\\':\\'Python\\' , \\'tweets\\' :\\'no\\', \\'phd\\':\\'no\\'},   True),\\n    ({\\'level\\':\\'Junior\\' , \\'lang\\':\\'R\\', \\'tweets\\' :\\'yes\\', \\'phd\\':\\'no\\'},       True),\\n    ({\\'level\\':\\'Junior\\' , \\'lang\\':\\'R\\', \\'tweets\\' :\\'yes\\', \\'phd\\':\\'yes\\'},     False),\\n    ({\\'level\\':\\'Mid\\', \\'lang\\':\\'R\\', \\'tweets\\' :\\'yes\\', \\'phd\\':\\'yes\\'},         True),\\n    ({\\'level\\':\\'Senior\\' , \\'lang\\':\\'Python\\' , \\'tweets\\' :\\'no\\', \\'phd\\':\\'no\\'},  False),\\n    ({\\'level\\':\\'Senior\\' , \\'lang\\':\\'R\\', \\'tweets\\' :\\'yes\\', \\'phd\\':\\'no\\'},       True),\\n    ({\\'level\\':\\'Junior\\' , \\'lang\\':\\'Python\\' , \\'tweets\\' :\\'yes\\', \\'phd\\':\\'no\\'},  True),\\n    ({\\'level\\':\\'Senior\\' , \\'lang\\':\\'Python\\' , \\'tweets\\' :\\'yes\\', \\'phd\\':\\'yes\\'}, True),\\n    ({\\'level\\':\\'Mid\\', \\'lang\\':\\'Python\\' , \\'tweets\\' :\\'no\\', \\'phd\\':\\'yes\\'},     True),\\n    ({\\'level\\':\\'Mid\\', \\'lang\\':\\'Java\\', \\'tweets\\' :\\'yes\\', \\'phd\\':\\'no\\'},       True),\\n    ({\\'level\\':\\'Junior\\' , \\'lang\\':\\'Python\\' , \\'tweets\\' :\\'no\\', \\'phd\\':\\'yes\\'}, False)\\n]\\nOur tree will consist of decision nodes  (which ask a question and direct us differently\\ndepending on the answer) and leaf nodes  (which give us a prediction). We will build it\\nusing the relatively simple ID3 algorithm, which operates in the following manner.\\nLet’s say we’re given some labeled data, and a list of attributes to consider branching\\non.\\n•If the data all have the same label, then create a leaf node that predicts that label\\nand then stop.\\n206 | Chapter 17: Decision Trees\\nwww.it-ebooks.info•If the list of attributes is empty (i.e., there are no more possible questions to ask),\\nthen create a leaf node that predicts the most common label and then stop.\\n•Otherwise, try partitioning the data by each of the attributes\\n•Choose the partition with the lowest partition entropy\\n•Add a decision node based on the chosen attribute\\n•Recur on each partitioned subset using the remaining attributes\\nThis is what’s known as a “greedy” algorithm because, at each step, it chooses the\\nmost immediately best option. Given a data set, there may be a better tree with a\\nworse-looking first move. If so, this algorithm won’t find it. Nonetheless, it is rela‐\\ntively easy to understand and implement, which makes it a good place to begin\\nexploring decision trees.\\nLet’s manually go through these steps on the interviewee data set. The data set has\\nboth True  and False  labels, and we have four attributes we can split on. So our first\\nstep will be to find the partition with the least entropy. We’ll start by writing a func‐\\ntion that does the partitioning:\\ndef partition_by (inputs, attribute ):\\n    \"\"\"each input is a pair (attribute_dict, label).\\n    returns a dict : attribute_value -> inputs\"\"\"\\n    groups = defaultdict (list)\\n    for input in inputs:\\n        key = input[0][attribute ]   # get the value of the specified attribute\\n        groups[key].append(input)   # then add this input to the correct list\\n    return groups\\nand one that uses it to compute entropy:\\ndef partition_entropy_by (inputs, attribute ):\\n    \"\"\"computes the entropy corresponding to the given partition\"\"\"\\n    partitions  = partition_by (inputs, attribute )\\n    return partition_entropy (partitions .values())\\nThen we just need to find the minimum-entropy partition for the whole data set:\\nfor key in [\\'level\\',\\'lang\\',\\'tweets\\' ,\\'phd\\']:\\n    print key, partition_entropy_by (inputs, key)\\n# level 0.693536138896\\n# lang 0.860131712855\\n# tweets 0.788450457308\\n# phd 0.892158928262\\nThe lowest entropy comes from splitting on level , so we’ll need to make a subtree\\nfor each possible level  value. Every Mid candidate is labeled True , which means that\\nthe Mid subtree is simply a leaf node predicting True . For Senior  candidates, we have\\na mix of True s and False s, so we need to split again:\\nCreating a Decision Tree | 207\\nwww.it-ebooks.infosenior_inputs  = [(input, label)\\n                 for input, label in inputs if input[\"level\"] == \"Senior\" ]\\nfor key in [\\'lang\\', \\'tweets\\' , \\'phd\\']:\\n    print key, partition_entropy_by (senior_inputs , key)\\n# lang 0.4\\n# tweets 0.0\\n# phd 0.950977500433\\nThis shows us that our next split should be on tweets , which results in a zero-\\nentropy partition. For these Senior-level candidates, “yes” tweets always result in True\\nwhile “no” tweets always result in False .\\nFinally, if we do the same thing for the Junior  candidates, we end up splitting on phd,\\nafter which we find that no PhD always results in True  and PhD always results in\\nFalse .\\nFigure 17-3  shows the complete decision tree.\\nFigure 17-3. The decision tree for hiring\\nPutting It All Together\\nNow that we’ve seen how the algorithm works, we would like to implement it more\\ngenerally. This means we need to decide how we want to represent trees. We’ll use\\npretty much the most lightweight representation possible. We define a tree to be one\\nof the following:\\n208 | Chapter 17: Decision Trees\\nwww.it-ebooks.info•True\\n•False\\n•a tuple (attribute, subtree_dict)\\nHere True  represents a leaf node that returns True  for any input, False  represents a\\nleaf node that returns False  for any input, and a tuple represents a decision node\\nthat, for any input, finds its attribute  value, and classifies the input using the corre‐\\nsponding subtree.\\nWith this representation, our hiring tree would look like:\\n(\\'level\\',\\n {\\'Junior\\' : (\\'phd\\', {\\'no\\': True, \\'yes\\': False}),\\n  \\'Mid\\': True,\\n  \\'Senior\\' : (\\'tweets\\' , {\\'no\\': False, \\'yes\\': True})})\\nThere’s still the question of what to do if we encounter an unexpected (or missing)\\nattribute value. What should our hiring tree do if it encounters a candidate whose\\nlevel  is “Intern”? We’ll handle this case by adding a None  key that just predicts the\\nmost common label. (Although this would be a bad idea if None  is actually a value\\nthat appears in the data.)\\nGiven such a representation, we can classify an input with:\\ndef classify (tree, input):\\n    \"\"\"classify the input using the given decision tree\"\"\"\\n    # if this is a leaf node, return its value\\n    if tree in [True, False]:\\n        return tree\\n    # otherwise this tree consists of an attribute to split on\\n    # and a dictionary whose keys are values of that attribute\\n    # and whose values of are subtrees to consider next\\n    attribute , subtree_dict  = tree\\n    subtree_key  = input.get(attribute )    # None if input is missing attribute\\n    if subtree_key  not in subtree_dict :   # if no subtree for key,\\n        subtree_key  = None                # we\\'ll use the None subtree\\n    subtree = subtree_dict [subtree_key ]   # choose the appropriate subtree\\n    return classify (subtree, input)       # and use it to classify the input\\nAll that’s left is to build the tree representation from our training data:\\ndef build_tree_id3 (inputs, split_candidates =None):\\n    # if this is our first pass,\\n    # all keys of the first input are split candidates\\nPutting It All Together | 209\\nwww.it-ebooks.info    if split_candidates  is None:\\n        split_candidates  = inputs[0][0].keys()\\n    # count Trues and Falses in the inputs\\n    num_inputs  = len(inputs)\\n    num_trues  = len([label for item, label in inputs if label])\\n    num_falses  = num_inputs  - num_trues\\n    if num_trues  == 0: return False     # no Trues? return a \"False\" leaf\\n    if num_falses  == 0: return True     # no Falses? return a \"True\" leaf\\n    if not split_candidates :            # if no split candidates left\\n        return num_trues  >= num_falses   # return the majority leaf\\n    # otherwise, split on the best attribute\\n    best_attribute  = min(split_candidates ,\\n                         key=partial(partition_entropy_by , inputs))\\n    partitions  = partition_by (inputs, best_attribute )\\n    new_candidates  = [a for a in split_candidates\\n                      if a != best_attribute ]\\n    # recursively build the subtrees\\n    subtrees  = { attribute_value  : build_tree_id3 (subset, new_candidates )\\n                 for attribute_value , subset in partitions .iteritems () }\\n    subtrees [None] = num_trues  > num_falses       # default case\\n    return (best_attribute , subtrees )\\nIn the tree we built, every leaf consisted entirely of True  inputs or entirely of False\\ninputs. This means that the tree predicts perfectly on the training data set. But we can\\nalso apply it to new data that wasn’t in the training set:\\ntree = build_tree_id3 (inputs)\\nclassify (tree, { \"level\" : \"Junior\" ,\\n                 \"lang\" : \"Java\",\\n                 \"tweets\"  : \"yes\",\\n                 \"phd\" : \"no\"} )        # True\\nclassify (tree, { \"level\" : \"Junior\" ,\\n                 \"lang\" : \"Java\",\\n                 \"tweets\"  : \"yes\",\\n                 \"phd\" : \"yes\"} )       # False\\nAnd also to data with missing or unexpected values:\\nclassify (tree, { \"level\" : \"Intern\"  } ) # True\\nclassify (tree, { \"level\" : \"Senior\"  } ) # False\\n210 | Chapter 17: Decision Trees\\nwww.it-ebooks.infoSince our goal was mainly to demonstrate how to build a tree, we\\nbuilt the tree using the entire data set. As always, if we were really\\ntrying to create a good model for something, we would have (col‐\\nlected more data and) split the data into train/validation/test sub‐\\nsets.\\nRandom Forests\\nGiven how closely decision trees can fit themselves to their training data, it’s not sur‐\\nprising that they have a tendency to overfit. One way of avoiding this is a technique\\ncalled random forests , in which we build multiple decision trees and let them vote on\\nhow to classify inputs:\\ndef forest_classify (trees, input):\\n    votes = [classify (tree, input) for tree in trees]\\n    vote_counts  = Counter(votes)\\n    return vote_counts .most_common (1)[0][0]\\nOur tree-building process was deterministic, so how do we get random trees?\\nOne piece involves bootstrapping data (recall “Digression: The Bootstrap” on page\\n183). Rather than training each tree on all the inputs  in the training set, we train\\neach tree on the result of bootstrap_sample(inputs) . Since each tree is built using\\ndifferent data, each tree will be different from every other tree. (A side benefit is that\\nit’s totally fair to use the nonsampled data to test each tree, which means you can get\\naway with using all of your data as the training set if you are clever in how you meas‐\\nure performance.) This technique is known as bootstrap aggregating  or bagging .\\nA second source of randomness involves changing the way we chose the\\nbest_attribute  to split on. Rather than looking at all the remaining attributes, we\\nfirst choose a random subset of them and then split on whichever of those is best:\\n    # if there\\'s already few enough split candidates, look at all of them\\n    if len(split_candidates ) <= self.num_split_candidates :\\n        sampled_split_candidates  = split_candidates\\n    # otherwise pick a random sample\\n    else:\\n        sampled_split_candidates  = random.sample(split_candidates ,\\n                                                 self.num_split_candidates )\\n    # now choose the best attribute only from those candidates\\n    best_attribute  = min(sampled_split_candidates ,\\n        key=partial(partition_entropy_by , inputs))\\n    partitions  = partition_by (inputs, best_attribute )\\nThis is an example of a broader technique  called ensemble learning  in which we com‐\\nbine several weak learners  (typically high-bias, low-variance models) in order to pro‐\\nduce an overall strong model.\\nRandom Forests | 211\\nwww.it-ebooks.infoRandom forests are one of the most popular and versatile models around.\\nFor Further Exploration\\n•scikit-learn has many Decision Tree  models. It also has an ensemble  module that\\nincludes a RandomForestClassifier  as well as other ensemble methods.\\n•We barely scratched the surface of decision trees and their algorithms. Wikipedia\\nis a good starting point for broader exploration.\\n212 | Chapter 17: Decision Trees\\nwww.it-ebooks.infoCHAPTER 18\\nNeural Networks\\nI like nonsense; it wakes up the brain cells.\\n—Dr. Seuss\\nAn artificial  neural network  (or neural network for short) is a predictive model moti‐\\nvated by the way the brain operates. Think of the brain as a collection of neurons\\nwired together. Each neuron looks at the outputs of the other neurons that feed into\\nit, does a calculation, and then either fires (if the calculation exceeds some thresh‐\\nhold) or doesn’t (if it doesn’t).\\nAccordingly, artificial neural networks consist of artificial neurons, which perform\\nsimilar calculations over their inputs. Neural networks can solve a wide variety of\\nproblems like handwriting recognition and face detection, and they are used heavily\\nin deep learning, one of the trendiest subfields of data science. However, most neural\\nnetworks are “black boxes”—inspecting their details doesn’t give you much under‐\\nstanding of how they’re solving a problem. And large neural networks can be difficult\\nto train. For most problems you’ll encounter as a budding data scientist, they’re prob‐\\nably not the right choice. Someday, when you’re trying to build an artificial intelli‐\\ngence to bring about the Singularity, they very well might be.\\nPerceptrons\\nPretty much the simplest neural network  is the perceptron , which approximates a sin‐\\ngle neuron with n binary inputs. It computes a weighted sum of its inputs and “fires”\\nif that weighted sum is zero or greater:\\ndef step_function (x):\\n    return 1 if x >= 0 else 0\\ndef perceptron_output (weights, bias, x):\\n    \"\"\"returns 1 if the perceptron \\'fires\\', 0 if not\"\"\"\\n213\\nwww.it-ebooks.info    calculation  = dot(weights, x) + bias\\n    return step_function (calculation )\\nThe perceptron is simply distinguishing between the half spaces separated by the\\nhyperplane of points x for which:\\ndot(weights,x) + bias == 0\\nWith properly chosen weights, perceptrons can solve a number of simple problems\\n(Figure 18-1 ). For example, we can create an AND gate  (which returns 1 if both its\\ninputs are 1 but returns 0 if one of its inputs is 0) with:\\nweights = [2, 2]\\nbias = -3\\nIf both inputs are 1, the calculation  equals 2 + 2 - 3 = 1, and the output is 1. If only\\none of the inputs is 1, the calculation  equals 2 + 0 - 3 = -1, and the output is 0. And\\nif both of the inputs are 0, the calculation  equals -3, and the output is 0.\\nSimilarly, we could build an OR gate  with:\\nweights = [2, 2]\\nbias = -1\\nFigure 18-1. Decision space for a two-input perceptron\\n214 | Chapter 18: Neural Networks\\nwww.it-ebooks.infoAnd we could build a NOT gate  (which has one input and converts 1 to 0 and 0 to 1)\\nwith:\\nweights = [-2]\\nbias = 1\\nHowever, there are some problems that simply can’t be solved by a single perceptron.\\nFor example, no matter how hard you try, you cannot use a perceptron to build an\\nXOR gate  that outputs 1 if exactly one of its inputs is 1 and 0 otherwise. This is where\\nwe start needing more-complicated neural networks.\\nOf course, you don’t need to approximate a neuron in order to build a logic gate:\\nand_gate  = min\\nor_gate = max\\nxor_gate  = lambda x, y: 0 if x == y else 1\\nLike real neurons, artificial neurons start getting more interesting when you start\\nconnecting them together.\\nFeed-Forward Neural Networks\\nThe topology of the brain is enormously complicated, so it’s common to approximate\\nit with an idealized feed-forward  neural network that consists of discrete layers  of\\nneurons, each connected to the next. This typically entails an input layer (which\\nreceives inputs and feeds them forward unchanged), one or more “hidden layers”\\n(each of which consists of neurons that take the outputs of the previous layer, per‐\\nforms some calculation, and passes the result to the next layer), and an output layer\\n(which produces the final outputs).\\nJust like the perceptron, each (noninput) neuron has a weight corresponding to each\\nof its inputs and a bias. To make our representation simpler, we’ll add the bias to the\\nend of our weights vector and give each neuron a bias input  that always equals 1.\\nAs with the perceptron, for each neuron we’ll sum up the products of its inputs and\\nits weights. But here, rather than outputting the step_function  applied to that prod‐\\nuct, we’ll output a smooth approximation of the step function. In particular, we’ll use\\nthe sigmoid  function ( Figure 18-2 ):\\ndef sigmoid(t):\\n    return 1 / (1 + math.exp(-t))\\nFeed-Forward Neural Networks | 215\\nwww.it-ebooks.infoFigure 18-2. The sigmoid function\\nWhy use sigmoid  instead of the simpler step_function ? In order to train a neural\\nnetwork, we’ll need to use calculus, and in order to use calculus, we need smooth\\nfunctions. The step function isn’t even continuous, and sigmoid is a good smooth\\napproximation of it.\\nY ou may remember sigmoid  from Chapter 16 , where it was called\\nlogistic . Technically “sigmoid” refers to the shape  of the function,\\n“logistic” to this particular function although people often use the\\nterms interchangeably.\\nWe then calculate the output as:\\ndef neuron_output (weights, inputs):\\n    return sigmoid(dot(weights, inputs))\\nGiven this function, we can represent a neuron simply as a list of weights whose\\nlength is one more than the number of inputs to that neuron (because of the bias\\nweight). Then we can represent a neural network as a list of (noninput) layers , where\\neach layer is just a list of the neurons in that layer.\\n216 | Chapter 18: Neural Networks\\nwww.it-ebooks.infoThat is, we’ll represent a neural network as a list (layers) of lists (neurons) of lists\\n(weights).\\nGiven such a representation, using the neural network is quite simple:\\ndef feed_forward (neural_network , input_vector ):\\n    \"\"\"takes in a neural network\\n    (represented as a list of lists of lists of weights)\\n    and returns the output from forward-propagating the input\"\"\"\\n    outputs = []\\n    # process one layer at a time\\n    for layer in neural_network :\\n        input_with_bias  = input_vector  + [1]              # add a bias input\\n        output = [neuron_output (neuron, input_with_bias )  # compute the output\\n                  for neuron in layer]                    # for each neuron\\n        outputs.append(output)                            # and remember it\\n        # then the input to the next layer is the output of this one\\n        input_vector  = output\\n    return outputs\\nNow it’s easy to build the XOR gate that we couldn’t build with a single perceptron.\\nWe just need to scale the weights up so that the neuron_output s are either really close\\nto 0 or really close to 1:\\nxor_network  = [# hidden layer\\n               [[20, 20, -30],      # \\'and\\' neuron\\n                [20, 20, -10]],     # \\'or\\'  neuron\\n               # output layer\\n               [[-60, 60, -30]]]    # \\'2nd input but not 1st input\\' neuron\\nfor x in [0, 1]:\\n    for y in [0, 1]:\\n        # feed_forward produces the outputs of every neuron\\n        # feed_forward[-1] is the outputs of the output-layer neurons\\n        print x, y, feed_forward (xor_network ,[x, y])[-1]\\n# 0 0 [9.38314668300676e-14]\\n# 0 1 [0.9999999999999059]\\n# 1 0 [0.9999999999999059]\\n# 1 1 [9.383146683006828e-14]\\nBy using a hidden layer, we are able to feed the output of an “and” neuron and the\\noutput of an “or” neuron into a “second input but not first input” neuron. The result\\nis a network that performs “or, but not and, ” which is precisely XOR ( Figure 18-3 ).\\nFeed-Forward Neural Networks | 217\\nwww.it-ebooks.infoFigure 18-3. A neural network for XOR\\nBackpropagation\\nUsually we don’t build neural networks by hand. This is in part because we use them\\nto solve much bigger problems—an image recognition problem might involve hun‐\\ndreds or thousands of neurons. And it’s in part because we usually won’t be able to\\n“reason out” what the neurons should be.\\nInstead (as usual) we use data to train  neural networks. One popular approach is an\\nalgorithm called backpropagation  that has similarities to the gradient descent algo‐\\nrithm we looked at earlier.\\nImagine we have a training set that consists of input vectors and corresponding target\\noutput vectors. For example, in our previous xor_network  example, the input vector\\n[1, 0]  corresponded to the target output [1]. And imagine that our network has\\nsome set of weights. We then adjust the weights using the following algorithm:\\n1.Run feed_forward  on an input vector to produce the outputs of all the neurons\\nin the network.\\n2.This results in an error for each output neuron—the difference between its out‐\\nput and its target.\\n3.Compute the gradient of this error as a function of the neuron’s weights, and\\nadjust its weights in the direction that most decreases the error.\\n4.“Propagate” these output errors backward to infer errors for the hidden layer.\\n5.Compute the gradients of these errors and adjust the hidden layer’s weights in the\\nsame manner.\\n218 | Chapter 18: Neural Networks\\nwww.it-ebooks.infoTypically we run this algorithm many times for our entire training set until the net‐\\nwork converges:\\ndef backpropagate (network, input_vector , targets):\\n    hidden_outputs , outputs = feed_forward (network, input_vector )\\n    # the output * (1 - output) is from the derivative of sigmoid\\n    output_deltas  = [output * (1 - output) * (output - target)\\n                     for output, target in zip(outputs, targets)]\\n    # adjust weights for output layer, one neuron at a time\\n    for i, output_neuron  in enumerate (network[-1]):\\n        # focus on the ith output layer neuron\\n        for j, hidden_output  in enumerate (hidden_outputs  + [1]):\\n            # adjust the jth weight based on both\\n            # this neuron\\'s delta and its jth input\\n            output_neuron [j] -= output_deltas [i] * hidden_output\\n    # back-propagate errors to hidden layer\\n    hidden_deltas  = [hidden_output  * (1 - hidden_output ) *\\n                      dot(output_deltas , [n[i] for n in output_layer ])\\n                     for i, hidden_output  in enumerate (hidden_outputs )]\\n    # adjust weights for hidden layer, one neuron at a time\\n    for i, hidden_neuron  in enumerate (network[0]):\\n        for j, input in enumerate (input_vector  + [1]):\\n            hidden_neuron [j] -= hidden_deltas [i] * input\\nThis is pretty much doing the same thing as if you explicitly wrote the squared error\\nas a function of the weights and used the minimize_stochastic  function we built in\\nChapter 8 .\\nIn this case, explicitly writing out the gradient function turns out to be kind of a pain.\\nIf you know calculus and the chain rule, the mathematical details are relatively\\nstraightforward, but keeping the notation straight (“the partial derivative of the error\\nfunction with respect to the weight that neuron i assigns to the input coming from\\nneuron j”) is not much fun.\\nExample: Defeating a CAPTCHA\\nTo make sure that people registering for your site are actually people, the VP of Prod‐\\nuct Management wants to implement a CAPTCHA as part of the registration process.\\nIn particular, he’ d like to show users a picture of a digit and require them to input that\\ndigit to prove they’re human.\\nHe doesn’t believe you that computers can easily solve this problem, so you decide to\\nconvince him by creating a program that can easily solve the problem.\\nWe’ll represent each digit as a 5 × 5 image:\\nExample: Defeating a CAPTCHA | 219\\nwww.it-ebooks.info@@@@@  ..@..  @@@@@  @@@@@  @...@  @@@@@  @@@@@  @@@@@  @@@@@  @@@@@\\n@...@  ..@..  ....@  ....@  @...@  @....  @....  ....@  @...@  @...@\\n@...@  ..@..  @@@@@  @@@@@  @@@@@  @@@@@  @@@@@  ....@  @@@@@  @@@@@\\n@...@  ..@..  @....  ....@  ....@  ....@  @...@  ....@  @...@  ....@\\n@@@@@  ..@..  @@@@@  @@@@@  ....@  @@@@@  @@@@@  ....@  @@@@@  @@@@@\\nOur neural network wants an input to be a vector of numbers. So we’ll transform\\neach image to a vector of length 25, whose elements are either 1 (“this pixel is in the\\nimage”) or 0 (“this pixel is not in the image”).\\nFor instance, the zero digit would be represented as:\\nzero_digit  = [1,1,1,1,1,\\n              1,0,0,0,1,\\n              1,0,0,0,1,\\n              1,0,0,0,1,\\n              1,1,1,1,1]\\nWe’ll want our output to indicate which digit the neural network thinks it is, so we’ll\\nneed 10 outputs. The correct output for digit 4, for instance, will be:\\n[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\\nThen, assuming our inputs  are correctly ordered from 0 to 9, our targets will be:\\ntargets = [[1 if i == j else 0 for i in range(10)]\\n           for j in range(10)]\\nso that (for example) targets[4]  is the correct output for digit 4.\\nAt which point we’re ready to build our neural network:\\nrandom.seed(0)      # to get repeatable results\\ninput_size  = 25     # each input is a vector of length 25\\nnum_hidden  = 5      # we\\'ll have 5 neurons in the hidden layer\\noutput_size  = 10    # we need 10 outputs for each input\\n# each hidden neuron has one weight per input, plus a bias weight\\nhidden_layer  = [[random.random() for __ in range(input_size  + 1)]\\n                for __ in range(num_hidden )]\\n# each output neuron has one weight per hidden neuron, plus a bias weight\\noutput_layer  = [[random.random() for __ in range(num_hidden  + 1)]\\n                for __ in range(output_size )]\\n# the network starts out with random weights\\nnetwork = [hidden_layer , output_layer ]\\nAnd we can train it using the backpropagation algorithm:\\n# 10,000 iterations seems enough to converge\\nfor __ in range(10000):\\n    for input_vector , target_vector  in zip(inputs, targets):\\n        backpropagate (network, input_vector , target_vector )\\n220 | Chapter 18: Neural Networks\\nwww.it-ebooks.infoIt works well on the training set, obviously:\\ndef predict(input):\\n    return feed_forward (network, input)[-1]\\npredict(inputs[7])\\n# [0.026, 0.0, 0.0, 0.018, 0.001, 0.0, 0.0, 0.967, 0.0, 0.0]\\nWhich indicates that the digit 7 output neuron produces 0.97, while all the other out‐\\nput neurons produce very small numbers.\\nBut we can also apply it to differently drawn digits, like my stylized 3:\\npredict([0,1,1,1,0,  # .@@@.\\n         0,0,0,1,1,  # ...@@\\n         0,0,1,1,0,  # ..@@.\\n         0,0,0,1,1,  # ...@@\\n         0,1,1,1,0]) # .@@@.\\n# [0.0, 0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.01, 0.0, 0.12]\\nThe network still thinks it looks like a 3, whereas my stylized 8 gets votes for being a\\n5, an 8, and a 9:\\npredict([0,1,1,1,0,  # .@@@.\\n         1,0,0,1,1,  # @..@@\\n         0,1,1,1,0,  # .@@@.\\n         1,0,0,1,1,  # @..@@\\n         0,1,1,1,0]) # .@@@.\\n# [0.0, 0.0, 0.0, 0.0, 0.0, 0.55, 0.0, 0.0, 0.93, 1.0]\\nHaving a larger training set would probably help.\\nAlthough the network’s operation is not exactly transparent, we can inspect the\\nweights of the hidden layer to get a sense of what they’re recognizing. In particular,\\nwe can plot the weights of each neuron as a 5 × 5 grid corresponding to the 5 × 5\\ninputs.\\nIn real life you’ d probably want to plot zero weights as white, with larger positive\\nweights more and more (say) green and larger negative weights more and more (say)\\nred. Unfortunately, that’s hard to do in a black-and-white book.\\nInstead, we’ll plot zero weights as white, with far-away-from-zero weights darker and\\ndarker. And we’ll use crosshatching to indicate negative weights.\\nTo do this we’ll use pyplot.imshow , which we haven’t seen before. With it we can plot\\nimages pixel by pixel. Normally this isn’t all that useful for data science, but here it’s a\\ngood choice:\\nimport matplotlib\\nweights = network[0][0]               # first neuron in hidden layer\\nabs_weights  = map(abs, weights)       # darkness only depends on absolute value\\nExample: Defeating a CAPTCHA | 221\\nwww.it-ebooks.infogrid = [abs_weights [row:(row+5)]      # turn the weights into a 5x5 grid\\n        for row in range(0,25,5)]     # [weights[0:5], ..., weights[20:25]]\\nax = plt.gca()                        # to use hatching, we\\'ll need the axis\\nax.imshow(grid,                       # here same as plt.imshow\\n          cmap=matplotlib .cm.binary,  # use white-black color scale\\n          interpolation =\\'none\\')       # plot blocks as blocks\\ndef patch(x, y, hatch, color):\\n    \"\"\"return a matplotlib \\'patch\\' object with the specified\\n    location, crosshatch pattern, and color\"\"\"\\n    return matplotlib .patches.Rectangle ((x - 0.5, y - 0.5), 1, 1,\\n                                        hatch=hatch, fill=False, color=color)\\n# cross-hatch the negative weights\\nfor i in range(5):                    # row\\n    for j in range(5):                # column\\n        if weights[5*i + j] < 0:      # row i, column j = weights[5*i + j]\\n            # add black and white hatches, so visible whether dark or light\\n            ax.add_patch (patch(j, i, \\'/\\',  \"white\"))\\n            ax.add_patch (patch(j, i, \\'\\\\\\\\\\', \"black\"))\\nplt.show()\\nFigure 18-4. Weights for the hidden layer\\nIn Figure 18-4  we see that the first hidden neuron has large positive weights in the left\\ncolumn and in the center of the middle row, while it has large negative weights in the\\nright column. (And you can see that it has a pretty large negative bias, which means\\nthat it won’t fire strongly unless it gets precisely the positive inputs it’s “looking for. ”)\\nIndeed, on those inputs, it does what you’ d expect:\\nleft_column_only  = [1, 0, 0, 0, 0] * 5\\nprint feed_forward (network, left_column_only )[0][0]  # 1.0\\ncenter_middle_row  = [0, 0, 0, 0, 0] * 2 + [0, 1, 1, 1, 0] + [0, 0, 0, 0, 0] * 2\\nprint feed_forward (network, center_middle_row )[0][0]  # 0.95\\n222 | Chapter 18: Neural Networks\\nwww.it-ebooks.inforight_column_only  = [0, 0, 0, 0, 1] * 5\\nprint feed_forward (network, right_column_only )[0][0]  # 0.0\\nSimilarly, the middle hidden neuron seems to “like” horizontal lines but not side ver‐\\ntical lines, and the last hidden neuron seems to “like” the center row but not the right\\ncolumn. (The other two neurons are harder to interpret.)\\nWhat happens when we run my stylized 3 through the network?\\nmy_three  =  [0,1,1,1,0,  # .@@@.\\n             0,0,0,1,1,  # ...@@\\n             0,0,1,1,0,  # ..@@.\\n             0,0,0,1,1,  # ...@@\\n             0,1,1,1,0]  # .@@@.\\nhidden, output = feed_forward (network, my_three )\\nThe hidden  outputs are:\\n0.121080  # from network[0][0], probably dinged by (1, 4)\\n0.999979  # from network[0][1], big contributions from (0, 2) and (2, 2)\\n0.999999  # from network[0][2], positive everywhere except (3, 4)\\n0.999992  # from network[0][3], again big contributions from (0, 2) and (2, 2)\\n0.000000  # from network[0][4], negative or zero everywhere except center row\\nwhich enter into the “three” output neuron with weights network[-1][3] :\\n-11.61  # weight for hidden[0]\\n -2.17  # weight for hidden[1]\\n  9.31  # weight for hidden[2]\\n -1.38  # weight for hidden[3]\\n-11.47  # weight for hidden[4]\\n- 1.92  # weight for bias input\\nSo that the neuron computes:\\nsigmoid(.121 * -11.61 + 1 * -2.17 + 1 * 9.31 - 1.38 * 1 - 0 * 11.47 - 1.92)\\nwhich is 0.92, as we saw. In essence, the hidden layer is computing five different parti‐\\ntions of 25-dimensional space, mapping each 25-dimensional input down to five\\nnumbers. And then each output neuron looks only at the results of those five parti‐\\ntions.\\nAs we saw, my_three  falls slightly on the “low” side of partition 0 (i.e., only slightly\\nactivates hidden neuron 0), far on the “high” side of partitions 1, 2, and 3, (i.e.,\\nstrongly activates those hidden neurons), and far on the low side of partition 4 (i.e.,\\ndoesn’t active that neuron at all).\\nAnd then each of the 10 output neurons uses only those five activations to decide\\nwhether my_three  is their digit or not.\\nExample: Defeating a CAPTCHA | 223\\nwww.it-ebooks.infoFor Further Exploration\\n•Coursera has a free course on Neural Networks for Machine Learning . As I write\\nthis it was last run in 2012, but the course materials are still available.\\n•Michael Nielsen is writing a free online book on Neural Networks and Deep\\nLearning . By the time you read this it might be finished.\\n•PyBrain  is a pretty simple Python neural network library.\\n•Pylearn2  is a much more advanced (and much harder to use) neural network\\nlibrary.\\n224 | Chapter 18: Neural Networks\\nwww.it-ebooks.infoCHAPTER 19\\nClustering\\nWhere we such clusters had\\nAs made us nobly wild, not mad\\n—Robert Herrick\\nMost of the algorithms in this book are what’s known as supervised learning, in that\\nthey start with a set of labeled  data and use that as the basis for making predictions\\nabout new, unlabeled data. Clustering, however, is an example of unsupervised learn‐\\ning, in which we work with completely unlabeled data (or in which our data has\\nlabels but we ignore them).\\nThe Idea\\nWhenever you look at some source of data, it’s likely that the data will somehow form\\nclusters . A data set showing where millionaires live probably has clusters in places like\\nBeverly Hills and Manhattan. A data set showing how many hours people work each\\nweek probably has a cluster around 40 (and if it’s taken from a state with laws man‐\\ndating special benefits for people who work at least 20 hours a week, it probably has\\nanother cluster right around 19). A data set of demographics of registered voters\\nlikely forms a variety of clusters (e.g., “soccer moms, ” “bored retirees, ” “unemployed\\nmillennials”) that pollsters and political consultants likely consider relevant.\\nUnlike some of the problems we’ve looked at, there is generally no “correct” cluster‐\\ning. An alternative clustering scheme might group some of the “unemployed millen‐\\nials” with “grad students, ” others with “parents’ basement dwellers. ” Neither scheme is\\nnecessarily more correct—instead, each is likely more optimal with respect to its own\\n“how good are the clusters?” metric.\\nFurthermore, the clusters won’t label themselves. Y ou’ll have to do that by looking at\\nthe data underlying each one.\\n225\\nwww.it-ebooks.infoThe Model\\nFor us, each input  will be a vector in d-dimensional space (which, as usual, we will\\nrepresent as a list of numbers). Our goal will be to identify clusters of similar inputs\\nand (sometimes) to find a representative value for each cluster.\\nFor example, each input could be (a numeric vector that somehow represents) the\\ntitle of a blog post, in which case the goal might be to find clusters of similar posts,\\nperhaps in order to understand what our users are blogging about. Or imagine that\\nwe have a picture containing thousands of (red, green, blue)  colors and that we\\nneed to screen-print a 10-color version of it. Clustering can help us choose 10 colors\\nthat will minimize the total “color error. ”\\nOne of the simplest clustering  methods is k-means , in which the number of clusters k\\nis chosen in advance, after which the goal is to partition the inputs into sets S1, ...,Sk\\nin a way that minimizes the total sum of squared distances from each point to the\\nmean of its assigned cluster.\\nThere are a lot of ways to assign n points to k clusters, which means that finding an\\noptimal clustering is a very hard problem. We’ll settle for an iterative algorithm that\\nusually finds a good clustering:\\n1.Start with a set of k-means , which are points in d-dimensional space.\\n2.Assign each point to the mean to which it is closest.\\n3.If no point’s assignment has changed, stop and keep the clusters.\\n4.If some point’s assignment has changed, recompute the means and return to\\nstep 2.\\nUsing the vector_mean  function from Chapter 4 , it’s pretty simple to create a class\\nthat does this:\\nclass KMeans:\\n    \"\"\"performs k-means clustering\"\"\"\\n    def __init__ (self, k):\\n        self.k = k          # number of clusters\\n        self.means = None   # means of clusters\\n    def classify (self, input):\\n        \"\"\"return the index of the cluster closest to the input\"\"\"\\n        return min(range(self.k),\\n                   key=lambda i: squared_distance (input, self.means[i]))\\n    def train(self, inputs):\\n        # choose k random points as the initial means\\n        self.means = random.sample(inputs, self.k)\\n        assignments  = None\\n226 | Chapter 19: Clustering\\nwww.it-ebooks.info        while True:\\n            # Find new assignments\\n            new_assignments  = map(self.classify , inputs)\\n            # If no assignments have changed, we\\'re done.\\n            if assignments  == new_assignments :\\n                return\\n            # Otherwise keep the new assignments,\\n            assignments  = new_assignments\\n            # And compute new means based on the new assignments\\n            for i in range(self.k):\\n                # find all the points assigned to cluster i\\n                i_points  = [p for p, a in zip(inputs, assignments ) if a == i]\\n                # make sure i_points is not empty so don\\'t divide by 0\\n                if i_points :\\n                    self.means[i] = vector_mean (i_points )\\nLet’s take a look at how this works.\\nExample: Meetups\\nTo celebrate DataSciencester’s growth, your VP of User Rewards wants to organize\\nseveral in-person meetups for your hometown users, complete with beer, pizza, and\\nDataSciencester t-shirts. Y ou know the locations of all your local users ( Figure 19-1 ),\\nand she’ d like you to choose meetup locations that make it convenient for everyone to\\nattend.\\nDepending on how you look at it, you probably see two or three clusters. (It’s easy to\\ndo visually because the data is only two-dimensional. With more dimensions, it\\nwould be a lot harder to eyeball.)\\nImagine first that she has enough budget for three meetups. Y ou go to your computer\\nand try this:\\nrandom.seed(0)          # so you get the same results as me\\nclusterer  = KMeans(3)\\nclusterer .train(inputs)\\nprint clusterer .means\\nExample: Meetups | 227\\nwww.it-ebooks.infoFigure 19-1. The locations of your hometown users\\nY ou find three clusters centered at [-44,5], [-16,-10], and [18, 20], and you look for\\nmeetup venues near those locations ( Figure 19-2 ).\\nY ou show it to the VP , who informs you that now she only has enough budget for two\\nmeetups.\\n“No problem, ” you say:\\nrandom.seed(0)\\nclusterer  = KMeans(2)\\nclusterer .train(inputs)\\nprint clusterer .means\\n228 | Chapter 19: Clustering\\nwww.it-ebooks.infoFigure 19-2. User locations grouped into three clusters\\nAs shown in Figure 19-3 , one meetup should still be near [18, 20], but now the other\\nshould be near [-26, -5].\\nExample: Meetups | 229\\nwww.it-ebooks.infoFigure 19-3. User locations grouped into two clusters\\nChoosing k\\nIn the previous example, the choice of k was driven by factors outside of our control.\\nIn general, this won’t be the case. There is a wide variety of ways to choose a k. One\\nthat’s reasonably easy to understand involves plotting the sum of squared errors\\n(between each point and the mean of its cluster) as a function of k and looking at\\nwhere the graph “bends”:\\ndef squared_clustering_errors (inputs, k):\\n    \"\"\"finds the total squared error from k-means clustering the inputs\"\"\"\\n    clusterer  = KMeans(k)\\n    clusterer .train(inputs)\\n    means = clusterer .means\\n    assignments  = map(clusterer .classify , inputs)\\n    return sum(squared_distance (input, means[cluster])\\n               for input, cluster in zip(inputs, assignments ))\\n# now plot from 1 up to len(inputs) clusters\\nks = range(1, len(inputs) + 1)\\n230 | Chapter 19: Clustering\\nwww.it-ebooks.infoerrors = [squared_clustering_errors (inputs, k) for k in ks]\\nplt.plot(ks, errors)\\nplt.xticks(ks)\\nplt.xlabel(\"k\")\\nplt.ylabel(\"total squared error\" )\\nplt.title(\"Total Error vs. # of Clusters\" )\\nplt.show()\\nFigure 19-4. Choosing a k\\nLooking at Figure 19-4 , this method agrees with our original eyeballing that 3 is the\\n“right” number of clusters.\\nExample: Clustering Colors\\nThe VP of Swag has designed attractive DataSciencester stickers that he’ d like you to\\nhand out at meetups. Unfortunately, your sticker printer can print at most five colors\\nper sticker. And since the VP of Art is on sabbatical, the VP of Swag asks if there’s\\nsome way you can take his design and modify it so that it only contains five colors.\\nComputer images can be represented as two-dimensional array of pixels, where each\\npixel is itself a three-dimensional vector (red, green, blue)  indicating its color.\\nExample: Clustering Colors | 231\\nwww.it-ebooks.infoCreating a five-color version of the image then entails:\\n1.Choosing five colors\\n2.Assigning one of those colors to each pixel\\nIt turns out this is a great task for k-means clustering, which can partition the pixels\\ninto five clusters in red-green-blue space. If we then recolor the pixels in each cluster\\nto the mean color, we’re done.\\nTo start with, we’ll need a way to load an image into Python. It turns out we can do\\nthis with matplotlib :\\npath_to_png_file  = r\"C:\\\\images\\\\image.png\"    # wherever your image is\\nimport matplotlib.image  as mpimg\\nimg = mpimg.imread(path_to_png_file )\\nBehind the scenes img is a NumPy array, but for our purposes, we can treat it as a list\\nof lists of lists.\\nimg[i][j]  is the pixel in the ith row and jth column, and each pixel is a list [red,\\ngreen, blue]  of numbers between 0 and 1 indicating the color of that pixel :\\ntop_row = img[0]\\ntop_left_pixel  = top_row[0]\\nred, green, blue = top_left_pixel\\nIn particular, we can get a flattened list of all the pixels as:\\npixels = [pixel for row in img for pixel in row]\\nand then feed them to our clusterer:\\nclusterer  = KMeans(5)\\nclusterer .train(pixels)   # this might take a while\\nOnce it finishes, we just construct a new image with the same format:\\ndef recolor(pixel):\\n    cluster = clusterer .classify (pixel)        # index of the closest cluster\\n    return clusterer .means[cluster]            # mean of the closest cluster\\nnew_img = [[recolor(pixel) for pixel in row]   # recolor this row of pixels\\n           for row in img]                     # for each row in the image\\nand display it, using plt.imshow() :\\nplt.imshow(new_img)\\nplt.axis(\\'off\\')\\nplt.show()\\nIt is difficult to show color results in a black-and-white book, but Figure 19-5  shows\\ngrayscale versions of a full-color picture and the output of using this process to\\nreduce it to five colors:\\n232 | Chapter 19: Clustering\\nwww.it-ebooks.infoFigure 19-5. Original picture and its 5-means decoloring\\nBottom-up Hierarchical Clustering\\nAn alternative approach to clustering is to “grow” clusters from the bottom up. We\\ncan do this in the following way:\\n1.Make each input its own cluster of one.\\n2.As long as there are multiple clusters remaining, find the two closest clusters and\\nmerge them.\\nAt the end, we’ll have one giant cluster containing all the inputs. If we keep track of\\nthe merge order, we can recreate any number of clusters by unmerging. For example,\\nif we want three clusters, we can just undo the last two merges.\\nWe’ll use a really simple representation of clusters. Our values will live in leaf clusters,\\nwhich we will represent as 1-tuples:\\nleaf1 = ([10, 20],)   # to make a 1-tuple you need the trailing comma\\nleaf2 = ([30, -15],)  # otherwise Python treats the parentheses as parentheses\\nWe’ll use these to grow merged  clusters, which we will represent as 2-tuples (merge\\norder, children):\\nmerged = (1, [leaf1, leaf2])\\nWe’ll talk about merge order in a bit, but first let’s create a few helper functions:\\ndef is_leaf(cluster):\\n    \"\"\"a cluster is a leaf if it has length 1\"\"\"\\n    return len(cluster) == 1\\ndef get_children (cluster):\\nBottom-up Hierarchical Clustering | 233\\nwww.it-ebooks.info    \"\"\"returns the two children of this cluster if it\\'s a merged cluster;\\n    raises an exception if this is a leaf cluster\"\"\"\\n    if is_leaf(cluster):\\n        raise TypeError (\"a leaf cluster has no children\" )\\n    else:\\n        return cluster[1]\\ndef get_values (cluster):\\n    \"\"\"returns the value in this cluster (if it\\'s a leaf cluster)\\n    or all the values in the leaf clusters below it (if it\\'s not)\"\"\"\\n    if is_leaf(cluster):\\n        return cluster      # is already a 1-tuple containing value\\n    else:\\n        return [value\\n                for child in get_children (cluster)\\n                for value in get_values (child)]\\nIn order to merge the closest clusters, we need some notion of the distance between\\nclusters. We’ll use the minimum  distance between elements of the two clusters, which\\nmerges the two clusters that are closest to touching (but will sometimes produce large\\nchain-like clusters that aren’t very tight). If we wanted tight spherical clusters, we\\nmight use the maximum  distance instead, as it merges the two clusters that fit in the\\nsmallest ball. Both are common choices, as is the average  distance:\\ndef cluster_distance (cluster1 , cluster2 , distance_agg =min):\\n    \"\"\"compute all the pairwise distances between cluster1 and cluster2\\n    and apply _distance_agg_ to the resulting list\"\"\"\\n    return distance_agg ([distance (input1, input2)\\n                         for input1 in get_values (cluster1 )\\n                         for input2 in get_values (cluster2 )])\\nWe’ll use the merge order slot to track the order in which we did the merging. Smaller\\nnumbers will represent later  merges. This means when we want to unmerge clusters,\\nwe do so from lowest merge order to highest. Since leaf clusters were never merged\\n(which means we never want to unmerge them), we’ll assign them infinity:\\ndef get_merge_order (cluster):\\n    if is_leaf(cluster):\\n        return float(\\'inf\\')\\n    else:\\n        return cluster[0]  # merge_order is first element of 2-tuple\\nNow we’re ready to create the clustering algorithm:\\ndef bottom_up_cluster (inputs, distance_agg =min):\\n    # start with every input a leaf cluster / 1-tuple\\n    clusters  = [(input,) for input in inputs]\\n    # as long as we have more than one cluster left...\\n    while len(clusters ) > 1:\\n        # find the two closest clusters\\n        c1, c2 = min([(cluster1 , cluster2 )\\n234 | Chapter 19: Clustering\\nwww.it-ebooks.info                      for i, cluster1  in enumerate (clusters )\\n                      for cluster2  in clusters [:i]],\\n                      key=lambda (x, y): cluster_distance (x, y, distance_agg ))\\n        # remove them from the list of clusters\\n        clusters  = [c for c in clusters  if c != c1 and c != c2]\\n        # merge them, using merge_order = # of clusters left\\n        merged_cluster  = (len(clusters ), [c1, c2])\\n        # and add their merge\\n        clusters .append(merged_cluster )\\n    # when there\\'s only one cluster left, return it\\n    return clusters [0]\\nIts use is very simple:\\nbase_cluster  = bottom_up_cluster (inputs)\\nThis produces a cluster whose ugly representation is:\\n(0, [(1, [(3, [(14, [(18, [([19, 28],),\\n                           ([21, 27],)]),\\n                     ([20, 23],)]),\\n               ([26, 13],)]),\\n          (16, [([11, 15],),\\n                ([13, 13],)])]),\\n     (2, [(4, [(5, [(9, [(11, [([-49, 0],),\\n                               ([-46, 5],)]),\\n                         ([-41, 8],)]),\\n                    ([-49, 15],)]),\\n               ([-34, -1],)]),\\n          (6, [(7, [(8, [(10, [([-22, -16],),\\n                               ([-19, -11],)]),\\n                         ([-25, -9],)]),\\n                    (13, [(15, [(17, [([-11, -6],),\\n                                      ([-12, -8],)]),\\n                                ([-14, -5],)]),\\n                          ([-18, -3],)])]),\\n               (12, [([-13, -19],),\\n                     ([-9, -16],)])])])])\\nFor every merged cluster, I lined up its children vertically. If we say “cluster 0” for the\\ncluster with merge order 0, you can interpret this as:\\n•Cluster 0 is the merger of cluster 1 and cluster 2.\\n•Cluster 1 is the merger of cluster 3 and cluster 16.\\n•Cluster 16 is the merger of the leaf [11, 15]  and the leaf [13, 13] .\\n•And so on…\\nBottom-up Hierarchical Clustering | 235\\nwww.it-ebooks.infoSince we had 20 inputs, it took 19 merges to get to this one cluster. The first merge\\ncreated cluster 18 by combining the leaves [19, 28]  and [21, 27] . And the last\\nmerge created cluster 0.\\nGenerally, though, we don’t want to be squinting at nasty text representations like\\nthis. (Although it could be an interesting exercise to create a user-friendlier visualiza‐\\ntion of the cluster hierarchy.) Instead let’s write a function that generates any number\\nof clusters by performing the appropriate number of unmerges:\\ndef generate_clusters (base_cluster , num_clusters ):\\n    # start with a list with just the base cluster\\n    clusters  = [base_cluster ]\\n    # as long as we don\\'t have enough clusters yet...\\n    while len(clusters ) < num_clusters :\\n        # choose the last-merged of our clusters\\n        next_cluster  = min(clusters , key=get_merge_order )\\n        # remove it from the list\\n        clusters  = [c for c in clusters  if c != next_cluster ]\\n        # and add its children to the list (i.e., unmerge it)\\n        clusters .extend(get_children (next_cluster ))\\n    # once we have enough clusters...\\n    return clusters\\nSo, for example, if we want to generate three clusters, we can just do:\\nthree_clusters  = [get_values (cluster)\\n                  for cluster in generate_clusters (base_cluster , 3)]\\nwhich we can easily plot:\\nfor i, cluster, marker, color in zip([1, 2, 3],\\n                                     three_clusters ,\\n                                     [\\'D\\',\\'o\\',\\'*\\'],\\n                                     [\\'r\\',\\'g\\',\\'b\\']):\\n    xs, ys = zip(*cluster)  # magic unzipping trick\\n    plt.scatter(xs, ys, color=color, marker=marker)\\n    # put a number at the mean of the cluster\\n    x, y = vector_mean (cluster)\\n    plt.plot(x, y, marker=\\'$\\' + str(i) + \\'$\\', color=\\'black\\')\\nplt.title(\"User Locations -- 3 Bottom-Up Clusters, Min\" )\\nplt.xlabel(\"blocks east of city center\" )\\nplt.ylabel(\"blocks north of city center\" )\\nplt.show()\\nThis gives very different results than k-means did, as shown in Figure 19-6 .\\n236 | Chapter 19: Clustering\\nwww.it-ebooks.infoFigure 19-6. Three  bottom-up clusters using min distance\\nAs we mentioned above, this is because using min in cluster_distance  tends to give\\nchain-like clusters. If we instead use max (which gives tight clusters) it looks the same\\nas the 3-means result ( Figure 19-7 ).\\nThe bottom_up_clustering  implementation above is relatively\\nsimple, but it’s also shockingly inefficient. In particular, it recom‐\\nputes the distance between each pair of inputs at every step. A\\nmore efficient implementation might precompute the distances\\nbetween each pair of inputs and then perform a lookup inside clus‐\\nter_distance. A really  efficient implementation would likely also\\nremember the cluster_distance s from the previous step.\\nBottom-up Hierarchical Clustering | 237\\nwww.it-ebooks.infoFigure 19-7. Three  bottom-up clusters using max distance\\nFor Further Exploration\\n•scikit-learn  has an entire module sklearn.cluster  that contains several clus‐\\ntering algorithms including KMeans  and the Ward  hierarchical clustering algo‐\\nrithm (which uses a different criterion for merging clusters than ours did).\\n•SciPy  has two clustering models scipy.cluster.vq  (which does k-means) and\\nscipy.cluster.hierarchy  (which has a variety of hierarchical clustering algo‐\\nrithms).\\n238 | Chapter 19: Clustering\\nwww.it-ebooks.infoCHAPTER 20\\nNatural Language Processing\\nThey  have been at a great feast of languages, and stolen the scraps.\\n—William Shakespeare\\nNatural language processing  (NLP) refers to computational techniques involving lan‐\\nguage. It’s a broad field, but we’ll look at a few techniques both simple and not simple.\\nWord Clouds\\nIn Chapter 1 , we computed word counts of users’ interests. One approach to visualiz‐\\ning words and counts is word clouds, which artistically lay out the words with sizes\\nproportional to their counts.\\nGenerally, though, data scientists don’t think much of word clouds, in large part\\nbecause the placement of the words doesn’t mean anything other than “here’s some\\nspace where I was able to fit a word. ”\\nIf you ever are forced to create a word cloud, think about whether you can make the\\naxes convey something. For example, imagine that, for each of some collection of data\\nscience–related buzzwords, you have two numbers between 0 and 100—the first rep‐\\nresenting how frequently it appears in job postings, the second how frequently it\\nappears on resumes:\\ndata = [ (\"big data\" , 100, 15), (\"Hadoop\" , 95, 25), (\"Python\" , 75, 50),\\n         (\"R\", 50, 40), (\"machine learning\" , 80, 20), (\"statistics\" , 20, 60),\\n         (\"data science\" , 60, 70), (\"analytics\" , 90, 3),\\n         (\"team player\" , 85, 85), (\"dynamic\" , 2, 90), (\"synergies\" , 70, 0),\\n         (\"actionable insights\" , 40, 30), (\"think out of the box\" , 45, 10),\\n         (\"self-starter\" , 30, 50), (\"customer focus\" , 65, 15),\\n         (\"thought leadership\" , 35, 35)]\\n239\\nwww.it-ebooks.infoThe word cloud approach is just to arrange the words on a page in a cool-looking\\nfont ( Figure 20-1 ).\\nFigure 20-1. Buzzword cloud\\nThis looks neat but doesn’t really tell us anything. A more interesting approach might\\nbe to scatter them so that horizontal position indicates posting popularity and vertical\\nposition indicates resume popularity, which produces a visualization that conveys a\\nfew insights ( Figure 20-2 ):\\ndef text_size (total):\\n    \"\"\"equals 8 if total is 0, 28 if total is 200\"\"\"\\n    return 8 + total / 200 * 20\\nfor word, job_popularity , resume_popularity  in data:\\n    plt.text(job_popularity , resume_popularity , word,\\n             ha=\\'center\\' , va=\\'center\\' ,\\n             size=text_size (job_popularity  + resume_popularity ))\\nplt.xlabel(\"Popularity on Job Postings\" )\\nplt.ylabel(\"Popularity on Resumes\" )\\nplt.axis([0, 100, 0, 100])\\nplt.xticks([])\\nplt.yticks([])\\nplt.show()\\n240 | Chapter 20: Natural Language Processing\\nwww.it-ebooks.infoFigure 20-2. A more meaningful (if less attractive) word cloud\\nn-gram Models\\nThe DataSciencester VP of Search Engine Marketing wants to create thousands of\\nweb pages about data science so that your site will rank higher in search results for\\ndata science–related terms. (Y ou attempt to explain to her that search engine algo‐\\nrithms are clever enough that this won’t actually work, but she refuses to listen.)\\nOf course, she doesn’t want to write thousands of web pages, nor does she want to pay\\na horde of “content strategists” to do so. Instead she asks you whether you can some‐\\nhow programatically generate these web pages. To do this, we’ll need some way of\\nmodeling language.\\nOne approach is to start with a corpus of documents and learn a statistical model of\\nlanguage. In our case, we’ll start with Mike Loukides’s essay “What is data science?”\\nAs in Chapter 9 , we’ll use requests  and BeautifulSoup  to retrieve the data. There are\\na couple of issues worth calling attention to.\\nThe first is that the apostrophes in the text are actually the Unicode character\\nu\"\\\\u2019\" . We’ll create a helper function to replace them with normal apostrophes:\\nn-gram Models | 241\\nwww.it-ebooks.infodef fix_unicode (text):\\n    return text.replace(u\"\\\\u2019\", \"\\'\")\\nThe second issue is that once we get the text of the web page, we’ll want to split it into\\na sequence of words and periods (so that we can tell where sentences end). We can do\\nthis using re.findall() :\\nfrom bs4 import BeautifulSoup\\nimport requests\\nurl = \"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\\nhtml = requests .get(url).text\\nsoup = BeautifulSoup (html, \\'html5lib\\' )\\ncontent = soup.find(\"div\", \"entry-content\" )   # find entry-content div\\nregex = r\"[\\\\w\\']+|[\\\\.]\"                         # matches a word or a period\\ndocument  = []\\nfor paragraph  in content(\"p\"):\\n    words = re.findall(regex, fix_unicode (paragraph .text))\\n    document .extend(words)\\nWe certainly could (and likely should) clean this data further. There is still some\\namount of extraneous text in the document (for example, the first word is “Section”),\\nand we’ve split on midsentence periods (for example, in “Web 2.0”), and there are a\\nhandful of captions and lists sprinkled throughout. Having said that, we’ll work with\\nthe document  as it is.\\nNow that we have the text as a sequence of words, we can model language in the fol‐\\nlowing way: given some starting word (say “book”) we look at all the words that fol‐\\nlow it in the source documents (here “isn’t, ” “a, ” “shows, ” “demonstrates, ” and\\n“teaches”). We randomly choose one of these to be the next word, and we repeat the\\nprocess until we get to a period, which signifies the end of the sentence. We call this a\\nbigram model , as it is determined completely by the frequencies of the bigrams (word\\npairs) in the original data.\\nWhat about a starting word? We can just pick randomly from words that follow  a\\nperiod. To start, let’s precompute the possible word transitions. Recall that zip stops\\nwhen any of its inputs is done, so that zip(document, document[1:])  gives us pre‐\\ncisely the pairs of consecutive elements of document :\\nbigrams = zip(document , document [1:])\\ntransitions  = defaultdict (list)\\nfor prev, current in bigrams:\\n    transitions [prev].append(current)\\nNow we’re ready to generate sentences:\\ndef generate_using_bigrams ():\\n    current = \".\"   # this means the next word will start a sentence\\n    result = []\\n242 | Chapter 20: Natural Language Processing\\nwww.it-ebooks.info    while True:\\n        next_word_candidates  = transitions [current]    # bigrams (current, _)\\n        current = random.choice(next_word_candidates )  # choose one at random\\n        result.append(current)                         # append it to results\\n        if current == \".\": return \" \".join(result)     # if \".\" we\\'re done\\nThe sentences it produces are gibberish, but they’re the kind of gibberish you might\\nput on your website if you were trying to sound data-sciencey. For example:\\nIf you may know which are you want to data sort the data feeds web friend someone on\\ntrending topics as the data in Hadoop is the data science requires a book demonstrates\\nwhy visualizations are but we do massive correlations across many commercial disk\\ndrives in Python language and creates more tractable form making connections then\\nuse and uses it to solve a data.\\n—Bigram Model\\nWe can make the  sentences less gibberishy by looking at trigrams , triplets of consecu‐\\ntive words. (More generally, you might look at n-grams  consisting of n consecutive\\nwords, but three will be plenty for us.) Now the transitions will depend on the previ‐\\nous two words:\\ntrigrams  = zip(document , document [1:], document [2:])\\ntrigram_transitions  = defaultdict (list)\\nstarts = []\\nfor prev, current, next in trigrams :\\n    if prev == \".\":              # if the previous \"word\" was a period\\n        starts.append(current)   # then this is a start word\\n    trigram_transitions [(prev, current)].append(next)\\nNotice that now we have to track the starting words separately. We can generate sen‐\\ntences in pretty much the same way:\\ndef generate_using_trigrams ():\\n    current = random.choice(starts)   # choose a random starting word\\n    prev = \".\"                        # and precede it with a \\'.\\'\\n    result = [current]\\n    while True:\\n        next_word_candidates  = trigram_transitions [(prev, current)]\\n        next_word  = random.choice(next_word_candidates )\\n        prev, current = current, next_word\\n        result.append(current)\\n        if current == \".\":\\n            return \" \".join(result)\\nThis produces better sentences like:\\nn-gram Models | 243\\nwww.it-ebooks.infoIn hindsight MapReduce seems like an epidemic and if so does that give us new\\ninsights into how economies work That’s not a question we could even have asked a\\nfew years there has been instrumented.\\n—Trigram Model\\nOf course, they sound better because at each step the generation process has fewer\\nchoices, and at many steps only a single choice. This means that you frequently gen‐\\nerate sentences (or at least long phrases) that were seen verbatim in the original data.\\nHaving more data would help; it would also work better if you collected n-grams\\nfrom multiple essays about data science.\\nGrammars\\nA different approach to modeling language is with grammars , rules for generating\\nacceptable sentences. In elementary school, you probably learned about parts of\\nspeech and how to combine them. For example, if you had a really bad English\\nteacher, you might say that a sentence necessarily consists of a noun  followed by a\\nverb. If you then have a list of nouns and verbs, you can generate sentences according\\nto the rule.\\nWe’ll define a slightly more complicated grammar:\\ngrammar = {\\n    \"_S\"  : [\"_NP _VP\" ],\\n    \"_NP\" : [\"_N\",\\n             \"_A _NP _P _A _N\" ],\\n    \"_VP\" : [\"_V\",\\n             \"_V _NP\" ],\\n    \"_N\"  : [\"data science\" , \"Python\" , \"regression\" ],\\n    \"_A\"  : [\"big\", \"linear\" , \"logistic\" ],\\n    \"_P\"  : [\"about\", \"near\"],\\n    \"_V\"  : [\"learns\" , \"trains\" , \"tests\", \"is\"]\\n}\\nI made up the convention that names starting with underscores refer to rules  that\\nneed further expanding, and that other names are terminals  that don’t need further\\nprocessing.\\nSo, for example, \"_S\"  is the “sentence” rule, which produces a \"_NP\"  (“noun phrase”)\\nrule followed by a \"_VP\"  (“verb phrase”) rule.\\nThe verb phrase rule can produce either the \"_V\"  (“verb”) rule, or the verb rule fol‐\\nlowed by the noun phrase rule.\\nNotice that the \"_NP\"  rule contains itself in one of its productions. Grammars can be\\nrecursive, which allows even finite grammars like this to generate infinitely many dif‐\\nferent sentences.\\n244 | Chapter 20: Natural Language Processing\\nwww.it-ebooks.infoHow do we generate sentences from this grammar? We’ll start with a list containing\\nthe sentence rule [\"_S\"] . And then we’ll repeatedly expand each rule by replacing it\\nwith a randomly chosen one of its productions. We stop when we have a list consist‐\\ning solely of terminals.\\nFor example, one such progression might look like:\\n[\\'_S\\']\\n[\\'_NP\\',\\'_VP\\']\\n[\\'_N\\',\\'_VP\\']\\n[\\'Python\\' ,\\'_VP\\']\\n[\\'Python\\' ,\\'_V\\',\\'_NP\\']\\n[\\'Python\\' ,\\'trains\\' ,\\'_NP\\']\\n[\\'Python\\' ,\\'trains\\' ,\\'_A\\',\\'_NP\\',\\'_P\\',\\'_A\\',\\'_N\\']\\n[\\'Python\\' ,\\'trains\\' ,\\'logistic\\' ,\\'_NP\\',\\'_P\\',\\'_A\\',\\'_N\\']\\n[\\'Python\\' ,\\'trains\\' ,\\'logistic\\' ,\\'_N\\',\\'_P\\',\\'_A\\',\\'_N\\']\\n[\\'Python\\' ,\\'trains\\' ,\\'logistic\\' ,\\'data science\\' ,\\'_P\\',\\'_A\\',\\'_N\\']\\n[\\'Python\\' ,\\'trains\\' ,\\'logistic\\' ,\\'data science\\' ,\\'about\\',\\'_A\\', \\'_N\\']\\n[\\'Python\\' ,\\'trains\\' ,\\'logistic\\' ,\\'data science\\' ,\\'about\\',\\'logistic\\' ,\\'_N\\']\\n[\\'Python\\' ,\\'trains\\' ,\\'logistic\\' ,\\'data science\\' ,\\'about\\',\\'logistic\\' ,\\'Python\\' ]\\nHow do we implement this? Well, to start, we’ll create a simple helper function to\\nidentify terminals:\\ndef is_terminal (token):\\n    return token[0] != \"_\"\\nNext we need to write a function to turn a list of tokens into a sentence. We’ll look for\\nthe first nonterminal token. If we can’t find one, that means we have a completed sen‐\\ntence and we’re done.\\nIf we do find a nonterminal, then we randomly choose one of its productions. If that\\nproduction is a terminal (i.e., a word), we simply replace the token with it. Otherwise\\nit’s a sequence of space-separated nonterminal tokens that we need to split  and then\\nsplice into the current tokens. Either way, we repeat the process on the new set of\\ntokens.\\nPutting it all together we get:\\ndef expand(grammar, tokens):\\n    for i, token in enumerate (tokens):\\n        # skip over terminals\\n        if is_terminal (token): continue\\n        # if we get here, we found a non-terminal token\\n        # so we need to choose a replacement at random\\n        replacement  = random.choice(grammar[token])\\n        if is_terminal (replacement ):\\n            tokens[i] = replacement\\n        else:\\nGrammars | 245\\nwww.it-ebooks.info            tokens = tokens[:i] + replacement .split() + tokens[(i+1):]\\n        # now call expand on the new list of tokens\\n        return expand(grammar, tokens)\\n    # if we get here we had all terminals and are done\\n    return tokens\\nAnd now we can start generating sentences:\\ndef generate_sentence (grammar):\\n    return expand(grammar, [\"_S\"])\\nTry changing the grammar—add more words, add more rules, add your own parts of\\nspeech—until you’re ready to generate as many web pages as your company needs.\\nGrammars are actually more interesting when they’re used in the other direction.\\nGiven a sentence we can use a grammar to parse  the sentence. This then allows us to\\nidentify subjects and verbs and helps us make sense of the sentence.\\nUsing data science to generate text is a neat trick; using it to understand  text is more\\nmagical. (See “For Further Investigation” on page 200 for libraries that you could use\\nfor this.)\\nAn Aside: Gibbs Sampling\\nGenerating samples from some distributions is easy. We can get uniform random\\nvariables with:\\nrandom.random()\\nand normal random variables with:\\ninverse_normal_cdf (random.random())\\nBut some distributions are harder to sample from. Gibbs sampling  is a technique  for\\ngenerating samples from multidimensional distributions when we only know some of\\nthe conditional distributions.\\nFor example, imagine rolling two dice. Let x be the value of the first die and y be the\\nsum of the dice, and imagine you wanted to generate lots of (x, y) pairs. In this case\\nit’s easy to generate the samples directly:\\ndef roll_a_die ():\\n    return random.choice([1,2,3,4,5,6])\\ndef direct_sample ():\\n    d1 = roll_a_die ()\\n    d2 = roll_a_die ()\\n    return d1, d1 + d2\\n246 | Chapter 20: Natural Language Processing\\nwww.it-ebooks.infoBut imagine that you only knew the conditional distributions. The distribution of y\\nconditional on x is easy—if you know the value of x, y is equally likely to be x + 1, x +\\n2, x + 3, x + 4, x + 5, or x + 6:\\ndef random_y_given_x (x):\\n    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\"\\n    return x + roll_a_die ()\\nThe other direction is more complicated. For example, if you know that y is 2, then\\nnecessarily x is 1 (since the only way two dice can sum to 2 is if both of them are 1). If\\nyou know y is 3, then x is equally likely to be 1 or 2. Similarly, if y is 11, then x has to\\nbe either 5 or 6:\\ndef random_x_given_y (y):\\n    if y <= 7:\\n        # if the total is 7 or less, the first die is equally likely to be\\n        # 1, 2, ..., (total - 1)\\n        return random.randrange (1, y)\\n    else:\\n        # if the total is 7 or more, the first die is equally likely to be\\n        # (total - 6), (total - 5), ..., 6\\n        return random.randrange (y - 6, 7)\\nThe way Gibbs sampling works is that we start with any (valid) value for x and y and\\nthen repeatedly alternate replacing x with a random value picked conditional on y\\nand replacing y with a random value picked conditional on x. After a number of iter‐\\nations, the resulting values of x and y will represent a sample from the unconditional\\njoint distribution:\\ndef gibbs_sample (num_iters =100):\\n    x, y = 1, 2 # doesn\\'t really matter\\n    for _ in range(num_iters ):\\n        x = random_x_given_y (y)\\n        y = random_y_given_x (x)\\n    return x, y\\nY ou can check that this gives similar results to the direct sample:\\ndef compare_distributions (num_samples =1000):\\n    counts = defaultdict (lambda: [0, 0])\\n    for _ in range(num_samples ):\\n        counts[gibbs_sample ()][0] += 1\\n        counts[direct_sample ()][1] += 1\\n    return counts\\nWe’ll use this technique in the next section.\\nTopic Modeling\\nWhen we built our Data Scientists Y ou Should Know recommender in Chapter 1 , we\\nsimply looked for exact matches in people’s stated interests.\\nTopic Modeling | 247\\nwww.it-ebooks.infoA more sophisticated approach to understanding our users’ interests might try to\\nidentify the topics  that underlie those interests. A technique called Latent Dirichlet\\nAnalysis  (LDA) is commonly used to identify common topics in a set of documents.\\nWe’ll apply it to documents that consist of each user’s interests.\\nLDA has some similarities to the Naive Bayes Classifier we built in Chapter 13 , in that\\nit assumes a probabilistic model for documents. We’ll gloss over the hairier mathe‐\\nmatical details, but for our purposes the model assumes that:\\n•There is some fixed number K of topics.\\n•There is a random variable that assigns each topic an associated probability dis‐\\ntribution over words. Y ou should think of this distribution as the probability of\\nseeing word w given topic k.\\n•There is another random variable that assigns each document a probability dis‐\\ntribution over topics. Y ou should think of this distribution as the mixture of top‐\\nics in document d.\\n•Each word in a document was generated by first randomly picking a topic (from\\nthe document’s distribution of topics) and then randomly picking a word (from\\nthe topic’s distribution of words).\\nIn particular, we have a collection of documents  each of which is a list  of words.\\nAnd we have a corresponding collection of document_topics  that assigns a topic\\n(here a number between 0 and K – 1) to each word in each document.\\nSo that the fifth word in the fourth document is:\\ndocuments [3][4]\\nand the topic from which that word was chosen is:\\ndocument_topics [3][4]\\nThis very explicitly defines each document’s distribution over topics, and it implicitly\\ndefines each topic’s distribution over words.\\nWe can estimate the likelihood that topic 1 produces a certain word by comparing\\nhow many times topic 1 produces that word with how many times topic 1 produces\\nany word. (Similarly, when we built a spam filter in Chapter 13 , we compared how\\nmany times each word appeared in spams with the total number of words appearing\\nin spams.)\\nAlthough these topics are just numbers, we can give them descriptive names by look‐\\ning at the words on which they put the heaviest weight. We just have to somehow\\ngenerate the document_topics . This is where Gibbs sampling comes into play.\\nWe start by assigning every word in every document a topic completely at random.\\nNow we go through each document one word at a time. For that word and document,\\n248 | Chapter 20: Natural Language Processing\\nwww.it-ebooks.infowe construct weights for each topic that depend on the (current) distribution of top‐\\nics in that document and the (current) distribution of words for that topic. We then\\nuse those weights to sample a new topic for that word. If we iterate this process many\\ntimes, we will end up with a joint sample from the topic-word distribution and the\\ndocument-topic distribution.\\nTo start with, we’ll need a function to randomly choose an index based on an arbi‐\\ntrary set of weights:\\ndef sample_from (weights):\\n    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\\n    total = sum(weights)\\n    rnd = total * random.random()      # uniform between 0 and total\\n    for i, w in enumerate (weights):\\n        rnd -= w                       # return the smallest i such that\\n        if rnd <= 0: return i          # weights[0] + ... + weights[i] >= rnd\\nFor instance, if you give it weights [1, 1, 3]  then one-fifth of the time it will return\\n0, one-fifth of the time it will return 1, and three-fifths of the time it will return 2.\\nOur documents are our users’ interests, which look like:\\ndocuments  = [\\n    [\"Hadoop\" , \"Big Data\" , \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\" ],\\n    [\"NoSQL\", \"MongoDB\" , \"Cassandra\" , \"HBase\", \"Postgres\" ],\\n    [\"Python\" , \"scikit-learn\" , \"scipy\", \"numpy\", \"statsmodels\" , \"pandas\" ],\\n    [\"R\", \"Python\" , \"statistics\" , \"regression\" , \"probability\" ],\\n    [\"machine learning\" , \"regression\" , \"decision trees\" , \"libsvm\" ],\\n    [\"Python\" , \"R\", \"Java\", \"C++\", \"Haskell\" , \"programming languages\" ],\\n    [\"statistics\" , \"probability\" , \"mathematics\" , \"theory\" ],\\n    [\"machine learning\" , \"scikit-learn\" , \"Mahout\" , \"neural networks\" ],\\n    [\"neural networks\" , \"deep learning\" , \"Big Data\" , \"artificial intelligence\" ],\\n    [\"Hadoop\" , \"Java\", \"MapReduce\" , \"Big Data\" ],\\n    [\"statistics\" , \"R\", \"statsmodels\" ],\\n    [\"C++\", \"deep learning\" , \"artificial intelligence\" , \"probability\" ],\\n    [\"pandas\" , \"R\", \"Python\" ],\\n    [\"databases\" , \"HBase\", \"Postgres\" , \"MySQL\", \"MongoDB\" ],\\n    [\"libsvm\" , \"regression\" , \"support vector machines\" ]\\n]\\nAnd we’ll try to find K = 4  topics.\\nIn order to calculate the sampling weights, we’ll need to keep track of several counts.\\nLet’s first create the data structures for them.\\nHow many times each topic is assigned to each document:\\n# a list of Counters, one for each document\\ndocument_topic_counts  = [Counter() for _ in documents ]\\nHow many times each word is assigned to each topic:\\nTopic Modeling | 249\\nwww.it-ebooks.info# a list of Counters, one for each topic\\ntopic_word_counts  = [Counter() for _ in range(K)]\\nThe total number of words assigned to each topic:\\n# a list of numbers, one for each topic\\ntopic_counts  = [0 for _ in range(K)]\\nThe total number of words contained in each document:\\n# a list of numbers, one for each document\\ndocument_lengths  = map(len, documents )\\nThe number of distinct words:\\ndistinct_words  = set(word for document  in documents  for word in document )\\nW = len(distinct_words )\\nAnd the number of documents:\\nD = len(documents )\\nFor example, once we populate these, we can find, for example, the number of words\\nin documents[3]  associated with topic 1 as:\\ndocument_topic_counts [3][1]\\nAnd we can find the number of times nlp is associated with topic 2 as:\\ntopic_word_counts [2][\"nlp\"]\\nNow we’re ready to define our conditional probability functions. As in Chapter 13 ,\\neach has a smoothing term that ensures every topic has a nonzero chance of being\\nchosen in any document and that every word has a nonzero chance of being chosen\\nfor any topic:\\ndef p_topic_given_document (topic, d, alpha=0.1):\\n    \"\"\"the fraction of words in document _d_\\n    that are assigned to _topic_ (plus some smoothing)\"\"\"\\n    return ((document_topic_counts [d][topic] + alpha) /\\n            (document_lengths [d] + K * alpha))\\ndef p_word_given_topic (word, topic, beta=0.1):\\n    \"\"\"the fraction of words assigned to _topic_\\n    that equal _word_ (plus some smoothing)\"\"\"\\n    return ((topic_word_counts [topic][word] + beta) /\\n            (topic_counts [topic] + W * beta))\\nWe’ll use these to create the weights for updating topics:\\ndef topic_weight (d, word, k):\\n    \"\"\"given a document and a word in that document,\\n    return the weight for the kth topic\"\"\"\\n250 | Chapter 20: Natural Language Processing\\nwww.it-ebooks.info    return p_word_given_topic (word, k) * p_topic_given_document (k, d)\\ndef choose_new_topic (d, word):\\n    return sample_from ([topic_weight (d, word, k)\\n                        for k in range(K)])\\nThere are solid mathematical reasons why topic_weight  is defined the way it is, but\\ntheir details would lead us too far afield. Hopefully it makes at least intuitive sense\\nthat—given a word and its document—the likelihood of any topic choice depends on\\nboth how likely that topic is for the document and how likely that word is for the\\ntopic.\\nThis is all the machinery we need. We start by assigning every word to a random\\ntopic, and populating our counters appropriately:\\nrandom.seed(0)\\ndocument_topics  = [[random.randrange (K) for word in document ]\\n                   for document  in documents ]\\nfor d in range(D):\\n    for word, topic in zip(documents [d], document_topics [d]):\\n        document_topic_counts [d][topic] += 1\\n        topic_word_counts [topic][word] += 1\\n        topic_counts [topic] += 1\\nOur goal is to get a joint sample of the topics-words distribution and the documents-\\ntopics distribution. We do this using a form of Gibbs sampling that uses the condi‐\\ntional probabilities defined previously:\\nfor iter in range(1000):\\n    for d in range(D):\\n        for i, (word, topic) in enumerate (zip(documents [d],\\n                                              document_topics [d])):\\n            # remove this word / topic from the counts\\n            # so that it doesn\\'t influence the weights\\n            document_topic_counts [d][topic] -= 1\\n            topic_word_counts [topic][word] -= 1\\n            topic_counts [topic] -= 1\\n            document_lengths [d] -= 1\\n            # choose a new topic based on the weights\\n            new_topic  = choose_new_topic (d, word)\\n            document_topics [d][i] = new_topic\\n            # and now add it back to the counts\\n            document_topic_counts [d][new_topic ] += 1\\n            topic_word_counts [new_topic ][word] += 1\\n            topic_counts [new_topic ] += 1\\n            document_lengths [d] += 1\\nTopic Modeling | 251\\nwww.it-ebooks.infoWhat are the topics? They’re just numbers 0, 1, 2, and 3. If we want names for them\\nwe have to do that ourselves. Let’s look at the five most heavily weighted words for\\neach ( Table 20-1 ):\\nfor k, word_counts  in enumerate (topic_word_counts ):\\n    for word, count in word_counts .most_common ():\\n        if count > 0: print k, word, count\\nTable 20-1. Most common words per topic\\nTopic 0 Topic 1 Topic 2 Topic 3\\nJava R HBase regression\\nBig Data statistics Postgres libsvm\\nHadoop Python MongoDB scikit-learn\\ndeep learning probability Cassandra machine learning\\nartificial  intelligence pandas NoSQL neural networks\\nBased on these I’ d probably assign topic names:\\ntopic_names  = [\"Big Data and programming languages\" ,\\n               \"Python and statistics\" ,\\n               \"databases\" ,\\n               \"machine learning\" ]\\nat which point we can see how the model assigns topics to each user’s interests:\\nfor document , topic_counts  in zip(documents , document_topic_counts ):\\n    print document\\n    for topic, count in topic_counts .most_common ():\\n        if count > 0:\\n            print topic_names [topic], count,\\n    print\\nwhich gives:\\n[\\'Hadoop\\', \\'Big Data\\', \\'HBase\\', \\'Java\\', \\'Spark\\', \\'Storm\\', \\'Cassandra\\']\\nBig Data and programming languages 4 databases 3\\n[\\'NoSQL\\', \\'MongoDB\\', \\'Cassandra\\', \\'HBase\\', \\'Postgres\\']\\ndatabases 5\\n[\\'Python\\', \\'scikit-learn\\', \\'scipy\\', \\'numpy\\', \\'statsmodels\\', \\'pandas\\']\\nPython and statistics 5 machine learning 1\\nand so on. Given the “ands” we needed in some of our topic names, it’s possible we\\nshould use more topics, although most likely we don’t have enough data to success‐\\nfully learn them.\\n252 | Chapter 20: Natural Language Processing\\nwww.it-ebooks.infoFor Further Exploration\\n•Natural Language Toolkit  is a popular (and pretty comprehensive) library of NLP\\ntools for Python. It has its own entire book , which is available to read online.\\n•gensim  is a Python library for topic modeling, which is a better bet than our\\nfrom-scratch model.\\nFor Further Exploration | 253\\nwww.it-ebooks.infowww.it-ebooks.infoCHAPTER 21\\nNetwork Analysis\\nYour connections to all the things around you literally define  who you are.\\n—Aaron O’Connell\\nMany interesting data problems can be fruitfully thought of in terms of networks ,\\nconsisting of nodes  of some type and the edges  that join them.\\nFor instance, your Facebook friends form the nodes of a network whose edges are\\nfriendship relations. A less obvious example is the World Wide Web itself, with each\\nweb page a node, and each hyperlink from one page to another an edge.\\nFacebook friendship is mutual—if I am Facebook friends with you than necessarily\\nyou are friends with me. In this case, we say that the edges are undirected . Hyperlinks\\nare not—my website links to whitehouse.gov, but (for reasons inexplicable to me)\\nwhitehouse.gov refuses to link to my website. We call these types of edges directed .\\nWe’ll look at both kinds of networks.\\nBetweenness Centrality\\nIn Chapter 1 , we computed the key connectors in the DataSciencester network by\\ncounting the number of friends each user had. Now we have enough machinery to\\nlook at other approaches. Recall that the network ( Figure 21-1 ) comprised users:\\nusers = [\\n    { \"id\": 0, \"name\": \"Hero\" },\\n    { \"id\": 1, \"name\": \"Dunn\" },\\n    { \"id\": 2, \"name\": \"Sue\" },\\n    { \"id\": 3, \"name\": \"Chi\" },\\n    { \"id\": 4, \"name\": \"Thor\" },\\n    { \"id\": 5, \"name\": \"Clive\" },\\n    { \"id\": 6, \"name\": \"Hicks\" },\\n    { \"id\": 7, \"name\": \"Devin\" },\\n255\\nwww.it-ebooks.info    { \"id\": 8, \"name\": \"Kate\" },\\n    { \"id\": 9, \"name\": \"Klein\" }\\n]\\nand friendships:\\nfriendships  = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\\n               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\\nFigure 21-1. The DataSciencester network\\nWe also added friend lists to each user dict :\\nfor user in users:\\n    user[\"friends\" ] = []\\nfor i, j in friendships :\\n    # this works because users[i] is the user whose id is i\\n    users[i][\"friends\" ].append(users[j]) # add i as a friend of j\\n    users[j][\"friends\" ].append(users[i]) # add j as a friend of i\\nWhen we left off we were dissatisfied with our notion of degree centrality , which\\ndidn’t really agree with our intuition about who were the key connectors of the net‐\\nwork.\\nAn alternative metric is betweenness centrality , which identifies people who frequently\\nare on the shortest paths between pairs of other people. In particular, the betweenness\\ncentrality of node i is computed by adding up, for every other pair of nodes j and k,\\nthe proportion of shortest paths between node j and node k that pass through i.\\nThat is, to figure out Thor’s betweenness centrality, we’ll need to compute all the\\nshortest paths between all pairs of people who aren’t Thor. And then we’ll need to\\ncount how many of those shortest paths pass through Thor. For instance, the only\\nshortest path between Chi ( id 3) and Clive ( id 5) passes through Thor, while neither\\nof the two shortest paths between Hero ( id 0) and Chi ( id 3) does.\\nSo, as a first step, we’ll need to figure out the shortest paths between all pairs of peo‐\\nple. There are some pretty sophisticated algorithms for doing so efficiently, but (as is\\nalmost always the case) we will use a less efficient, easier-to-understand algorithm.\\n256 | Chapter 21: Network Analysis\\nwww.it-ebooks.infoThis algorithm (an implementation of breadth-first search) is one of the more com‐\\nplicated ones in the book, so let’s talk through it carefully:\\n1.Our goal is a function that takes a from_user  and finds all shortest paths to every\\nother user.\\n2.We’ll represent a path as list  of user IDs. Since every path starts at from_user ,\\nwe won’t include her ID in the list. This means that the length of the list repre‐\\nsenting the path will be the length of the path itself.\\n3.We’ll maintain a dictionary shortest_paths_to  where the keys are user IDs and\\nthe values are lists of paths that end at the user with the specified ID. If there is a\\nunique shortest path, the list will just contain that one path. If there are multiple\\nshortest paths, the list will contain all of them.\\n4.We’ll also maintain a queue frontier  that contains the users we want to explore\\nin the order we want to explore them. We’ll store them as pairs (prev_user,\\nuser)  so that we know how we got to each one. We initialize the queue with all\\nthe neighbors of from_user . (We haven’t ever talked about queues, which are data\\nstructures optimized for “add to the end” and “remove from the front” opera‐\\ntions. In Python, they are implemented as collections.deque  which is actually a\\ndouble-ended queue.)\\n5.As we explore the graph, whenever we find new neighbors that we don’t already\\nknow shortest paths to, we add them to the end of the queue to explore later, with\\nthe current user as prev_user .\\n6.When we take a user off the queue, and we’ve never encountered that user before,\\nwe’ve definitely found one or more shortest paths to him—each of the shortest\\npaths to prev_user  with one extra step added.\\n7.When we take a user off the queue and we have  encountered that user before,\\nthen either we’ve found another shortest path (in which case we should add it) or\\nwe’ve found a longer path (in which case we shouldn’t).\\n8.When no more users are left on the queue, we’ve explored the whole graph (or, at\\nleast, the parts of it that are reachable from the starting user) and we’re done.\\nWe can put this all together into a (large) function:\\nfrom collections  import deque\\ndef shortest_paths_from (from_user ):\\n    # a dictionary from \"user_id\" to *all* shortest paths to that user\\n    shortest_paths_to  = { from_user [\"id\"] : [[]] }\\n    # a queue of (previous user, next user) that we need to check.\\n    # starts out with all pairs (from_user, friend_of_from_user)\\nBetweenness Centrality | 257\\nwww.it-ebooks.info    frontier  = deque((from_user , friend)\\n                     for friend in from_user [\"friends\" ])\\n    # keep going until we empty the queue\\n    while frontier :\\n        prev_user , user = frontier .popleft()   # remove the user who\\'s\\n        user_id = user[\"id\"]                   # first in the queue\\n        # because of the way we\\'re adding to the queue,\\n        # necessarily we already know some shortest paths to prev_user\\n        paths_to_prev_user  = shortest_paths_to [prev_user [\"id\"]]\\n        new_paths_to_user  = [path + [user_id] for path in paths_to_prev_user ]\\n        # it\\'s possible we already know a shortest path\\n        old_paths_to_user  = shortest_paths_to .get(user_id, [])\\n        # what\\'s the shortest path to here that we\\'ve seen so far?\\n        if old_paths_to_user :\\n            min_path_length  = len(old_paths_to_user [0])\\n        else:\\n            min_path_length  = float(\\'inf\\')\\n        # only keep paths that aren\\'t too long and are actually new\\n        new_paths_to_user  = [path\\n                             for path in new_paths_to_user\\n                             if len(path) <= min_path_length\\n                             and path not in old_paths_to_user ]\\n        shortest_paths_to [user_id] = old_paths_to_user  + new_paths_to_user\\n        # add never-seen neighbors to the frontier\\n        frontier .extend((user, friend)\\n                        for friend in user[\"friends\" ]\\n                        if friend[\"id\"] not in shortest_paths_to )\\n    return shortest_paths_to\\nNow we can store these dict s with each node:\\nfor user in users:\\n    user[\"shortest_paths\" ] = shortest_paths_from (user)\\nAnd we’re finally ready to compute betweenness centrality. For every pair of nodes i\\nand j, we know the n shortest paths from i to j. Then, for each of those paths, we just\\nadd 1/n to the centrality of each node on that path:\\nfor user in users:\\n    user[\"betweenness_centrality\" ] = 0.0\\nfor source in users:\\n    source_id  = source[\"id\"]\\n    for target_id , paths in source[\"shortest_paths\" ].iteritems ():\\n258 | Chapter 21: Network Analysis\\nwww.it-ebooks.info        if source_id  < target_id :      # don\\'t double count\\n            num_paths  = len(paths)     # how many shortest paths?\\n            contrib = 1 / num_paths     # contribution to centrality\\n            for path in paths:\\n                for id in path:\\n                    if id not in [source_id , target_id ]:\\n                        users[id][\"betweenness_centrality\" ] += contrib\\nFigure 21-2. The DataSciencester network sized by betweenness centrality\\nAs shown in Figure 21-2 , users 0 and 9 have centrality 0 (as neither is on any shortest\\npath between other users), whereas 3, 4, and 5 all have high centralities (as all three lie\\non many shortest paths).\\nGenerally the centrality numbers aren’t that meaningful them‐\\nselves. What we care about is how the numbers for each node com‐\\npare to the numbers for other nodes.\\nAnother measure we can look at is closeness centrality . First, for each user we compute\\nher farness , which is the sum of the lengths of her shortest paths to each other user.\\nSince we’ve already computed the shortest paths between each pair of nodes, it’s easy\\nto add their lengths. (If there are multiple shortest paths, they all have the same\\nlength, so we can just look at the first one.)\\ndef farness(user):\\n    \"\"\"the sum of the lengths of the shortest paths to each other user\"\"\"\\n    return sum(len(paths[0])\\n               for paths in user[\"shortest_paths\" ].values())\\nafter which it’s very little work to compute closeness centrality ( Figure 21-3 ):\\nfor user in users:\\n    user[\"closeness_centrality\" ] = 1 / farness(user)\\nBetweenness Centrality | 259\\nwww.it-ebooks.infoFigure 21-3. The DataSciencester network sized by closeness centrality\\nThere is much less variation here—even the very central nodes are still pretty far\\nfrom the nodes out on the periphery.\\nAs we saw, computing shortest paths is kind of a pain. For this reason, betweenness\\nand closeness centrality aren’t often used on large networks. The less intuitive (but\\ngenerally easier to compute) eigenvector centrality  is more frequently used.\\nEigenvector Centrality\\nIn order to talk about eigenvector centrality, we have to talk about eigenvectors, and\\nin order to talk about eigenvectors, we have to talk about matrix multiplication.\\nMatrix Multiplication\\nIf A is a n1×k1 matrix and B is a n2×k2 matrix, and if k1=n2, then their product AB\\nis the n1×k2 matrix whose (i,j)th entry is:\\nAi1B1j+Ai2B2j+⋯+AikBkj\\nWhich is just the dot product of the ith row of A (thought of as a vector) with the jth\\ncolumn of B (also thought of as a vector):\\ndef matrix_product_entry (A, B, i, j):\\n    return dot(get_row(A, i), get_column (B, j))\\nafter which we have:\\ndef matrix_multiply (A, B):\\n    n1, k1 = shape(A)\\n    n2, k2 = shape(B)\\n    if k1 != n2:\\n        raise ArithmeticError (\"incompatible shapes!\" )\\n    return make_matrix (n1, k2, partial(matrix_product_entry , A, B))\\n260 | Chapter 21: Network Analysis\\nwww.it-ebooks.infoNotice that if A is a n×k matrix and B is a k× 1 matrix, then AB is a n× 1 matrix. If\\nwe treat a vector as a one-column matrix, we can think of A as a function that maps\\nk-dimensional vectors to n-dimensional vectors, where the function is just matrix\\nmultiplication.\\nPreviously we represented vectors simply as lists, which isn’t quite the same:\\nv = [1, 2, 3]\\nv_as_matrix  = [[1],\\n               [2],\\n               [3]]\\nSo we’ll need some helper functions to convert back and forth between the two repre‐\\nsentations:\\ndef vector_as_matrix (v):\\n    \"\"\"returns the vector v (represented as a list) as a n x 1 matrix\"\"\"\\n    return [[v_i] for v_i in v]\\ndef vector_from_matrix (v_as_matrix ):\\n    \"\"\"returns the n x 1 matrix as a list of values\"\"\"\\n    return [row[0] for row in v_as_matrix ]\\nafter which we can define the matrix operation using matrix_multiply :\\ndef matrix_operate (A, v):\\n    v_as_matrix  = vector_as_matrix (v)\\n    product = matrix_multiply (A, v_as_matrix )\\n    return vector_from_matrix (product)\\nWhen A is a square  matrix, this operation maps n-dimensional vectors to other n-\\ndimensional vectors. It’s possible that, for some matrix A and vector v, when A oper‐\\nates on v we get back a scalar multiple of v. That is, that the result is a vector that\\npoints in the same direction as v. When this happens (and when, in addition, v is not\\na vector of all zeroes), we call v an eigenvector  of A. And we call the multiplier an\\neigenvalue .\\nOne possible way to find an eigenvector of A is by picking a starting vector v, apply‐\\ning matrix_operate , rescaling the result to have magnitude 1, and repeating until the\\nprocess converges:\\ndef find_eigenvector (A, tolerance =0.00001):\\n    guess = [random.random() for __ in A]\\n    while True:\\n        result = matrix_operate (A, guess)\\n        length = magnitude (result)\\n        next_guess  = scalar_multiply (1/length, result)\\n        if distance (guess, next_guess ) < tolerance :\\n            return next_guess , length   # eigenvector, eigenvalue\\nEigenvector Centrality | 261\\nwww.it-ebooks.info        guess = next_guess\\nBy construction, the returned guess  is a vector such that, when you apply\\nmatrix_operate  to it and rescale it to have length 1, you get back (a vector very close\\nto) itself. Which means it’s an eigenvector.\\nNot all matrices of real numbers have eigenvectors and eigenvalues. For example the\\nmatrix:\\nrotate = [[ 0, 1],\\n          [-1, 0]]\\nrotates vectors 90 degrees clockwise, which means that the only vector it maps to a\\nscalar multiple of itself is a vector of zeroes. If you tried find_eigenvector(rotate)\\nit would run forever. Even matrices that have eigenvectors can sometimes get stuck in\\ncycles. Consider the matrix:\\nflip = [[0, 1],\\n        [1, 0]]\\nThis matrix maps any vector [x, y]  to [y, x] . This means that, for example, [1, 1]\\nis an eigenvector with eigenvalue 1. However, if you start with a random vector with\\nunequal coordinates, find_eigenvector  will just repeatedly swap the coordinates\\nforever. (Not-from-scratch libraries like NumPy use different methods that would\\nwork in this case.) Nonetheless, when find_eigenvector  does return a result, that\\nresult is indeed an eigenvector.\\nCentrality\\nHow does this help us understand the DataSciencester network?\\nTo start with, we’ll need to represent the connections in our network as an adja\\ncency_matrix , whose (i,j)th entry is either 1 (if user i and user j are friends) or 0 (if\\nthey’re not):\\ndef entry_fn (i, j):\\n    return 1 if (i, j) in friendships  or (j, i) in friendships  else 0\\nn = len(users)\\nadjacency_matrix  = make_matrix (n, n, entry_fn )\\nThe eigenvector centrality for each user is then the entry corresponding to that user\\nin the eigenvector returned by find_eigenvector  (Figure 21-4 ):\\nFor technical reasons that are way beyond the scope of this book,\\nany nonzero adjacency matrix necessarily has an eigenvector all of\\nwhose values are non-negative. And fortunately for us, for this\\nadjacency_matrix  our find_eigenvector  function finds it.\\n262 | Chapter 21: Network Analysis\\nwww.it-ebooks.infoeigenvector_centralities , _ = find_eigenvector (adjacency_matrix )\\nFigure 21-4. The DataSciencester network sized by eigenvector centrality\\nUsers with high eigenvector centrality should be those who have a lot of connections\\nand connections to people who themselves have high centrality.\\nHere users 1 and 2 are the most central, as they both have three connections to people\\nwho are themselves highly central. As we move away from them, people’s centralities\\nsteadily drop off.\\nOn a network this small, eigenvector centrality behaves somewhat erratically. If you\\ntry adding or subtracting links, you’ll find that small changes in the network can dra‐\\nmatically change the centrality numbers. In a much larger network this would not\\nparticularly be the case.\\nWe still haven’t motivated why an eigenvector might lead to a reasonable notion of\\ncentrality. Being an eigenvector means that if you compute:\\nmatrix_operate (adjacency_matrix , eigenvector_centralities )\\nthe result is a scalar multiple of eigenvector_centralities .\\nIf you look at how matrix multiplication works, matrix_operate  produces a vector\\nwhose ith element is:\\ndot(get_row(adjacency_matrix , i), eigenvector_centralities )\\nwhich is precisely the sum of the eigenvector centralities of the users connected to\\nuser i.\\nIn other words, eigenvector centralities are numbers, one per user, such that each\\nuser’s value is a constant multiple of the sum of his neighbors’ values. In this case cen‐\\ntrality means being connected to people who themselves are central. The more cen‐\\ntrality you are directly connected to, the more central you are. This is of course a\\ncircular definition—eigenvectors are the way of breaking out of the circularity.\\nEigenvector Centrality | 263\\nwww.it-ebooks.infoAnother way of understanding this is by thinking about what find_eigenvector  is\\ndoing here. It starts by assigning each node a random centrality. It then repeats the\\nfollowing two steps until the process converges:\\n1.Give each node a new centrality score that equals the sum of its neighbors’ (old)\\ncentrality scores.\\n2.Rescale the vector of centralities to have magnitude 1.\\nAlthough the mathematics behind it may seem somewhat opaque at first, the calcula‐\\ntion itself is relatively straightforward (unlike, say, betweenness centrality) and is\\npretty easy to perform on even very large graphs.\\nDirected Graphs and PageRank\\nDataSciencester isn’t getting much traction, so the VP of Revenue considers pivoting\\nfrom a friendship model to an endorsement model. It turns out that no one particu‐\\nlarly cares which data scientists are friends  with one another, but tech recruiters care\\nvery much which data scientists are respected by other data scientists.\\nIn this new model, we’ll track endorsements (source, target)  that no longer repre‐\\nsent a reciprocal relationship, but rather that source  endorses target  as an awesome\\ndata scientist ( Figure 21-5 ). We’ll need to account for this asymmetry:\\nendorsements  = [(0, 1), (1, 0), (0, 2), (2, 0), (1, 2),\\n                (2, 1), (1, 3), (2, 3), (3, 4), (5, 4),\\n                (5, 6), (7, 5), (6, 8), (8, 7), (8, 9)]\\nfor user in users:\\n    user[\"endorses\" ] = []       # add one list to track outgoing endorsements\\n    user[\"endorsed_by\" ] = []    # and another to track endorsements\\nfor source_id , target_id  in endorsements :\\n    users[source_id ][\"endorses\" ].append(users[target_id ])\\n    users[target_id ][\"endorsed_by\" ].append(users[source_id ])\\nFigure 21-5. The DataSciencester network of endorsements\\n264 | Chapter 21: Network Analysis\\nwww.it-ebooks.infoafter which we can easily find the most_endorsed  data scientists and sell that infor‐\\nmation to recruiters:\\nendorsements_by_id  = [(user[\"id\"], len(user[\"endorsed_by\" ]))\\n                      for user in users]\\nsorted(endorsements_by_id ,\\n       key=lambda (user_id, num_endorsements ): num_endorsements ,\\n       reverse=True)\\nHowever, “number of endorsements” is an easy metric to game. All you need to do is\\ncreate phony accounts and have them endorse you. Or arrange with your friends to\\nendorse each other. (As users 0, 1, and 2 seem to have done.)\\nA better metric would take into account who endorses you. Endorsements from peo‐\\nple who have a lot of endorsements should somehow count more than endorsements\\nfrom people with few endorsements. This is the essence of the PageRank algorithm,\\nused by Google to rank websites based on which other websites link to them, which\\nother websites link to those, and so on.\\n(If this sort of reminds you of the idea behind eigenvector centrality, it should.)\\nA simplified version looks like this:\\n1.There is a total of 1.0 (or 100%) PageRank in the network.\\n2.Initially this PageRank is equally distributed among nodes.\\n3.At each step, a large fraction of each node’s PageRank is distributed evenly among\\nits outgoing links.\\n4.At each step, the remainder of each node’s PageRank is distributed evenly among\\nall nodes.\\ndef page_rank (users, damping = 0.85, num_iters  = 100):\\n    # initially distribute PageRank evenly\\n    num_users  = len(users)\\n    pr = { user[\"id\"] : 1 / num_users  for user in users }\\n    # this is the small fraction of PageRank\\n    # that each node gets each iteration\\n    base_pr = (1 - damping) / num_users\\n    for __ in range(num_iters ):\\n        next_pr = { user[\"id\"] : base_pr for user in users }\\n        for user in users:\\n            # distribute PageRank to outgoing links\\n            links_pr  = pr[user[\"id\"]] * damping\\n            for endorsee  in user[\"endorses\" ]:\\n                next_pr[endorsee [\"id\"]] += links_pr  / len(user[\"endorses\" ])\\nDirected Graphs and PageRank | 265\\nwww.it-ebooks.info        pr = next_pr\\n    return pr\\nPageRank ( Figure 21-6 ) identifies user 4 (Thor) as the highest ranked data scientist.\\nFigure 21-6. The DataSciencester network sized by PageRank\\nEven though he has fewer endorsements (2) than users 0, 1, and 2, his endorsements\\ncarry with them rank from their endorsements. Additionally, both of his endorsers\\nendorsed only him, which means that he doesn’t have to divide their rank with any‐\\none else.\\nFor Further Exploration\\n•There are many other notions of centrality  besides the ones we used (although\\nthe ones we used are pretty much the most popular ones).\\n•NetworkX  is a Python library for network analysis. It has functions for comput‐\\ning centralities and for visualizing graphs.\\n•Gephi  is a love-it/hate-it GUI-based network-visualization tool.\\n266 | Chapter 21: Network Analysis\\nwww.it-ebooks.infoCHAPTER 22\\nRecommender Systems\\nO nature, nature, why art thou so dishonest, as ever to send men with these false recommen‐\\ndations into the world!\\n—Henry Fielding\\nAnother common data problem is producing recommendations  of some sort. Netflix\\nrecommends movies you might want to watch. Amazon recommends products you\\nmight want to buy. Twitter recommends users you might want to follow. In this chap‐\\nter, we’ll look at several ways to use data to make recommendations.\\nIn particular, we’ll look at the data set of users_interests  that we’ve used before:\\nusers_interests  = [\\n    [\"Hadoop\" , \"Big Data\" , \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\" ],\\n    [\"NoSQL\", \"MongoDB\" , \"Cassandra\" , \"HBase\", \"Postgres\" ],\\n    [\"Python\" , \"scikit-learn\" , \"scipy\", \"numpy\", \"statsmodels\" , \"pandas\" ],\\n    [\"R\", \"Python\" , \"statistics\" , \"regression\" , \"probability\" ],\\n    [\"machine learning\" , \"regression\" , \"decision trees\" , \"libsvm\" ],\\n    [\"Python\" , \"R\", \"Java\", \"C++\", \"Haskell\" , \"programming languages\" ],\\n    [\"statistics\" , \"probability\" , \"mathematics\" , \"theory\" ],\\n    [\"machine learning\" , \"scikit-learn\" , \"Mahout\" , \"neural networks\" ],\\n    [\"neural networks\" , \"deep learning\" , \"Big Data\" , \"artificial intelligence\" ],\\n    [\"Hadoop\" , \"Java\", \"MapReduce\" , \"Big Data\" ],\\n    [\"statistics\" , \"R\", \"statsmodels\" ],\\n    [\"C++\", \"deep learning\" , \"artificial intelligence\" , \"probability\" ],\\n    [\"pandas\" , \"R\", \"Python\" ],\\n    [\"databases\" , \"HBase\", \"Postgres\" , \"MySQL\", \"MongoDB\" ],\\n    [\"libsvm\" , \"regression\" , \"support vector machines\" ]\\n]\\nAnd we’ll think about the problem of recommending new interests to a user based on\\nher currently specified interests.\\n267\\nwww.it-ebooks.infoManual Curation\\nBefore the Internet, when you needed book recommendations you would go to the\\nlibrary, where a librarian was available to suggest books that were relevant to your\\ninterests or similar to books you liked.\\nGiven DataSciencester’s limited number of users and interests, it would be easy for\\nyou to spend an afternoon manually recommending interests for each user. But this\\nmethod doesn’t scale particularly well, and it’s limited by your personal knowledge\\nand imagination. (Not that I’m suggesting that your personal knowledge and imagi‐\\nnation are limited.) So let’s think about what we can do with data .\\nRecommending What’s Popular\\nOne easy approach is to simply recommend what’s popular:\\npopular_interests  = Counter(interest\\n                            for user_interests  in users_interests\\n                            for interest  in user_interests ).most_common ()\\nwhich looks like:\\n[(\\'Python\\' , 4),\\n (\\'R\\', 4),\\n (\\'Java\\', 3),\\n (\\'regression\\' , 3),\\n (\\'statistics\\' , 3),\\n (\\'probability\\' , 3),\\n # ...\\n]\\nHaving computed this, we can just suggest to a user the most popular interests that\\nhe’s not already interested in:\\ndef most_popular_new_interests (user_interests , max_results =5):\\n    suggestions  = [(interest , frequency )\\n                   for interest , frequency  in popular_interests\\n                   if interest  not in user_interests ]\\n    return suggestions [:max_results ]\\nSo, if you are user 1, with interests:\\n[\"NoSQL\", \"MongoDB\" , \"Cassandra\" , \"HBase\", \"Postgres\" ]\\nthen we’ d recommend you:\\nmost_popular_new_interests (users_interests [1], 5)\\n# [(\\'Python\\', 4), (\\'R\\', 4), (\\'Java\\', 3), (\\'regression\\', 3), (\\'statistics\\', 3)]\\nIf you are user 3, who’s already interested in many of those things, you’ d instead get:\\n268 | Chapter 22: Recommender Systems\\nwww.it-ebooks.info[(\\'Java\\', 3),\\n (\\'HBase\\', 3),\\n (\\'Big Data\\' , 3),\\n (\\'neural networks\\' , 2),\\n (\\'Hadoop\\' , 2)]\\nOf course, “lots of people are interested in Python so maybe you should be too” is not\\nthe most compelling sales pitch. If someone is brand new to our site and we don’t\\nknow anything about them, that’s possibly the best we can do. Let’s see how we can do\\nbetter by basing each user’s recommendations on her interests.\\nUser-Based Collaborative Filtering\\nOne way of taking a user’s interests into account is to look for users who are somehow\\nsimilar  to him, and then suggest the things that those users are interested in.\\nIn order to do that, we’ll need a way to measure how similar two users are. Here we’ll\\nuse a metric called cosine similarity . Given two vectors, v and w, it’s defined as:\\ndef cosine_similarity (v, w):\\n    return dot(v, w) / math.sqrt(dot(v, v) * dot(w, w))\\nIt measures the “angle” between v and w. If v and w point in the same direction, then\\nthe numerator and denominator are equal, and their cosine similarity equals 1. If v\\nand w point in opposite directions, then their cosine similarity equals -1. And if v is 0\\nwhenever w is not (and vice versa) then dot(v, w)  is 0 and so the cosine similarity\\nwill be 0.\\nWe’ll apply this to vectors of 0s and 1s, each vector v representing one user’s interests.\\nv[i]  will be 1 if the user is specified the ith interest, 0 otherwise. Accordingly, “similar\\nusers” will mean “users whose interest vectors most nearly point in the same direc‐\\ntion. ” Users with identical interests will have similarity 1. Users with no identical\\ninterests will have similarity 0. Otherwise the similarity will fall in between, with\\nnumbers closer to 1 indicating “very similar” and numbers closer to 0 indicating “not\\nvery similar. ”\\nA good place to start is collecting the known interests and (implicitly) assigning indi‐\\nces to them. We can do this by using a set comprehension to find the unique interests,\\nputting them in a list, and then sorting them. The first interest in the resulting list will\\nbe interest 0, and so on:\\nunique_interests  = sorted(list({ interest\\n                                 for user_interests  in users_interests\\n                                 for interest  in user_interests  }))\\nThis gives us a list that starts:\\n[\\'Big Data\\' ,\\n \\'C++\\',\\nUser-Based Collaborative Filtering | 269\\nwww.it-ebooks.info \\'Cassandra\\' ,\\n \\'HBase\\',\\n \\'Hadoop\\' ,\\n \\'Haskell\\' ,\\n # ...\\n]\\nNext we want to produce an “interest” vector of 0s and 1s for each user. We just need\\nto iterate over the unique_interests  list, substituting a 1 if the user has each interest,\\na 0 if not:\\ndef make_user_interest_vector (user_interests ):\\n    \"\"\"given a list of interests, produce a vector whose ith element is 1\\n    if unique_interests[i] is in the list, 0 otherwise\"\"\"\\n    return [1 if interest  in user_interests  else 0\\n            for interest  in unique_interests ]\\nafter which, we can create a matrix of user interests simply by map-ping this function\\nagainst the list of lists of interests:\\nuser_interest_matrix  = map(make_user_interest_vector , users_interests )\\nNow user_interest_matrix[i][j]  equals 1 if user i specified interest j, 0 other‐\\nwise.\\nBecause we have a small data set, it’s no problem to compute the pairwise similarities\\nbetween all of our users:\\nuser_similarities  = [[cosine_similarity (interest_vector_i , interest_vector_j )\\n                      for interest_vector_j  in user_interest_matrix ]\\n                     for interest_vector_i  in user_interest_matrix ]\\nafter which, user_similarities[i][j]  gives us the similarity between users i and j.\\nFor instance, user_similarities[0][9]  is 0.57, as those two users share interests in\\nHadoop, Java, and Big Data. On the other hand, user_similarities[0][8]  is only\\n0.19, as users 0 and 8 share only one interest, Big Data.\\nIn particular, user_similarities[i]  is the vector of user i’s similarities to every\\nother user. We can use this to write a function that finds the most similar users to a\\ngiven user. We’ll make sure not to include the user herself, nor any users with zero\\nsimilarity. And we’ll sort the results from most similar to least similar:\\ndef most_similar_users_to (user_id):\\n    pairs = [(other_user_id , similarity )                      # find other\\n             for other_user_id , similarity  in                 # users with\\n                enumerate (user_similarities [user_id])         # nonzero\\n             if user_id != other_user_id  and similarity  > 0]  # similarity\\n    return sorted(pairs,                                      # sort them\\n                  key=lambda (_, similarity ): similarity ,     # most similar\\n                  reverse=True)                               # first\\n270 | Chapter 22: Recommender Systems\\nwww.it-ebooks.infoFor instance, if we call most_similar_users_to(0)  we get:\\n[(9, 0.5669467095138409 ),\\n (1, 0.3380617018914066 ),\\n (8, 0.1889822365046136 ),\\n (13, 0.1690308509457033 ),\\n (5, 0.1543033499620919 )]\\nHow do we use this to suggest new interests to a user? For each interest, we can just\\nadd up the user-similarities of the other users interested in it:\\ndef user_based_suggestions (user_id, include_current_interests =False):\\n    # sum up the similarities\\n    suggestions  = defaultdict (float)\\n    for other_user_id , similarity  in most_similar_users_to (user_id):\\n        for interest  in users_interests [other_user_id ]:\\n            suggestions [interest ] += similarity\\n    # convert them to a sorted list\\n    suggestions  = sorted(suggestions .items(),\\n                         key=lambda (_, weight): weight,\\n                         reverse=True)\\n    # and (maybe) exclude already-interests\\n    if include_current_interests :\\n        return suggestions\\n    else:\\n        return [(suggestion , weight)\\n                for suggestion , weight in suggestions\\n                if suggestion  not in users_interests [user_id]]\\nIf we call user_based_suggestions(0) , the first several suggested interests are:\\n[(\\'MapReduce\\' , 0.5669467095138409 ),\\n (\\'MongoDB\\' , 0.50709255283711 ),\\n (\\'Postgres\\' , 0.50709255283711 ),\\n (\\'NoSQL\\', 0.3380617018914066 ),\\n (\\'neural networks\\' , 0.1889822365046136 ),\\n (\\'deep learning\\' , 0.1889822365046136 ),\\n (\\'artificial intelligence\\' , 0.1889822365046136 ),\\n #...\\n]\\nThese seem like pretty decent suggestions for someone whose stated interests are “Big\\nData” and database-related. (The weights aren’t intrinsically meaningful; we just use\\nthem for ordering.)\\nThis approach doesn’t work as well when the number of items gets very large. Recall\\nthe curse of dimensionality from Chapter 12 —in large-dimensional vector spaces\\nmost vectors are very far apart (and therefore point in very different directions). That\\nis, when there are a large number of interests the “most similar users” to a given user\\nmight not be similar at all.\\nUser-Based Collaborative Filtering | 271\\nwww.it-ebooks.infoImagine a site like Amazon.com, from which I’ve bought thousands of items over the\\nlast couple of decades. Y ou could attempt to identify similar users to me based on\\nbuying patterns, but most likely in all the world there’s no one whose purchase his‐\\ntory looks even remotely like mine. Whoever my “most similar” shopper is, he’s prob‐\\nably not similar to me at all, and his purchases would almost certainly make for lousy\\nrecommendations.\\nItem-Based Collaborative Filtering\\nAn alternative approach is to compute similarities between interests directly. We can\\nthen generate suggestions for each user by aggregating interests that are similar to her\\ncurrent interests.\\nTo start with, we’ll want to transpose  our user-interest matrix so that rows correspond\\nto interests and columns correspond to users:\\ninterest_user_matrix  = [[user_interest_vector [j]\\n                         for user_interest_vector  in user_interest_matrix ]\\n                        for j, _ in enumerate (unique_interests )]\\nWhat does this look like? Row j of interest_user_matrix  is column j of\\nuser_interest_matrix . That is, it has 1 for each user with that interest and 0 for\\neach user without that interest.\\nFor example, unique_interests[0]  is Big Data, and so interest_user_matrix[0]\\nis:\\n[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\\nbecause users 0, 8, and 9 indicated interest in Big Data.\\nWe can now use cosine similarity again. If precisely the same users are interested in\\ntwo topics, their similarity will be 1. If no two users are interested in both topics, their\\nsimilarity will be 0:\\ninterest_similarities  = [[cosine_similarity (user_vector_i , user_vector_j )\\n                          for user_vector_j  in interest_user_matrix ]\\n                         for user_vector_i  in interest_user_matrix ]\\nFor example, we can find the interests most similar to Big Data (interest 0) using:\\ndef most_similar_interests_to (interest_id ):\\n    similarities  = interest_similarities [interest_id ]\\n    pairs = [(unique_interests [other_interest_id ], similarity )\\n             for other_interest_id , similarity  in enumerate (similarities )\\n             if interest_id  != other_interest_id  and similarity  > 0]\\n    return sorted(pairs,\\n                  key=lambda (_, similarity ): similarity ,\\n                  reverse=True)\\nwhich suggests the following similar interests:\\n272 | Chapter 22: Recommender Systems\\nwww.it-ebooks.info[(\\'Hadoop\\' , 0.8164965809277261 ),\\n (\\'Java\\', 0.6666666666666666 ),\\n (\\'MapReduce\\' , 0.5773502691896258 ),\\n (\\'Spark\\', 0.5773502691896258 ),\\n (\\'Storm\\', 0.5773502691896258 ),\\n (\\'Cassandra\\' , 0.4082482904638631 ),\\n (\\'artificial intelligence\\' , 0.4082482904638631 ),\\n (\\'deep learning\\' , 0.4082482904638631 ),\\n (\\'neural networks\\' , 0.4082482904638631 ),\\n (\\'HBase\\', 0.3333333333333333 )]\\nNow we can create recommendations for a user by summing up the similarities of the\\ninterests similar to his:\\ndef item_based_suggestions (user_id, include_current_interests =False):\\n    # add up the similar interests\\n    suggestions  = defaultdict (float)\\n    user_interest_vector  = user_interest_matrix [user_id]\\n    for interest_id , is_interested  in enumerate (user_interest_vector ):\\n        if is_interested  == 1:\\n            similar_interests  = most_similar_interests_to (interest_id )\\n            for interest , similarity  in similar_interests :\\n                suggestions [interest ] += similarity\\n    # sort them by weight\\n    suggestions  = sorted(suggestions .items(),\\n                         key=lambda (_, similarity ): similarity ,\\n                         reverse=True)\\n    if include_current_interests :\\n        return suggestions\\n    else:\\n        return [(suggestion , weight)\\n                for suggestion , weight in suggestions\\n                if suggestion  not in users_interests [user_id]]\\nFor user 0, this generates the following (seemingly reasonable) recommendations:\\n[(\\'MapReduce\\' , 1.861807319565799 ),\\n (\\'Postgres\\' , 1.3164965809277263 ),\\n (\\'MongoDB\\' , 1.3164965809277263 ),\\n (\\'NoSQL\\', 1.2844570503761732 ),\\n (\\'programming languages\\' , 0.5773502691896258 ),\\n (\\'MySQL\\', 0.5773502691896258 ),\\n (\\'Haskell\\' , 0.5773502691896258 ),\\n (\\'databases\\' , 0.5773502691896258 ),\\n (\\'neural networks\\' , 0.4082482904638631 ),\\n (\\'deep learning\\' , 0.4082482904638631 ),\\n (\\'C++\\', 0.4082482904638631 ),\\n (\\'artificial intelligence\\' , 0.4082482904638631 ),\\n (\\'Python\\' , 0.2886751345948129 ),\\n (\\'R\\', 0.2886751345948129 )]\\nItem-Based Collaborative Filtering | 273\\nwww.it-ebooks.infoFor Further Exploration\\n•Crab  is a framework for building recommender systems in Python.\\n•Graphlab also has a recommender toolkit .\\n•The Netflix Prize  was a somewhat famous competition to build a better system to\\nrecommend movies to Netflix users.\\n274 | Chapter 22: Recommender Systems\\nwww.it-ebooks.infoCHAPTER 23\\nDatabases and SQL\\nMemory is man’s greatest friend and worst enemy.\\n—Gilbert Parker\\nThe data you need will often live in databases , systems designed for efficiently storing\\nand querying data. The bulk of these  are relational  databases, such as Oracle, MySQL,\\nand SQL Server, which store data in tables  and are typically queried using Structured\\nQuery Language (SQL), a declarative language for manipulating data.\\nSQL is a pretty essential part of the data scientist’s toolkit. In this chapter, we’ll create\\nNotQuiteABase, a Python implementation of something that’s not quite a database.\\nWe’ll also cover the basics of SQL while showing how they work in our not-quite\\ndatabase, which is the most “from scratch” way I could think of to help you under‐\\nstand what they’re doing. My hope is that solving problems in NotQuiteABase will\\ngive you a good sense of how you might solve the same problems using SQL.\\nCREATE TABLE and INSERT\\nA relational database is a collection of tables (and of relationships among them). A\\ntable is simply a collection of rows, not unlike the matrices we’ve been working with.\\nHowever, a table also has associated with it a fixed schema  consisting of column\\nnames and column types.\\nFor example, imagine a users  data set containing for each user her user_id , name ,\\nand num_friends :\\nusers = [[0, \"Hero\", 0],\\n         [1, \"Dunn\", 2],\\n         [2, \"Sue\", 3],\\n         [3, \"Chi\", 3]]\\n275\\nwww.it-ebooks.infoIn SQL, we might create this table with:\\nCREATE TABLE users (\\n    user_id INT NOT NULL,\\n    name VARCHAR(200),\\n    num_friends  INT);\\nNotice that we specified that the user_id  and num_friends  must be integers (and that\\nuser_id  isn’t allowed to be NULL , which indicates a missing value and is sort of like\\nour None ) and that the name should be a string of length 200 or less. NotQuiteABase\\nwon’t take types into account, but we’ll behave as if it did.\\nSQL is almost completely case and indentation insensitive. The\\ncapitalization and indentation style here is my preferred style. If\\nyou start learning SQL, you will surely encounter other examples\\nstyled differently.\\nY ou can insert the rows with INSERT  statements:\\nINSERT INTO users (user_id, name, num_friends ) VALUES (0, \\'Hero\\', 0);\\nNotice also that SQL statements need to end with semicolons, and that SQL requires\\nsingle quotes for its strings.\\nIn NotQuiteABase, you’ll create a Table  simply by specifying the names of its col‐\\numns. And to insert a row, you’ll use the table’s insert()  method, which takes a list\\nof row values that need to be in the same order as the table’s column names.\\nBehind the scenes, we’ll store each row as a dict  from column names to values. A real\\ndatabase would never use such a space-wasting representation, but doing so will\\nmake NotQuiteABase much easier to work with:\\nclass Table:\\n    def __init__ (self, columns):\\n        self.columns = columns\\n        self.rows = []\\n    def __repr__ (self):\\n        \"\"\"pretty representation of the table: columns then rows\"\"\"\\n        return str(self.columns) + \"\\\\n\" + \"\\\\n\".join(map(str, self.rows))\\n    def insert(self, row_values ):\\n        if len(row_values ) != len(self.columns):\\n            raise TypeError (\"wrong number of elements\" )\\n        row_dict  = dict(zip(self.columns, row_values ))\\n        self.rows.append(row_dict )\\nFor example, we could set up:\\n276 | Chapter 23: Databases and SQL\\nwww.it-ebooks.infousers = Table([\"user_id\" , \"name\", \"num_friends\" ])\\nusers.insert([0, \"Hero\", 0])\\nusers.insert([1, \"Dunn\", 2])\\nusers.insert([2, \"Sue\", 3])\\nusers.insert([3, \"Chi\", 3])\\nusers.insert([4, \"Thor\", 3])\\nusers.insert([5, \"Clive\", 2])\\nusers.insert([6, \"Hicks\", 3])\\nusers.insert([7, \"Devin\", 2])\\nusers.insert([8, \"Kate\", 2])\\nusers.insert([9, \"Klein\", 3])\\nusers.insert([10, \"Jen\", 1])\\nIf you now print users , you’ll see:\\n[\\'user_id\\', \\'name\\', \\'num_friends\\']\\n{\\'user_id\\': 0, \\'name\\': \\'Hero\\', \\'num_friends\\': 0}\\n{\\'user_id\\': 1, \\'name\\': \\'Dunn\\', \\'num_friends\\': 2}\\n{\\'user_id\\': 2, \\'name\\': \\'Sue\\', \\'num_friends\\': 3}\\n...\\nUPDATE\\nSometimes you need to update the data that’s already in the database. For instance, if\\nDunn acquires another friend, you might need to do this:\\nUPDATE users\\nSET num_friends  = 3\\nWHERE user_id = 1;\\nThe key features are:\\n•What table to update\\n•Which rows to update\\n•Which fields to update\\n•What their new values should be\\nWe’ll add a similar update  method to NotQuiteABase. Its first argument will be a\\ndict  whose keys are the columns to update and whose values are the new values for\\nthose fields. And its second argument is a predicate  that returns True  for rows that\\nshould be updated, False  otherwise:\\n    def update(self, updates, predicate ):\\n        for row in self.rows:\\n            if predicate (row):\\n                for column, new_value  in updates.iteritems ():\\n                    row[column] = new_value\\nafter which we can simply do this:\\nUPDATE | 277\\nwww.it-ebooks.infousers.update({\\'num_friends\\'  : 3},               # set num_friends = 3\\n             lambda row: row[\\'user_id\\' ] == 1)   # in rows where user_id == 1\\nDELETE\\nThere are two ways to delete rows from a table in SQL. The dangerous way deletes\\nevery row from a table:\\nDELETE FROM users;\\nThe less dangerous way adds a WHERE  clause  and only deletes rows that match a cer‐\\ntain condition:\\nDELETE FROM users WHERE user_id = 1;\\nIt’s easy to add this functionality to our Table :\\n    def delete(self, predicate =lambda row: True):\\n        \"\"\"delete all rows matching predicate\\n        or all rows if no predicate supplied\"\"\"\\n        self.rows = [row for row in self.rows if not(predicate (row))]\\nIf you supply a predicate  function (i.e., a WHERE  clause), this  deletes only the rows\\nthat satisfy it. If you don’t supply one, the default predicate always returns True , and\\nyou will delete every row.\\nFor example:\\nusers.delete(lambda row: row[\"user_id\" ] == 1)  # deletes rows with user_id == 1\\nusers.delete()                                 # deletes every row\\nSELECT\\nTypically you don’t inspect SQL tables directly. Instead you query them with  a SELECT\\nstatement:\\nSELECT * FROM users;                            -- get the entire contents\\nSELECT * FROM users LIMIT 2;                    -- get the first two rows\\nSELECT user_id FROM users;                      -- only get specific columns\\nSELECT user_id FROM users WHERE name = \\'Dunn\\';  -- only get specific rows\\nY ou can also use SELECT  statements to calculate fields:\\nSELECT LENGTH(name) AS name_length  FROM users;\\nWe’ll give our Table  class a select()  method that returns a new Table . The method\\naccepts two optional arguments:\\n•keep_columns  specifies the name of the columns you want to keep in the result.\\nIf you don’t supply it, the result contains all the columns.\\n278 | Chapter 23: Databases and SQL\\nwww.it-ebooks.info•additional_columns  is a dictionary whose keys are new column names and\\nwhose values are functions specifying how to compute the values of the new col‐\\numns.\\nIf you were to supply neither of them, you’ d simply get back a copy of the table:\\n    def select(self, keep_columns =None, additional_columns =None):\\n        if keep_columns  is None:         # if no columns specified,\\n            keep_columns  = self.columns  # return all columns\\n        if additional_columns  is None:\\n            additional_columns  = {}\\n        # new table for results\\n        result_table  = Table(keep_columns  + additional_columns .keys())\\n        for row in self.rows:\\n            new_row = [row[column] for column in keep_columns ]\\n            for column_name , calculation  in additional_columns .iteritems ():\\n                new_row.append(calculation (row))\\n            result_table .insert(new_row)\\n        return result_table\\nOur select()  returns a new Table , while the typical SQL SELECT  just produces some\\nsort of transient result set (unless you explicitly insert the results into a table).\\nWe’ll also need where()  and limit()  methods. Both are pretty simple:\\n    def where(self, predicate =lambda row: True):\\n        \"\"\"return only the rows that satisfy the supplied predicate\"\"\"\\n        where_table  = Table(self.columns)\\n        where_table .rows = filter(predicate , self.rows)\\n        return where_table\\n    def limit(self, num_rows ):\\n        \"\"\"return only the first num_rows rows\"\"\"\\n        limit_table  = Table(self.columns)\\n        limit_table .rows = self.rows[:num_rows ]\\n        return limit_table\\nafter which we can easily construct NotQuiteABase equivalents to the preceding SQL\\nstatements:\\n# SELECT * FROM users;\\nusers.select()\\n# SELECT * FROM users LIMIT 2;\\nusers.limit(2)\\n# SELECT user_id FROM users;\\nusers.select(keep_columns =[\"user_id\" ])\\nSELECT | 279\\nwww.it-ebooks.info# SELECT user_id FROM users WHERE name = \\'Dunn\\';\\nusers.where(lambda row: row[\"name\"] == \"Dunn\") \\\\\\n     .select(keep_columns =[\"user_id\" ])\\n# SELECT LENGTH(name) AS name_length FROM users;\\ndef name_length (row): return len(row[\"name\"])\\nusers.select(keep_columns =[],\\n             additional_columns  = { \"name_length\"  : name_length  })\\nNotice that—unlike in the rest of the book—here I use backslash \\\\ to continue state‐\\nments across multiple lines. I find it makes the chained-together NotQuiteABase\\nqueries more readable than any other way of writing them.\\nGROUP BY\\nAnother common SQL operation is GROUP BY , which groups together rows with iden‐\\ntical values in specified columns and produces aggregate values like MIN and MAX and\\nCOUNT  and SUM. (This should remind you of the group_by  function from “Manipulat‐\\ning Data” on page 129 .)\\nFor example, you might want to find the number of users and the smallest user_id\\nfor each possible name length:\\nSELECT LENGTH(name) as name_length ,\\n MIN(user_id) AS min_user_id ,\\n COUNT(*) AS num_users\\nFROM users\\nGROUP BY LENGTH(name);\\nEvery field we SELECT  needs to be either in the GROUP BY  clause (which name_length\\nis) or an aggregate computation (which min_user_id  and num_users  are).\\nSQL also supports a HAVING  clause that behaves similarly to a WHERE  clause except that\\nits filter is applied to the aggregates (whereas a WHERE  would filter out rows before\\naggregation even took place).\\nY ou might want to know the average number of friends for users whose names start\\nwith specific letters but only see the results for letters whose corresponding average is\\ngreater than 1. (Y es, some of these examples are contrived.)\\nSELECT SUBSTR(name, 1, 1) AS first_letter ,\\n AVG(num_friends ) AS avg_num_friends\\nFROM users\\nGROUP BY SUBSTR(name, 1, 1)\\nHAVING AVG(num_friends ) > 1;\\n(Functions for working with strings vary across SQL implementations; some databa‐\\nses might instead use SUBSTRING  or something else.)\\n280 | Chapter 23: Databases and SQL\\nwww.it-ebooks.infoY ou can also compute overall aggregates. In that case, you leave off the GROUP BY :\\nSELECT SUM(user_id) as user_id_sum\\nFROM users\\nWHERE user_id > 1;\\nTo add this functionality to NotQuiteABase Table s, we’ll add a group_by()  method.\\nIt takes the names of the columns you want to group by, a dictionary of the aggrega‐\\ntion functions you want to run over each group, and an optional predicate having\\nthat operates on multiple rows.\\nThen it does the following steps:\\n1.Creates a defaultdict  to map tuple s (of the group-by-values) to rows (contain‐\\ning the group-by-values). Recall that you can’t use lists as dict  keys; you have to\\nuse tuples.\\n2.Iterates over the rows of the table, populating the defaultdict .\\n3.Creates a new table with the correct output columns.\\n4.Iterates over the defaultdict  and populates the output table, applying the hav\\ning filter if any.\\n(An actual database would almost certainly do this in a more efficient manner.)\\n    def group_by (self, group_by_columns , aggregates , having=None):\\n        grouped_rows  = defaultdict (list)\\n        # populate groups\\n        for row in self.rows:\\n            key = tuple(row[column] for column in group_by_columns )\\n            grouped_rows [key].append(row)\\n        # result table consists of group_by columns and aggregates\\n        result_table  = Table(group_by_columns  + aggregates .keys())\\n        for key, rows in grouped_rows .iteritems ():\\n            if having is None or having(rows):\\n                new_row = list(key)\\n                for aggregate_name , aggregate_fn  in aggregates .iteritems ():\\n                    new_row.append(aggregate_fn (rows))\\n                result_table .insert(new_row)\\n        return result_table\\nAgain, let’s see how we would do the equivalent of the preceding SQL statements. The\\nname_length  metrics are:\\ndef min_user_id (rows): return min(row[\"user_id\" ] for row in rows)\\nGROUP BY | 281\\nwww.it-ebooks.infostats_by_length  = users \\\\\\n    .select(additional_columns ={\"name_length\"  : name_length }) \\\\\\n    .group_by (group_by_columns =[\"name_length\" ],\\n              aggregates ={ \"min_user_id\"  : min_user_id ,\\n                           \"num_users\"  : len })\\nThe first_letter  metrics:\\ndef first_letter_of_name (row):\\n    return row[\"name\"][0] if row[\"name\"] else \"\"\\ndef average_num_friends (rows):\\n    return sum(row[\"num_friends\" ] for row in rows) / len(rows)\\ndef enough_friends (rows):\\n    return average_num_friends (rows) > 1\\navg_friends_by_letter  = users \\\\\\n    .select(additional_columns ={\\'first_letter\\'  : first_letter_of_name }) \\\\\\n    .group_by (group_by_columns =[\\'first_letter\\' ],\\n              aggregates ={ \"avg_num_friends\"  : average_num_friends  },\\n              having=enough_friends )\\nand the user_id_sum  is:\\ndef sum_user_ids (rows): return sum(row[\"user_id\" ] for row in rows)\\nuser_id_sum  = users \\\\\\n    .where(lambda row: row[\"user_id\" ] > 1) \\\\\\n    .group_by (group_by_columns =[],\\n              aggregates ={ \"user_id_sum\"  : sum_user_ids  })\\nORDER BY\\nFrequently, you’ll want to sort your results.  For example, you might want to know the\\n(alphabetically) first two names of your users:\\nSELECT * FROM users\\nORDER BY name\\nLIMIT 2;\\nThis is easy to implement by giving our Table  an order_by()  method that takes an\\norder  function:\\n    def order_by (self, order):\\n        new_table  = self.select()       # make a copy\\n        new_table .rows.sort(key=order)\\n        return new_table\\nwhich we can then use as follows:\\n282 | Chapter 23: Databases and SQL\\nwww.it-ebooks.infofriendliest_letters  = avg_friends_by_letter  \\\\\\n    .order_by (lambda row: -row[\"avg_num_friends\" ]) \\\\\\n    .limit(4)\\nThe SQL ORDER BY  lets you specify ASC (ascending) or DESC  (descending) for each\\nsort field; here we’ d have to bake that into our order  function.\\nJOIN\\nRelational database tables are often normalized , which means that they’re organized\\nto minimize redundancy. For example, when we work with our users’ interests in\\nPython we can just give each user a list  containing his interests.\\nSQL tables can’t typically contain lists, so the typical solution is to create a second\\ntable user_interests  containing the one-to-many relationship between user_id s\\nand interests. In SQL you might do:\\nCREATE TABLE user_interests  (\\n    user_id INT NOT NULL,\\n    interest  VARCHAR(100) NOT NULL\\n);\\nwhereas in NotQuiteABase you’ d create the table:\\nuser_interests  = Table([\"user_id\" , \"interest\" ])\\nuser_interests .insert([0, \"SQL\"])\\nuser_interests .insert([0, \"NoSQL\"])\\nuser_interests .insert([2, \"SQL\"])\\nuser_interests .insert([2, \"MySQL\"])\\nThere’s still plenty of redundancy—the interest “SQL ” is stored in\\ntwo different places. In a real database you might store user_id\\nand interest_id  in the user_interests  table and then create a\\nthird table interests  mapping interest_id  to interest  so you\\ncould store the interest names only once each. Here that would just\\nmake our examples more complicated than they need to be.\\nWhen our data lives across different tables, how do we analyze it? By JOIN ing the\\ntables together. A JOIN  combines rows in the left table with corresponding rows in\\nthe right table, where the meaning of “corresponding” is based on how we specify the\\njoin.\\nFor example, to find the users interested in SQL you’ d query:\\nSELECT users.name\\nFROM users\\nJOIN user_interests\\nON users.user_id = user_interests .user_id\\nWHERE user_interests .interest  = \\'SQL\\'\\nJOIN | 283\\nwww.it-ebooks.infoThe JOIN  says that, for each row in users , we should look at the user_id  and asso‐\\nciate that row with every row in user_interests  containing the same user_id .\\nNotice we had to specify which tables to JOIN  and also which columns to join ON. This\\nis an INNER JOIN , which returns the combinations of rows (and only the combina‐\\ntions of rows) that match according to the specified join criteria.\\nThere is also a LEFT JOIN , which—in addition to the combinations of matching rows\\n—returns a row for each left-table row with no matching rows (in which case, the\\nfields that would have come from the right table are all NULL ).\\nUsing a LEFT JOIN , it’s easy to count the number of interests each user has:\\nSELECT users.id, COUNT(user_interests .interest ) AS num_interests\\nFROM users\\nLEFT JOIN user_interests\\nON users.user_id = user_interests .user_id\\nThe LEFT JOIN  ensures that users with no interests will still have rows in the joined\\ndata set (with NULL  values for the fields coming from user_interests ), and COUNT\\nonly counts values that are non- NULL .\\nThe NotQuiteABase join()  implementation will be more restrictive—it simply joins\\ntwo tables on whatever columns they have in common. Even so, it’s not trivial to\\nwrite:\\n    def join(self, other_table , left_join =False):\\n        join_on_columns  = [c for c in self.columns           # columns in\\n                           if c in other_table .columns]      # both tables\\n        additional_columns  = [c for c in other_table .columns # columns only\\n                              if c not in join_on_columns ]   # in right table\\n        # all columns from left table + additional_columns from right table\\n        join_table  = Table(self.columns + additional_columns )\\n        for row in self.rows:\\n            def is_join(other_row ):\\n                return all(other_row [c] == row[c] for c in join_on_columns )\\n            other_rows  = other_table .where(is_join).rows\\n            # each other row that matches this one produces a result row\\n            for other_row  in other_rows :\\n                join_table .insert([row[c] for c in self.columns] +\\n                                  [other_row [c] for c in additional_columns ])\\n            # if no rows match and it\\'s a left join, output with Nones\\n            if left_join  and not other_rows :\\n                join_table .insert([row[c] for c in self.columns] +\\n284 | Chapter 23: Databases and SQL\\nwww.it-ebooks.info                                  [None for c in additional_columns ])\\n        return join_table\\nSo, we could find users interested in SQL with:\\nsql_users  = users \\\\\\n    .join(user_interests ) \\\\\\n    .where(lambda row: row[\"interest\" ] == \"SQL\") \\\\\\n    .select(keep_columns =[\"name\"])\\nAnd we could get the interest counts with:\\ndef count_interests (rows):\\n    \"\"\"counts how many rows have non-None interests\"\"\"\\n    return len([row for row in rows if row[\"interest\" ] is not None])\\nuser_interest_counts  = users \\\\\\n    .join(user_interests , left_join =True) \\\\\\n    .group_by (group_by_columns =[\"user_id\" ],\\n              aggregates ={\"num_interests\"  : count_interests  })\\nIn SQL, there is also a RIGHT JOIN , which keeps rows from the right table that have\\nno matches, and a FULL OUTER JOIN , which keeps rows from both tables that have no\\nmatches. We won’t implement either of those.\\nSubqueries\\nIn SQL, you can SELECT  from (and JOIN ) the results of queries as if they were tables.\\nSo if you wanted to find the smallest user_id  of anyone interested in SQL, you could\\nuse a subquery. (Of course, you could do the same calculation using a JOIN , but that\\nwouldn’t illustrate subqueries.)\\nSELECT MIN(user_id) AS min_user_id  FROM\\n(SELECT user_id FROM user_interests  WHERE interest  = \\'SQL\\') sql_interests ;\\nGiven the way we’ve designed NotQuiteABase, we get this for free. (Our query results\\nare actual tables.)\\nlikes_sql_user_ids  = user_interests  \\\\\\n    .where(lambda row: row[\"interest\" ] == \"SQL\") \\\\\\n    .select(keep_columns =[\\'user_id\\' ])\\nlikes_sql_user_ids .group_by (group_by_columns =[],\\n                            aggregates ={ \"min_user_id\"  : min_user_id  })\\nIndexes\\nTo find rows containing a specific value (say, where name  is “Hero”), NotQuiteABase\\nhas to inspect every row in the table.  If the table has a lot of rows, this can take a very\\nlong time.\\nSubqueries | 285\\nwww.it-ebooks.infoSimilarly, our join  algorithm is extremely inefficient. For each row in the left table, it\\ninspects every row in the right table to see if it’s a match. With two large tables this\\ncould take approximately forever.\\nAlso, you’ d often like to apply constraints to some of your columns. For example, in\\nyour users  table you probably don’t want to allow two different users to have the\\nsame user_id .\\nIndexes solve all these problems. If the user_interests  table had an index on\\nuser_id , a smart join  algorithm could find matches directly rather than scanning the\\nwhole table. If the users  table had a “unique” index on user_id , you’ d get an error if\\nyou tried to insert a duplicate.\\nEach table in a database can have one or more indexes, which allow you to quickly\\nlook up rows by key columns, efficiently join tables together, and enforce unique con‐\\nstraints on columns or combinations of columns.\\nDesigning and using indexes well is somewhat of a black art (which varies somewhat\\ndepending on the specific database), but if you end up doing a lot of database work\\nit’s worth learning about.\\nQuery Optimization\\nRecall the query to find all users who are interested in SQL:\\nSELECT users.name\\nFROM users\\nJOIN user_interests\\nON users.user_id = user_interests .user_id\\nWHERE user_interests .interest  = \\'SQL\\'\\nIn NotQuiteABase there are (at least) two different ways to write this query. Y ou\\ncould filter the user_interests  table before performing the join:\\nuser_interests  \\\\\\n    .where(lambda row: row[\"interest\" ] == \"SQL\") \\\\\\n    .join(users) \\\\\\n    .select([\"name\"])\\nOr you could filter the results of the join:\\nuser_interests  \\\\\\n    .join(users) \\\\\\n    .where(lambda row: row[\"interest\" ] == \"SQL\") \\\\\\n    .select([\"name\"])\\nY ou’ll end up with the same results either way, but filter-before-join is almost cer‐\\ntainly more efficient, since in that case join  has many fewer rows to operate on.\\n286 | Chapter 23: Databases and SQL\\nwww.it-ebooks.infoIn SQL, you generally wouldn’t worry about this. Y ou “declare” the results you want\\nand leave it up to the query engine to execute them (and use indexes efficiently).\\nNoSQL\\nA recent trend in databases is toward nonrelational “NoSQL ” databases, which don’t\\nrepresent data in tables. For instance, MongoDB is a popular schema-less database\\nwhose elements are arbitrarily complex JSON documents rather than rows.\\nThere are column databases that store data in columns instead of rows (good when\\ndata has many columns but queries need few of them), key-value stores that are opti‐\\nmized for retrieving single (complex) values by their keys, databases for storing and\\ntraversing graphs, databases that are optimized to run across multiple datacenters,\\ndatabases that are designed to run in memory, databases for storing time-series data,\\nand hundreds more.\\nTomorrow’s flavor of the day might not even exist now, so I can’t do much more than\\nlet you know that NoSQL is a thing. So now you know. It’s a thing.\\nFor Further Exploration\\n•If you’ d like to download a relational database to play with, SQLite  is fast and tiny,\\nwhile MySQL  and PostgreSQL  are larger and featureful. All are free and have lots\\nof documentation.\\n•If you want to explore NoSQL, MongoDB  is very simple to get started with,\\nwhich can be both a blessing and somewhat of a curse. It also has pretty good\\ndocumentation.\\n•The Wikipedia article on NoSQL  almost certainly now contains links to databa‐\\nses that didn’t even exist when this book was written.\\nNoSQL | 287\\nwww.it-ebooks.infowww.it-ebooks.infoCHAPTER 24\\nMapReduce\\nThe future has already arrived. It’s just not evenly distributed yet.\\n—William Gibson\\nMapReduce is a programming model for performing parallel processing on large data\\nsets. Although it is a powerful technique, its basics are relatively simple.\\nImagine we have a collection of items we’ d like to process somehow. For instance, the\\nitems might be website logs, the texts of various books, image files, or anything else. \\nA basic version of the MapReduce algorithm consists of the following steps:\\n1.Use a mapper  function to turn each item into zero or more key-value pairs.\\n(Often this is called the map function, but there is already a Python function\\ncalled map and we don’t need to confuse the two.)\\n2.Collect together all the pairs with identical keys.\\n3.Use a reducer  function on each collection of grouped values to produce output\\nvalues for the corresponding key.\\nThis is all sort of abstract, so let’s look at a specific example. There are few absolute\\nrules of data science, but one of them is that your first MapReduce example has to\\ninvolve counting words.\\nExample: Word Count\\nDataSciencester has grown to millions of users! This is great for your job security, but\\nit makes routine analyses slightly more difficult.\\n289\\nwww.it-ebooks.infoFor example, your VP of Content wants to know what sorts of things people are talk‐\\ning about in their status updates. As a first attempt, you decide to count the words\\nthat appear, so that you can prepare a report on the most frequent ones.\\nWhen you had a few hundred users this was simple to do:\\ndef word_count_old (documents ):\\n    \"\"\"word count not using MapReduce\"\"\"\\n    return Counter(word\\n        for document  in documents\\n        for word in tokenize (document ))\\nWith millions of users the set of documents  (status updates) is suddenly too big to fit\\non your computer. If you can just fit this into the MapReduce model, you can use\\nsome “big data” infrastructure that your engineers have implemented.\\nFirst, we need a function that turns a document into a sequence of key-value pairs.\\nWe’ll want our output to be grouped by word, which means that the keys should be\\nwords. And for each word, we’ll just emit the value 1 to indicate that this pair corre‐\\nsponds to one occurrence of the word:\\ndef wc_mapper (document ):\\n    \"\"\"for each word in the document, emit (word,1)\"\"\"\\n    for word in tokenize (document ):\\n        yield (word, 1)\\nSkipping the “plumbing” step 2 for the moment, imagine that for some word we’ve\\ncollected a list of the corresponding counts we emitted. Then to produce the overall\\ncount for that word we just need:\\ndef wc_reducer (word, counts):\\n    \"\"\"sum up the counts for a word\"\"\"\\n    yield (word, sum(counts))\\nReturning to step 2, we now need to collect the results from wc_mapper  and feed them\\nto wc_reducer . Let’s think about how we would do this on just one computer:\\ndef word_count (documents ):\\n    \"\"\"count the words in the input documents using MapReduce\"\"\"\\n    # place to store grouped values\\n    collector  = defaultdict (list)\\n    for document  in documents :\\n        for word, count in wc_mapper (document ):\\n            collector [word].append(count)\\n    return [output\\n            for word, counts in collector .iteritems ()\\n            for output in wc_reducer (word, counts)]\\n290 | Chapter 24: MapReduce\\nwww.it-ebooks.infoImagine that we have three documents [\"data science\", \"big data\", \"science\\nfiction\"] .\\nThen wc_mapper  applied to the first document yields the two pairs (\"data\", 1)  and\\n(\"science\", 1) . After we’ve gone through all three documents, the collector  con‐\\ntains\\n{ \"data\" : [1, 1],\\n  \"science\"  : [1, 1],\\n  \"big\" : [1],\\n  \"fiction\"  : [1] }\\nThen wc_reducer  produces the count for each word:\\n[(\"data\", 2), (\"science\" , 2), (\"big\", 1), (\"fiction\" , 1)]\\nWhy MapReduce?\\nAs mentioned earlier, the primary benefit of MapReduce  is that it allows us to distrib‐\\nute computations by moving the processing to the data. Imagine we want to word-\\ncount across billions of documents.\\nOur original (non-MapReduce) approach requires the machine doing the processing\\nto have access to every document. This means that the documents all need to either\\nlive on that machine or else be transferred to it during processing. More important, it\\nmeans that the machine can only process one document at a time.\\nPossibly it can process up to a few at a time if it has multiple cores\\nand if the code is rewritten to take advantage of them. But even so,\\nall the documents still have to get to  that machine.\\nImagine now that our billions of documents are scattered across 100 machines. With\\nthe right infrastructure (and glossing over some of the details), we can do the follow‐\\ning:\\n•Have each machine run the mapper on its documents, producing lots of (key,\\nvalue) pairs.\\n•Distribute those (key, value) pairs to a number of “reducing” machines, making\\nsure that the pairs corresponding to any given key all end up on the same\\nmachine.\\n•Have each reducing machine group the pairs by key and then run the reducer on\\neach set of values.\\n•Return each (key, output) pair.\\nWhy MapReduce? | 291\\nwww.it-ebooks.infoWhat is amazing about this is that it scales horizontally. If we double the number of\\nmachines, then (ignoring certain fixed-costs of running a MapReduce system) our\\ncomputation should run approximately twice as fast. Each mapper machine will only\\nneed to do half as much work, and (assuming there are enough distinct keys to fur‐\\nther distribute the reducer work) the same is true for the reducer machines.\\nMapReduce More Generally\\nIf you think about it for a minute, all of the word-count-specific code in the previous\\nexample is contained in the wc_mapper  and wc_reducer  functions. This means that\\nwith a couple of changes we have a much more general framework (that still runs on\\na single machine):\\ndef map_reduce (inputs, mapper, reducer):\\n    \"\"\"runs MapReduce on the inputs using mapper and reducer\"\"\"\\n    collector  = defaultdict (list)\\n    for input in inputs:\\n        for key, value in mapper(input):\\n            collector [key].append(value)\\n    return [output\\n            for key, values in collector .iteritems ()\\n            for output in reducer(key,values)]\\nAnd then we can count words simply by using:\\nword_counts  = map_reduce (documents , wc_mapper , wc_reducer )\\nThis gives us the flexibility to solve a wide variety of problems.\\nBefore we proceed, observe that wc_reducer  is just summing the values correspond‐\\ning to each key. This kind of aggregation is common enough that it’s worth abstract‐\\ning it out:\\ndef reduce_values_using (aggregation_fn , key, values):\\n    \"\"\"reduces a key-values pair by applying aggregation_fn to the values\"\"\"\\n    yield (key, aggregation_fn (values))\\ndef values_reducer (aggregation_fn ):\\n    \"\"\"turns a function (values -> output) into a reducer\\n    that maps (key, values) -> (key, output)\"\"\"\\n    return partial(reduce_values_using , aggregation_fn )\\nafter which we can easily create:\\nsum_reducer  = values_reducer (sum)\\nmax_reducer  = values_reducer (max)\\nmin_reducer  = values_reducer (min)\\ncount_distinct_reducer  = values_reducer (lambda values: len(set(values)))\\n292 | Chapter 24: MapReduce\\nwww.it-ebooks.infoand so on.\\nExample: Analyzing Status Updates\\nThe content VP was impressed with the word counts and asks what else you can learn\\nfrom people’s status updates. Y ou manage to extract a data set of status updates that\\nlook like:\\n{\"id\": 1,\\n \"username\"  : \"joelgrus\" ,\\n \"text\" : \"Is anyone interested in a data science book?\" ,\\n \"created_at\"  : datetime .datetime (2013, 12, 21, 11, 47, 0),\\n \"liked_by\"  : [\"data_guy\" , \"data_gal\" , \"mike\"] }\\nLet’s say we need to figure out which day of the week people talk the most about data\\nscience. In order to find this, we’ll just count how many data science updates there are\\non each day of the week. This means we’ll need to group by the day of week, so that’s\\nour key. And if we emit a value of 1 for each update that contains “data science, ” we\\ncan simply get the total number using sum:\\ndef data_science_day_mapper (status_update ):\\n    \"\"\"yields (day_of_week, 1) if status_update contains \"data science\" \"\"\"\\n    if \"data science\"  in status_update [\"text\"].lower():\\n        day_of_week  = status_update [\"created_at\" ].weekday()\\n        yield (day_of_week , 1)\\ndata_science_days  = map_reduce (status_updates ,\\n                               data_science_day_mapper ,\\n                               sum_reducer )\\nAs a slightly more complicated example, imagine we need to find out for each user\\nthe most common word that she puts in her status updates. There are three possible\\napproaches that spring to mind for the mapper :\\n•Put the username in the key; put the words and counts in the values.\\n•Put the word in key; put the usernames and counts in the values.\\n•Put the username and word in the key; put the counts in the values.\\nIf you think about it a bit more, we definitely want to group by username , because we\\nwant to consider each person’s words separately. And we don’t want to group by word ,\\nsince our reducer will need to see all the words for each person to find out which is\\nthe most popular. This means that the first option is the right choice:\\ndef words_per_user_mapper (status_update ):\\n    user = status_update [\"username\" ]\\n    for word in tokenize (status_update [\"text\"]):\\n        yield (user, (word, 1))\\nExample: Analyzing Status Updates | 293\\nwww.it-ebooks.infodef most_popular_word_reducer (user, words_and_counts ):\\n    \"\"\"given a sequence of (word, count) pairs,\\n    return the word with the highest total count\"\"\"\\n    word_counts  = Counter()\\n    for word, count in words_and_counts :\\n        word_counts [word] += count\\n    word, count = word_counts .most_common (1)[0]\\n    yield (user, (word, count))\\nuser_words  = map_reduce (status_updates ,\\n                        words_per_user_mapper ,\\n                        most_popular_word_reducer )\\nOr we could find out the number of distinct status-likers for each user:\\ndef liker_mapper (status_update ):\\n    user = status_update [\"username\" ]\\n    for liker in status_update [\"liked_by\" ]:\\n        yield (user, liker)\\ndistinct_likers_per_user  = map_reduce (status_updates ,\\n                                      liker_mapper ,\\n                                      count_distinct_reducer )\\nExample: Matrix Multiplication\\nRecall from “Matrix Multiplication” on page 260 that given a m×n matrix A and a\\nn×k matrix B, we can multiply them to form a m×k matrix C, where the element of\\nC in row i and column j is given by:\\nCij=Ai1B1j+Ai2B2j+ ... + AinBnj\\nAs we’ve seen, a “natural” way to represent a m×n matrix is with a list  of list s,\\nwhere the element Aij is the jth element of the ith list.\\nBut large matrices are sometimes sparse , which means that most of their elements\\nequal zero. For large sparse matrices, a list of lists can be a very wasteful representa‐\\ntion. A more compact representation is a list of tuples (name, i, j, value)  where\\nname  identifies the matrix, and where i, j, value  indicates a location with nonzero\\nvalue.\\nFor example, a billion × billion matrix has a quintillion  entries, which would not be\\neasy to store on a computer. But if there are only a few nonzero entries in each row,\\nthis alternative representation is many orders of magnitude smaller.\\n294 | Chapter 24: MapReduce\\nwww.it-ebooks.infoGiven this sort of representation, it turns out that we can use MapReduce to perform\\nmatrix multiplication in a distributed manner.\\nTo motivate our algorithm, notice that each element Aij is only used to compute the\\nelements of C in row i, and each element Bij is only used to compute the elements of\\nC in column j. Our goal will be for each output of our reducer  to be a single entry of\\nC, which means we’ll need our mapper to emit keys identifying a single entry of C.\\nThis suggests the following:\\ndef matrix_multiply_mapper (m, element):\\n    \"\"\"m is the common dimension (columns of A, rows of B)\\n    element is a tuple (matrix_name, i, j, value)\"\"\"\\n    name, i, j, value = element\\n    if name == \"A\":\\n        # A_ij is the jth entry in the sum for each C_ik, k=1..m\\n        for k in range(m):\\n            # group with other entries for C_ik\\n            yield((i, k), (j, value))\\n    else:\\n        # B_ij is the i-th entry in the sum for each C_kj\\n        for k in range(m):\\n            # group with other entries for C_kj\\n            yield((k, j), (i, value))\\ndef matrix_multiply_reducer (m, key, indexed_values ):\\n    results_by_index  = defaultdict (list)\\n    for index, value in indexed_values :\\n        results_by_index [index].append(value)\\n    # sum up all the products of the positions with two results\\n    sum_product  = sum(results[0] * results[1]\\n                      for results in results_by_index .values()\\n                      if len(results) == 2)\\n    if sum_product  != 0.0:\\n        yield (key, sum_product )\\nFor example, if you had the two matrices\\nA = [[3, 2, 0],\\n     [0, 0, 0]]\\nB = [[4, -1, 0],\\n     [10, 0, 0],\\n     [0, 0, 0]]\\nyou could rewrite them as tuples:\\nentries = [(\"A\", 0, 0, 3), (\"A\", 0, 1,  2),\\n           (\"B\", 0, 0, 4), (\"B\", 0, 1, -1), (\"B\", 1, 0, 10)]\\nmapper = partial(matrix_multiply_mapper , 3)\\nExample: Matrix Multiplication | 295\\nwww.it-ebooks.inforeducer = partial(matrix_multiply_reducer , 3)\\nmap_reduce (entries, mapper, reducer) # [((0, 1), -3), ((0, 0), 32)]\\nThis isn’t terribly interesting on such small matrices, but if you had millions of rows\\nand millions of columns, it could help you a lot.\\nAn Aside: Combiners\\nOne thing you have probably noticed is that many of our mappers seem to include a\\nbunch of extra information.  For example, when counting words, rather than emitting\\n(word, 1)  and summing over the values, we could have emitted (word, None)  and\\njust taken the length.\\nOne reason we didn’t do this is that, in the distributed setting, we sometimes want to\\nuse combiners  to reduce the amount of data that has to be transferred around from\\nmachine to machine. If one of our mapper machines sees the word “data” 500 times,\\nwe can tell it to combine the 500 instances of (\"data\", 1)  into a single (\"data\",\\n500)  before handing off to the reducing machine. This results in a lot less data getting\\nmoved around, which can make our algorithm substantially faster still.\\nBecause of the way we wrote our reducer, it would handle this combined data cor‐\\nrectly. (If we’ d written it using len it would not have.)\\nFor Further Exploration\\n•The most widely used MapReduce system is Hadoop , which itself merits many\\nbooks. There are various commercial and noncommercial distributions and a\\nhuge ecosystem of Hadoop-related tools.\\nIn order to use it, you have to set up your own cluster  (or find someone to let you\\nuse theirs), which is not necessarily a task for the faint-hearted. Hadoop mappers\\nand reducers are commonly written in Java, although there is a facility known as\\n“Hadoop streaming” that allows you to write them in other languages (including\\nPython).\\n•Amazon.com offers an Elastic MapReduce  service that can programmatically cre‐\\nate and destroy clusters, charging you only for the amount of time that you’re\\nusing them.\\n•mrjob  is a Python package for interfacing with Hadoop (or Elastic MapReduce).\\n•Hadoop jobs are typically high-latency, which makes them a poor choice for\\n“real-time” analytics. There are various “real-time” tools built on top of Hadoop,\\nbut there are also several alternative frameworks that are growing in popularity.\\nTwo of the most popular are Spark  and Storm .\\n296 | Chapter 24: MapReduce\\nwww.it-ebooks.info•All that said, by now it’s quite likely that the flavor of the day is some hot new\\ndistributed framework that didn’t even exist when this book was written. Y ou’ll\\nhave to find that one yourself.\\nFor Further Exploration | 297\\nwww.it-ebooks.infowww.it-ebooks.infoCHAPTER 25\\nGo Forth and Do Data Science\\nAnd now, once again, I bid my hideous progeny go forth and prosper.\\n—Mary Shelley\\nWhere do you go from here? Assuming I haven’t scared you off of data science, there\\nare a number of things you should learn next.\\nIPython\\nWe mentioned IPython  earlier in the book. It provides a shell with far more function‐\\nality than the standard Python shell, and it adds “magic functions” that allow you to\\n(among other things) easily copy and paste code (which is normally complicated by\\nthe combination of blank lines and whitespace formatting) and run scripts from\\nwithin the shell.\\nMastering IPython will make your life far easier. (Even learning just a little bit of IPy‐\\nthon will make your life a lot easier.)\\nAdditionally, it allows you to create “notebooks” combining text, live Python code,\\nand visualizations that you can share with other people, or just keep around as a jour‐\\nnal of what you did ( Figure 25-1 ).\\n299\\nwww.it-ebooks.infoFigure 25-1. An IPython notebook\\nMathematics\\nThroughout this book, we dabbled in linear algebra ( Chapter 4 ), statistics ( Chap‐\\nter 5 ), probability ( Chapter 6 ), and various aspects of machine learning.\\nTo be a good data scientist, you should know much more about these topics, and I\\nencourage you to give each of them a more in-depth study, using the textbooks rec‐\\nommended at the end of the chapters, your own preferred textbooks, online courses,\\nor even real-life courses.\\nNot from Scratch\\nImplementing things “from scratch” is great for understanding how they work. But\\nit’s generally not great for  performance (unless you’re implementing them specifically\\nwith performance in mind), ease of use, rapid prototyping, or error handling.\\nIn practice, you’ll want to use well-designed libraries that solidly implement the fun‐\\ndamentals. (My original proposal for this book involved a second “now let’s learn the\\nlibraries” half that O’Reilly, thankfully, vetoed.)\\n300 | Chapter 25: Go Forth and Do Data Science\\nwww.it-ebooks.infoNumPy\\nNumPy  (for “Numeric Python”) provides facilities for doing “real” scientific comput‐\\ning. It features arrays that perform better than our list -vectors, matrices that per‐\\nform better than our list -of-list -matrices, and lots of numeric functions for\\nworking with them.\\nNumPy is a building block for many other libraries, which makes it especially valua‐\\nble to know.\\npandas\\npandas  provides  additional data structures for working with data sets in Python. Its\\nprimary abstraction is the DataFrame , which is conceptually similar to the NotQui‐\\nteABase Table  class we constructed in Chapter 23 , but with much more functionality\\nand better performance.\\nIf you’re going to use Python to munge, slice, group, and manipulate data sets, pandas\\nis an invaluable tool.\\nscikit-learn\\nscikit-learn  is probably the most popular library for doing machine learning in\\nPython. It contains all the models we’ve implemented and many more that we haven’t.\\nOn a real problem, you’ d never build a decision tree from scratch; you’ d let scikit-\\nlearn  do the heavy lifting. On a real problem, you’ d never write an optimization algo‐\\nrithm by hand; you’ d count on scikit-learn  to be already using a really good one.\\nIts documentation contains many, many examples  of what it can do (and, more gen‐\\nerally, what machine learning can do).\\nVisualization\\nThe matplotlib  charts we’ve been creating have been clean and functional but not\\nparticularly stylish (and not at all interactive). If you want to get deeper into data vis‐\\nualization, you have several options.\\nThe first is to further explore matplotlib , only a handful of whose features we’ve\\nactually covered. Its website contains many examples  of its functionality and a Gallery\\nof some of the more interesting ones. If you want to create static visualizations (say,\\nfor printing in a book), this is probably your best next step.\\nY ou should also check out seaborn , which is a library that (among other things)\\nmakes matplotlib  more attractive.\\nNot from Scratch | 301\\nwww.it-ebooks.infoIf you’ d like to create interactive  visualizations that you can share on the Web, the\\nobvious choice is probably D3.js , a JavaScript library for creating “Data Driven Docu‐\\nments” (those are the three Ds). Even if you don’t know much JavaScript, it’s often\\npossible to crib examples from the D3 gallery  and tweak them to work with your\\ndata. (Good data scientists copy from the D3 gallery; great data scientists steal  from\\nthe D3 gallery.)\\nEven if you have no interest in D3, just browsing the gallery is itself a pretty incredi‐\\nble education in data visualization.\\nBokeh  is a project that brings D3-style functionality into Python.\\nR\\nAlthough you can totally get away with not learning R, a lot of data scientists and data\\nscience projects use it, so it’s worth getting at least familiar with it.\\nIn part, this is so that you can understand people’s R-based blog posts and examples\\nand code; in part, this is to help you better appreciate the (comparatively) clean ele‐\\ngance of Python; and in part, this is to help you be a more informed participant in the\\nnever-ending “R versus Python” flamewars.\\nThe world has no shortage of R tutorials, R courses, and R books. I hear good things\\nabout Hands-On Programming with R , and not just because it’s also an O’Reilly book.\\n(OK, mostly because it’s also an O’Reilly book.)\\nFind Data\\nIf you’re doing data science as part of your job, you’ll most likely get the data as part\\nof your job (although not necessarily). What if you’re doing data science for fun? Data\\nis everywhere, but here are some starting points:\\n•Data.gov  is the government’s open data portal. If you want data on anything that\\nhas to do with the government (which seems to be most things these days) it’s a\\ngood place to start.\\n•reddit has a couple of forums, r/datasets  and r/data , that are places to both ask\\nfor and discover data.\\n•Amazon.com maintains a collection of public data sets  that they’ d like you to\\nanalyze using their products (but that you can analyze with whatever products\\nyou want).\\n•Robb Seaton has a quirky list of curated data sets on his blog .\\n302 | Chapter 25: Go Forth and Do Data Science\\nwww.it-ebooks.info•Kaggle  is a site that holds data science competitions. I never managed to get into\\nit (I don’t have much of a competitive nature when it comes to data science), but\\nyou might.\\nDo Data Science\\nLooking through data catalogs is fine, but the best projects (and products) are ones\\nthat tickle some sort of itch. Here are a few that I’ve done.\\nHacker News\\nHacker News  is a news aggregation and discussion site for technology-related news. It\\ncollects lots and lots of articles, many of which aren’t interesting to me.\\nAccordingly, several years ago, I set out to build a Hacker News story classifier  to pre‐\\ndict whether I would or would not be interested in any given story. This did not go\\nover so well with the users of Hacker News, who resented the idea that someone\\nmight not be interested in every story on the site.\\nThis involved hand-labeling a lot of stories (in order to have a training set), choosing\\nstory features (for example, words in the title, and domains of the links), and training\\na Naive Bayes classifier not unlike our spam filter.\\nFor reasons now lost to history, I built it in Ruby. Learn from my mistakes.\\nFire Trucks\\nI live on a major street in downtown Seattle, halfway between a fire station and most\\nof the city’s fires (or so it seems). Accordingly, over the years, I have developed a rec‐\\nreational interest in the Seattle Fire Department.\\nLuckily (from a data perspective) they maintain a Realtime 911 site  that lists every fire\\nalarm along with the fire trucks involved.\\nAnd so, to indulge my interest, I scraped many years’ worth of fire alarm data and\\nperformed a social network analysis  of the fire trucks. Among other things, this\\nrequired me to invent a fire-truck-specific notion of centrality, which I called Truck‐\\nRank.\\nT-shirts\\nI have a young daughter, and an incessant  source of frustration to me throughout her\\nchildhood has been that most “girls shirts” are quite boring, while many “boys shirts”\\nare a lot of fun.\\nDo Data Science | 303\\nwww.it-ebooks.infoIn particular, it felt clear to me that there was a distinct difference between the shirts\\nmarketed to toddler boys and toddler girls. And so I asked myself if I could train a\\nmodel to recognize these differences.\\nSpoiler: I could .\\nThis involved downloading the images of hundreds of shirts, shrinking them all to\\nthe same size, turning them into vectors of pixel colors, and using logistic regression\\nto build a classifier.\\nOne approach looked simply at which colors were present in each shirt; a second\\nfound the first 10 principal components of the shirt image vectors and classified each\\nshirt using its projections into the 10-dimensional space spanned by the “eigenshirts”\\n(Figure 25-2 ).\\nFigure 25-2. Eigenshirts corresponding to the first principal component\\nAnd You?\\nWhat interests you? What questions keep you up at night? Look for a data set (or\\nscrape some websites) and do some data science.\\nLet me know what you find! Email me at joelgrus@gmail.com  or find me on Twitter at\\n@joelgrus.\\n304 | Chapter 25: Go Forth and Do Data Science\\nwww.it-ebooks.infoIndex\\nA\\nA/B test, 87\\naccuracy, 145\\nof model performance, 146\\nall function (Python), 26\\nAnaconda distribution of Python, 15\\nany function (Python), 26\\nAPIs, using to get data, 114-120\\nexample, using Twitter APIs, 117-120\\ngetting credentials, 117\\nusing twython, 118\\nfinding APIs, 116\\nJSON (and XML), 114\\nunauthenticated API, 115\\nargs and kwargs (Python), 34\\nargument unpacking, 33\\narithmetic\\nin Python, 18\\nperforming on vectors, 50\\nartificial neural networks, 213\\n(see also neural networks)\\nassignment, multiple, in Python, 21\\nB\\nbackpropagation, 218\\nbagging, 211\\nbar charts, 39-43\\nBayes\\'s Theorem, 72, 165\\nBayesian Inference, 88\\nBeautiful Soup library, 108, 241\\nusing with XML data, 115\\nBernoulli trial, 82\\nBeta distributions, 89\\nbetweenness centrality, 256-260bias, 147\\nadditional data and, 148\\nbigram model, 242\\nbinary relationships, representing with matri‐\\nces, 55\\nbinomial random variables, 78, 82\\nBokeh project, 302\\nbooleans (Python), 25\\nbootstrap aggregating, 211\\nbootstrapping data, 184\\nbottom-up hierarchical clustering, 233-237\\nbreak statement (Python), 25\\nbuckets, grouping data into, 121\\nbusiness models, 141\\nC\\nCAPTCHA, defeating with a neural network,\\n219-223\\ncausation, correlation and, 67, 173\\ncdf (see cumulative distribtion function)\\ncentral limit theorem, 78, 86\\ncentral tendencies\\nmean, 59\\nmedian, 59\\nmode, 60\\nquantile, 60\\ncentrality\\nbetweenness, 255-260\\ncloseness, 259\\ndegree, 5, 256\\neigenvector, 260-264\\nclasses (Python), 30\\nclassification trees, 203\\ncloseness centrality, 259\\n305\\nwww.it-ebooks.infoclustering, 225-238\\nbottom-up hierarchical clustering, 233-237\\nchoosing k, 230\\nexample, clustering colors, 231\\nexample, meetups, 227-229\\nk-means clustering, 226\\nclusters, 132, 225\\ndistance between, 234\\ncode examples from this book, xiv\\ncoefficient of determination, 175\\ncombiners (in MapReduce), 296\\ncomma-separated values files, 106\\ncleaning comma-delimited stock prices, 128\\ncommand line, running Python scripts at, 103\\nconditional probability, 70\\nrandom variables and, 73\\nconfidence intervals, 85\\nconfounding variables, 65\\nconfusion matrix, 145\\ncontinue statement (Python), 25\\ncontinuity correction, 84\\ncontinuous distributions, 74\\ncontrol flow (in Python), 25\\ncorrectness, 145\\ncorrelation, 63\\nand causation, 67\\nin simple linear regression, 174\\nother caveats, 66\\noutliers and, 64\\nSimpson\\'s Paradox and, 65\\ncorrelation function, 173\\ncosine similarity, 269, 272\\nCounter (Python), 24\\ncovariance, 62\\nCREATE TABLE statement (SQL), 276\\ncumulative distribution function (cdf), 74\\ncurrying (Python), 31\\ncurse of dimensionality, 156-162, 271\\nD\\nD3.js library, 302\\ndata\\ncleaning and munging, 127\\nexploring, 121-127\\nfinding, 302\\ngetting, 103-120\\nreading files, 105-108\\nscraping from web pages, 108-114\\nusing APIs, 114-120using stdin and stdout, 103\\nmanipulating, 129-132\\nrescaling, 132-133\\ndata mining, 142\\ndata science\\nabout, xi\\ndefined, 1\\ndoing, projects of the author, 303\\nfrom scratch, xii\\nlearning more about, 299-304\\nskills needed for, xi\\nusing libraries, 300\\ndata visualization, 37-47\\nbar charts, 39-43\\nfurther exploration of, 301\\nline charts, 43\\nmatplotlib, 37\\nscatterplots, 44-46\\ndatabases and SQL, 275-287\\nCREATE TABLE and INSERT statements,\\n275-277\\nDELETE statement, 278\\nGROUP BY statement, 280-282\\nJOIN statement, 283\\nNoSQL, 287\\nORDER BY statement, 282\\nquery optimization, 286\\nSELECT statement, 278-280\\nsubqueries, 285\\nUPDATE statement, 277\\ndecision trees, 201-212\\ncreating, 206\\ndefined, 201\\nentropy, 203\\nentropy of a partition, 205\\nhiring tree implementation (example), 208\\nrandom forests, 211\\ndegree centrality, 5, 256\\nDELETE statement (SQL), 278\\ndelimited files, 106\\ndependence, 69\\nderivatives, approximating with difference quo‐\\ntients, 94\\ndictionaries (Python), 21\\ndefaultdict, 22\\nitems and iteritems methods, 29\\ndimensionality reduction, 134-139\\nusing principal component analysis, 134\\ndimensionality, curse of, 156-162, 271\\n306 | Index\\nwww.it-ebooks.infodiscrete distribution, 74\\ndispersion, 61\\nrange, 61\\nstandard deviation, 62\\nvariance, 61\\ndistance, 151\\n(see also nearest neighbors classification)\\nbetween clusters, 234\\ndistance function, 132, 152\\ndistribution\\nbernoulli, 78, 82\\nbeta, 89\\nbinomial, 78, 82\\ncontinuous, 74\\nnormal, 75\\ndot product, 51, 260\\ndummy variables, 179\\nE\\nedges, 255\\neigenshirts project, 303\\neigenvector centrality, 260-264\\nensemble learning, 211\\nentropy, 203\\nof a partition, 205\\nenumerate function (Python), 32\\nerrors\\nin clustering, 230\\nin multiple linear regression model, 181\\nin simple linear regression model, 174, 177\\nminimizing in models, 93-100\\nstandard errors of regression coefficients,\\n184-186\\nEuclidean distance function, 132\\nexceptions in Python, 19\\nexperience optimization, 87\\nF\\nF1 score, 146\\nfalse positives, 83\\nfarness, 259\\nfeatures, 148\\nchoosing, 149\\nextracting, 149\\nfeed-forward neural networks, 215\\nfiles, reading, 105\\ndelimited files, 106\\ntext files, 105\\nfilter function (Python), 32fire trucks project, 303\\nfor comprehensions (Python), 28\\nfor loops (Python), 25\\nin list comprehensions, 27\\nfull outer joins, 285\\nfunctions (Python), 18\\nG\\ngenerators (Python), 28\\ngetting data (see data, getting)\\nGibbs sampling, 246-247\\nGithub\\'s API, 115\\ngradient, 93\\ngradient descent, 93-101\\nchoosing the right step size, 97\\nestimating the gradient, 94\\nexample, minimize_batch function, 98\\nstochastic, 99\\nusing for multiple regression model, 181\\nusing in simple linear regression, 176\\ngrammars, 244-246\\ngreedy algorithms, 207\\nGROUP BY statement (SQL), 280-282\\nH\\nHacker News, 303\\nharmonic mean, 146\\nhierarchical clustering, 233-237\\nhistograms\\nof friend counts (example), 57\\nplotting using bar charts, 40\\nHTML, parsing, 108\\nexample, O\\'Reilly books about data, 110-114\\nusing Beautiful Soup library, 108\\nhypotheses, 81\\nhypothesis testing, 81\\nexample, an A/B test, 87\\nexample, flipping a coin, 81-84\\np-hacking, 86\\nregression coefficients, 184-186\\nusing confidence intervals, 85\\nusing p-values, 84\\nI\\nif statements (Python), 25\\nif-then-else statements (Python), 25\\nin operator (Python), 20, 22\\nin for loops, 25\\nIndex | 307\\nwww.it-ebooks.infousing on sets, 24\\nindependence, 69\\nindexes (database tables), 285\\ninference\\nBayesian Inference, 88\\nstatistical, in A/B test, 87\\ninner joins, 284\\nINSERT statement (SQL), 276\\ninteractive visualizations, 302\\ninverse normal cumulative distribution func‐\\ntion, 77\\nIPython, 15, 299\\nitem-based collaborative filtering, 272-274\\nJ\\nJavaScript, D3.js library, 302\\nJOIN statement (SQL), 283\\nJSON (JavaScript Object Notation), 114\\nK\\nk-means clustering, 226\\nchoosing k, 230\\nk-nearest neighbors classification (see nearest\\nneighbors classification)\\nkernel trick, 198\\nkey/value pairs (in Python dictionaries), 21\\nkwargs (Python), 34\\nL\\nLasso regression, 188\\nLatent Dirichlet Analysis (LDA), 248\\nlayers (neural network), 216\\nleast squares model\\nassumptions, 180\\nin simple linear regression, 174\\nleft joins, 284\\nlikelihood, 177, 193\\nline charts\\ncreating with matplotlib, 37\\nshowing trends, 43\\nlinear algebra, 49-56, 300\\nmatrices, 53-55\\nvectors, 49-53\\nlinear regression\\nmultiple, 179-188\\nassumptions of least squares model, 180\\nbootstrapping new data sets, 183\\ngoodness of fit, 183interpreting the model, 182\\nmodel, 179\\nregularization, 186\\nstandard errors of regression coeffi‐\\ncients, 184-186\\nsimple, 173-177\\nmaximum likelihood estimation, 177\\nmodel, 173\\nusing gradient descent, 176\\nusing to predict paid accounts, 189\\nlist comprehensions (Python), 27\\nlists (in Python), 20\\nrepresenting matrices as, 53\\nsort method, 27\\nusing to represent vectors, 49\\nzipping and unzipping, 33\\nlog likelihood, 193\\nlogistic regression, 189-200\\napplying the model, 194\\ngoodness of fit, 195\\nlogistic function, 192\\nproblem, predicting paid user accounts, 189\\nM\\nmachine learning, 141-150\\nbias-variance trade-off, 147\\ncorrectness, 145\\ndefined, 142\\nfeature extraction and selection, 148\\nmodeling data, 141\\noverfitting and underfitting, 142\\nscikit-learn library for, 301\\nmagnitude of a vector, 52\\nmanipulating data, 129-132\\nmap function (Python), 32\\nMapReduce, 289-297\\nbasic algorithm, 289\\nbenefits of using, 291\\ncombiners, 296\\nexample, analyzing status updates, 293\\nexample, matrix multiplication, 294-296\\nexample, word count, 289-291\\nmath.erf function (Python), 77\\nmatplotlib, 37, 301\\nmatrices, 53-55\\nimportance of, 54\\nmatrix multiplication, 260\\nusing MapReduce, 294-296\\nscatterplot matrix, 126\\n308 | Index\\nwww.it-ebooks.infomaximum likelihood estimation, 177\\nmaximum, finding using gradient descent, 94,\\n99\\nmean\\ncomputing, 59\\nremoving from PCA data, 134\\nmedian, 59\\nmeetups (example), 227-229\\nmember functions, 30\\nmerged clusters, 233\\nminimum, finding using gradient descent, 94\\nmode, 60\\nmodels, 141\\nbias-variance trade-off, 147\\nin machine learning, 142\\nmodules (Python), 17\\nmultiple assignment (Python), 21\\nN\\nn-gram models, 241-244\\nbigram, 242\\ntrigrams, 243\\nn-grams, 243\\nNaive Bayes algorithm, 165-172\\nexample, filtering spam, 165-167\\nimplementation, 168\\nnatural language processing (NLP), 239-253\\ngrammars, 244-246\\ntopic modeling, 247-252\\ntopics of interest, finding, 11\\nword clouds, 239-240\\nnearest neighbors classification, 151-163\\ncurse of dimensionality, 156-162\\nexample, favorite programming languages,\\n153-156\\nmodel, 151\\nnetwork analysis, 255-266\\nbetweenness centrality, 255-260\\ncloseness centrality, 259\\ndegree centrality, 5, 256\\ndirected graphs and PageRank, 264-266\\neigenvector centrality, 260-264\\nnetworks, 255\\nneural networks, 213-224\\nbackpropagation, 218\\nexample, defeating a CAPTCHA, 219-223\\nfeed-forward, 215\\nperceptrons, 213\\nneurons, 213NLP (see natural language processing)\\nnodes, 255\\nnoise, 133\\nin machine learning, 142\\nNone (Python), 25\\nnormal distribution, 75\\nand p-value computation, 85\\ncentral limit theorem and, 78\\nin coin flip example, 82\\nstandard, 76\\nnormalized tables, 283\\nNoSQL databases, 287\\nNotQuiteABase, 275\\nnull hypothesis, 81\\ntesting in A/B test, 88\\nNumPy, 301\\nO\\none-sided tests, 83\\nORDER BY statement (SQL), 282\\noverfitting, 142, 147\\nP\\np-hacking, 87\\np-values, 84\\nPageRank algorithm, 265\\npaid accounts, predicting, 11\\npandas, 120, 139, 301\\nparameterized models, 142\\nparameters, probability judgments about, 89\\npartial derivatives, 96\\npartial functions (Python), 31\\nPCA (see principal component analysis)\\nperceptrons, 213\\npip (Python package manager), 15\\npipe operator (|), 104\\npiping data through Python scripts, 103\\nposterior distributions, 89\\nprecision and recall, 146\\npredicate functions, 278\\npredictive modeling, 142\\nprincipal component analysis, 134\\nprobability, 69-80, 300\\nBayes\\'s Theorem, 72\\ncentral limit theorem, 78\\nconditional, 70\\ncontinuous distributions, 74\\ndefined, 69\\ndependence and independence, 69\\nIndex | 309\\nwww.it-ebooks.infonormal distribution, 75\\nrandom variables, 73\\nprobability density function, 74\\nprogramming languages for learning data sci‐\\nence, xii\\nPython, 15-35\\nargs and kwargs, 34\\narithmetic, 18\\nbenefits of using for data science, xii\\nBooleans, 25\\ncontrol flow, 25\\nCounter, 24\\ndictionaries, 21-23\\nenumerate function, 32\\nexceptions, 19\\nfunctional tools, 31\\nfunctions, 18\\ngenerators and iterators, 28\\nlist comprehensions, 27\\nlists, 20\\nobject-oriented programming, 30\\npiping data through scripts using stdin and\\nstdout, 103\\nrandom numbers, generating, 29\\nregular expressions, 30\\nsets, 24\\nsorting in, 26\\nstrings, 19\\ntuples, 21\\nwhitespace formatting, 16\\nzip function and argument unpacking, 33\\nQ\\nquantile, computing, 60\\nquery optimization (SQL), 286\\nR\\nR (programming language), xii, 302\\nrandom forests, 211\\nrandom module (Python), 29\\nrandom variables, 73\\nBernoulli, 79\\nbinomial, 78\\nconditioned on events, 73\\nexpected value, 73\\nnormal, 75-78\\nuniform, 74\\nrange, 61\\nrange function (Python), 28reading files (see files, reading)\\nrecall, 146\\nrecommendations, 267\\nrecommender systems, 267-274\\nData Scientists Y ou May Know (example), 6\\nitem-based collaborative filtering, 272-274\\nmanual curation, 268\\nrecommendations based on popularity, 268\\nuser-based collaborative filtering, 269-272\\nreduce function (Python), 32\\nusing with vectors, 51\\nregression (see linear regression; logistic regres‐\\nsion)\\nregression trees, 203\\nregular expressions, 30\\nregularization, 186\\nrelational databases, 275\\nrescaling data, 132-133, 188\\nridge regression, 186\\nright joins, 285\\nS\\nscalars, 49\\nscale of data, 132\\nscatterplot matrices, 126\\nscatterplots, 44-46\\nschema, 275\\nscikit-learn, 301\\nscraping data from web pages, 108-114\\nHTML, parsing, 108\\nexample, O\\'Reilly books about data,\\n110-114\\nSELECT statement (SQL), 278-280\\nsets (Python), 24\\nsigmoid function, 215\\nSimpson\\'s Paradox, 65\\nsmooth functions, 216\\nsocial network analysis (fire trucks), 303\\nsorting (in Python), 27\\nspam filters (see Naive Bayes algorithm)\\nsparse matrices, 294\\nSQL (Structured Query Language), 275\\n(see also databases and SQL)\\nsquare brackets ([]), working with lists in\\nPython, 20\\nstandard deviation, 62\\nstandard errors of coefficients, 183, 184-186\\nstandard normal distribution, 76\\nstatistics, 57-68, 300\\n310 | Index\\nwww.it-ebooks.infocorrelation, 62\\nand causation, 67\\nother caveats, 66\\nSimpson\\'s Paradox, 65\\ndescribing a single dataset, 57\\ncentral tendencies, 59\\ndispersion, 61\\ntesting hypotheses with, 81\\nstdin and stdout, 103\\nstemming words, 171\\nstochastic gradient descent, 99\\nusing to find optimal beta in multiple\\nregression model, 182\\nusing with PCA data, 136\\nstrings (in Python), 19\\nStructured Query Language (see databases and\\nSQL; SQL)\\nsubqueries, 285\\nsum of squares, computing for a vector, 52\\nsupervised learning, 225\\nsupervised models, 142\\nsupport vector machines, 196\\nT\\nt-shirts project, 303\\ntab-separated values files, 106\\ntables (database), 275\\nindexes, 285\\nnormalized, 283\\ntext files, working with, 105\\ntokenization, 245\\nfor Naive Bayes spam filter, 168\\ntopic modeling, 247-252\\ntransforming data (dimensionality reduction),\\n138\\ntrends, showing with line charts, 43\\ntrigrams, 243\\ntruthiness (in Python), 25\\ntuples (Python), 21\\nTwenty Questions, 201\\nTwitter APIs, using to get data, 117-120\\ngetting credentials, 117\\nusing twython, 118U\\nunderfitting, 143, 147\\nuniform distribution, 74\\ncumulative distribution function for, 74\\nunsupervised learning, 225\\nunsupervised models, 142\\nUPDATE statement (SQL), 277\\nuser-based collaborative filtering\\nV\\nvariance, 61, 147\\ncovariance versus, 62\\nreducing with more data, 147\\nvectors, 49-53\\nadding, 50\\ndataset of multiple vectors, representing as\\nmatrix, 54\\ndistance between, computing, 52\\ndot product of, 51\\nmultiplying by a scalar, 51\\nsubtracting, 51\\nsum of squares and magnitude, computing,\\n52\\nvisualizing data (see data visualization)\\nW\\nWHERE clause (SQL), 278\\nwhile loops (Python), 25\\nwhitespace in Python code, 16\\nword clouds, 239-240\\nX\\nXML data from APIs, 115\\nxrange function (Python), 28\\nY\\nyield operator (Python), 28\\nZ\\nzip function (Python), 33\\nusing with vectors, 50\\nIndex | 311\\nwww.it-ebooks.infoAbout the Author\\nJoel Grus  is a software engineer at Google. Previously he worked as a data scientist at\\nseveral startups. He lives in Seattle, where he regularly attends data science happy\\nhours. He blogs infrequently at joelgrus.com  and tweets all day long at @joelgrus .\\nColophon\\nThe animal on the cover of Data Science from Scratch  is a Rock Ptarmigan ( Lagopus\\nmuta ). This medium-sized gamebird of the grouse family is called simply “ptarmigan”\\nin the UK and Canada, and “snow chicken” in the United States. The rock ptarmigan\\nis sedentary, and breeds across arctic and subarctic Eurasia as well as North America\\nas far as Greenland. It prefers barren, isolated habitats, such as Scotland’s mountains,\\nthe Pyrenees, the Alps, the Urals, the Pamir Mountains, Bulgaria, the Altay Moun‐\\ntains, and the Japan Alps. It eats primarily birch and willow buds, but also feeds on\\nseeds, flowers, leaves, and berries. Developing young rock ptarmigans eat insects.\\nMale rock ptarmigans don’t have the typical ornaments of a grouse, aside from the\\ncomb, which is used for courtship display or altercations between males. Many stud‐\\nies have shown a correlation between comb size and testosterone levels in males. Its\\nfeathers moult from winter to spring and summer, changing from white to brown,\\nproviding it a sort of seasonal camouflage. Breeding males have white wings and grey\\nupper parts except in winter, when its plumage is completely white save for its black\\ntail.\\nAt six months of age, the ptarmigan becomes sexually mature; a breeding rate of six\\nchicks per breeding season is common, which helps protect the population from out‐\\nside factors such as hunting. It’s also spared many predators because of its remote\\nhabitat, and is hunted mainly by golden eagles.\\nRock ptarmigan meat is a popular staple in Icelandic festive meals. Hunting of rock\\nptarmigans was banned in 2003 and 2004 because of declining population. In 2005,\\nhunting was allowed again with restrictions to certain days. All rock ptarmigan trade\\nis illegal.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Cassell’s Natural History . The cover fonts are URW Type‐\\nwriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is\\nAdobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\\nwww.it-ebooks.info']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44ef01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e7536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff0ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
